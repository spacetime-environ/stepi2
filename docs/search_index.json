[["index.html", "Spatio-Temporal Methods in Environmental Epidemiology with R. Second Edition Introduction", " Spatio-Temporal Methods in Environmental Epidemiology with R. Second Edition Gavin Shaddick, James V. Zidek, and Alexandra M. Schmidt 2024-06-18 Introduction This is the online companion for the book Spatio-Temporal Methods in Environmental Epidemiology with R published by Chapman and Hall/CRC. Most of the codes used for the examples in the book are presented here to ensure the material is reproducible, transparent, and accessible. Please feel free to contact Alex Schmidt if you find any typos, or errors in the book or in our codes (Errare humanum est). We thank Sara Zapata-Marin, Johnny Xi, Paritosh Kumar Roy, and Mariana Carmona Baez for their help developing some of the additional examples and exercises for this edition. We would like to thank Yang ‘Seagle’ Liu, Yiping Dou, and Yi Liu for allowing us to include their code and other material in the book and in the online resources. "],["preface-to-second-edition.html", "PREFACE TO SECOND EDITION", " PREFACE TO SECOND EDITION The overall aims and academic level of the revision remain the same as those of the first edition. Briefly, it aims to explore the interface between environmental epidemiology (EE) and spatio-temporal (ST) modelling. Its intended audience: graduate students in epidemiology with an intermediate-level background in statistics, and graduate students in statistics. The original steered a course between and cut-and-paste style cookbook and a scholarly work on underlying theory. The field of spatio-temporal statistics has continued to evolve rapidly since the original version was published in 2015, and thanks especially to the addition of Professor Alex Schmidt as a third author, we have been able to include a broader range of theoretical methods, methods for computational implementation and more applications. New ways of implementing theory and methods in applications have been added, replacing some of the now lesser used approaches that were in the first edition. As a result of these changes, the book has grown from 368 pages to almost 458 pages. This gives instructors more topics to choose from when planning their courses, depending on the educational backgrounds of their students. This book will also assist practitioners to take advantage of these changes, as these advances in statistical science have led to the possibility of more sophisticated models for evaluating the health risks of exposures, which vary over space and time. In summary, motivated by real-life problems, this book will continue to aim at providing a link between recent advances in spatial-temporal methodology and epidemiological applications, together with the means to implement these methods in practice. Chapter titles have been changed to better reflect the story the book is trying to tell. The book’s foundation has three general components, which together will lead to an understanding of processes associated with spatio-temporal environmental epidemiology, and hence lead to a reduction in the uncertainty associated with them: The first component is represented by a process model, which incorporates direct and indirect prior knowledge about the natural process of interest e.g. a pollution field. Together with the form of the underlying process model we require prior knowledge to inform the parameters of the model, that may change dynamically over space and/or time. The third, empirical component of the investigation, leads to the data model i.e. measurement model. This third route to understanding has seen an explosion of interest in data science, and has lead to an entire chapter being devoted to it in this book. The Book now has a GitHub site (https://spacetime-environ.github.io/stepi2) that contains a toolbox of R-packages that has grown to enhance the value of the new methods now available for spatio-temporal modelling and make a wider array of applications feasible. That GitHub site includes an R Bookdown, which provides chapter-by-chapter material including the material from the first edition. The latter’s supplementary material found on a website, will be migrated to the R Bookdown. New material lies there, and is being added, as well including solutions to some selected exercises, R code, data or links to data files. The authors intend to keep the site updated and will add errata, additional data, new code, reviews, and so on. The authors remain indebted to all those individuals and institutions named above in the Preface to the first edition. To this list we now add, acknowledge, and warmly thank, individuals who helped put this revision together: Ms Mariana Carmona Baez inspired the creation of the Bookdown format for the examples on the GitHub site associated with the second Edition; Dr. Sara Zapata-Marin together with Mariana provided the code and analyses for the worked examples using Bookdown and helped develop the GitHub site they can be accessed; Mr Paritosh Kumar Roy provided the code for the forward-filtering-backward-sampling algorithm discussed in Chapter 11; Mr. Johnny Li assisted with a number of software issues and assembled solutions for selected exercises; Laís P. Freitas for her inspired graphic on the front cover, of spatio-temporal processes combined with their epidemiological impacts; Dr. Joe Watson for help in assembling his preferential sampling software for modelling the effect. Dr. Matthew L. Thomas for providing code, models, data and ideas related to the examples based on the WHO air quality database and modeling air pollutants in Europe. A big thanks also to Mr. Rob Calver, Senior Publisher-Mathematics, Statistics, and Physics. Chapman and Hall/CRD, Taylor and Francis Group for his advice and encouragement. Also, to Shashi Kumar on publisher’s Helpdesk for assistance with the Latex file. Finally, thank you, Lynne Zidek, for an enormous amount of help in coordinating and managing the revision. In the Second Edition, R code is now fully listed in the examples where it is used. These examples are provided as in the first edition, along with embedded R code and details on the use of specific R packages and other software. Additional code, data and examples are provided on the Book’s GitHub site along with other online resources associated with the book. These can be found on the GitHub site for the book. The following is a list of what has has been added to the first edition: dramatic new advances in spatio-temporal modelling, especially R packages for implementing those models, notably with NIMBLE and STAN, which replace the outdated WinBugs used in the first edition; a new chapter on data science that includes such things as data wrangling along with a clear description; of the complimentary roles of data modelling, process modelling and parameter modelling; listed code for software used in examples; modern computational methods, including INLA, together with code for implementation are provided; a new section on causality showing how the comparison of the impact of Covid in China and Italy are completely reversed when Simpson decomposition is applied; the R code needed for the examples is now fully listed in the text with additional code in R Bookdown posted on the book’s GitHub site; solutions to selected problems appear in the GitHub site; how to wrap a deterministic model in a stochastic shell by a unified approach to physical and statistical modeling; an illustration of how a build such as shell is illustrated by application to a deterministic model for the spread of an infectious disease. new sections on causality and confounders, including the relationship of Simpson’s paradox with process models. "],["why.html", "Chapter 1 An overview of spatio-temporal epidemiology and knowledge discovery?", " Chapter 1 An overview of spatio-temporal epidemiology and knowledge discovery? This book provides a comprehensive treatment of methods for spatio-temporal modelling and their use in epidemiological studies. Throughout the book, we present examples of spatio-temporal modelling within a Bayesian framework and describe how data can be used to update knowledge, together with in-depth consideration of the role of uncertainty. Throughout the book we will use the R statistical software. R is an object-orientated programming language and provides an integrated suite of software facilities for data manipulation, calculation and graphical display. It provides an environment within which almost all classical and modern statistical techniques are available. R can be used in a variety of ways, including directly from the command line or from within other programming environments. For many of the examples in this book we use , an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Both R (cran.r-project.org) and RStudio are open source and can be downloaded free of charge. They are both available for Windows, Mac OSX and Linux. The base R environment provides a rich set of functions with many thousands of functions with many more provided as additional packages, including those which allow the manipulation and visualization of spatio-temporal data, performing epidemiological analysis and advanced spatio-temporal analyses. These are available at The Comprehensive R Archive Network (CRAN) and can be downloaded from cran.r-project.org, a repository for R software. Additional packages need to be downloaded and installed. From this book, the reader will have gained an understanding of the following topics: The basic concepts of epidemiology and the estimation of risks associated with environmental hazards; The theory of spatial, temporal and spatio-temporal processes needed for environmental health risk analysis; Fundamental questions related to the nature and role of uncertainty in environmental epidemiology, and methods that may help answer those questions; How data can be used to update knowledge and reducing uncertainty; A history of data collection and how databases may be constructed and how data is formally represented through the sample space and associated formal constructs such as the sample space and sampling distributions; Important areas of application within environmental epidemiology, together with strategies for building the models that are needed and coping with challenges that arise; Computational methods for the analysis of complex data measured over both space and time and how they can be implemented using R; Areas of current and future research directions in spatio-temporal modelling; Examples of R code are given throughout the book and the code, together with data for the examples, are available online on the book’s GitHub site; The book contains a variety of exercises, both theoretical and practical, to assist in the development of the skills needed to perform spatio-temporal analyses. "],["basics.html", "Chapter 2 An introduction to modelling health risks and impacts Example 2.7: Estimating the SMR using a Poisson GLM Example 2.8: Estimating the SMR using quasi-likelihood Example 2.9: Modelling differences in SMRs in relation to differences in exposures Example 2.10: Modelling the risks associated with lagged effects of air pollution Example 2.10: Estimating the odds ratio in a case-control study using a logistic model Example 2.11: Estimating the odds ratio of asthma associated with proximity to roads", " Chapter 2 An introduction to modelling health risks and impacts This chapter contains the basic principles of epidemiological analysis and how estimates of the risks associated with exposures can be obtained. From this chapter, the reader will have gained an understanding of the following topics: Methods for expressing risk and their use with different types of epidemiological study. Calculating risks based on calculations of the expected number of health counts in an area, allowing for the age–sex structure of the underlying population. The use of generalised linear models (GLMS) to model counts of disease and case–control indicators. Modelling the effect of exposures on health and allowing for the possible effects of covariates. Cumulative exposures to environmental hazards. Example 2.7: Estimating the SMR using a Poisson GLM We consider the observed and expected number of cases of respiratory mortality in small areas in the UK from a study examining the long-term effects of air pollution (Elliott, Shaddick, Wakefield, de Hoogh, &amp; Briggs, 2007). Taking a single area (a ward) that had a population of 1601, the observed counts of deaths in people over 65 years old was 29 compared to the expected number which was 19.88. The SMR is therefore 29/19.88 = 1.46. # Finding MLE and SE of log(SMR) = beta0 on one single area y &lt;- 29 # Total observed death E &lt;- 19.88 # Expected deaths summary(glm(y ~ offset(log(E)), family = &quot;poisson&quot;)) ## ## Call: ## glm(formula = y ~ offset(log(E)), family = &quot;poisson&quot;) ## ## Deviance Residuals: ## [1] 0 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3776 0.1857 2.033 0.042 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6.6613e-16 on 0 degrees of freedom ## Residual deviance: 4.4409e-15 on 0 degrees of freedom ## AIC: 7.2109 ## ## Number of Fisher Scoring iterations: 3 Noting that the linear predictor, and thus the coefficient of interest, \\(\\beta_0\\), is on the log scale, the estimate of the SMR = \\(\\exp(\\beta_0) = \\exp(0.3776) = 1.458\\). The standard error of the estimate of beta_0 can be used to construct a 95% confidence interval: \\(\\hat{\\beta_0} \\pm 1.96 \\times se(\\hat{\\beta_0}) = (0.014, 0.741)\\). Again this is on the log–scale and so the exponent is taken of both the lower and upper limits; \\((\\exp(0.014), \\exp(0.741))\\). The SMR in this case is therefore 1.458 (95% CI; 1.013 – 2.099) meaning that the number of observed cases of disease in the area is significantly greater than that expected based on the age–sex profile of the population. We now consider the estimation of the SMR over more than a single area; in this case, we use the observed counts and expected numbers for \\(N=393\\) areas. The parameter, \\(\\beta_0\\), now corresponds to the log of the overall SMR using data from all the areas, i.e. \\(\\frac{\\sum^{393}_{i=1} O_i}{\\sum^{393}_{i=1} E_i}\\). From the data, this is 8282/7250.2 = 1.142 with the log of this being 0.133. # Finding MLE and SE of log(SMR) = beta0 over multiple areas summary(glm(Y ~ offset(log(E)), family = &quot;poisson&quot;, data = data)) We can see that the estimate of the SMR will be \\(\\exp(0.13305)=1.142\\), the overall SMR for all the areas. The 95% CI will be \\(\\exp(0.112) - \\exp(0.155) = 1.118 - 1.167\\), indicating an overall increased risk of respiratory mortality in these areas compared to what might be expected if they experienced the same mortality rates (by age and sex) as the national population. Example 2.8: Estimating the SMR using quasi-likelihood The R code for using quasi-likelihood to find the MLE of the \\(\\log\\)(SMR)\\(=\\beta_0\\) and its standard error using the data from the previous example would be # Using quasi-likelihood to find the MLE and standard error of log(SMR) = beta0 summary(glm(y ~ offset(log(E)), family=&quot;quasipoisson&quot;), data = data) Note that the estimate itself is the same as with the Poisson case, but the standard error has increased from 0.01099 to 0.02078, reflecting the over-dispersion which is present, with the dispersion parameter having been estimated to be over 3. The 95% confidence interval will therefore be wider; \\(\\exp(0.092)- \\exp(0.133) = 1.096- 1.190\\), with the increase in width reflecting the extra uncertainty that is present. Example 2.9: Modelling differences in SMRs in relation to differences in exposures We now consider the possible effects of air pollution in relation to the SMRs observed in the different areas; the exposure, \\(X_{i1}\\) for each area being the annual average of measurements from monitoring sites located within the health area. In addition, we consider the possible effects of a covariate; in this case, the covariate is a measure of deprivation known as the Carstairs score. Smoking is known to be a major risk factor for respiratory illness, and it is known that smoking habits vary with social class (Kleinschmidt, Hills, &amp; Elliott, 1995) and may therefore correlate with pollution levels, and act as a potential confounder. Although routine data on smoking levels at small area level are not available in Great Britain, strong correlations have, however, been demonstrated on several occasions between smoking rates and the Carstairs index of deprivation, which has also been shown to be a strong predictor of disease risk (Carstairs &amp; Morris, 1989). The index is derived from a weighted aggregation of data on four census variables: unemployment, overcrowding, car ownership and social class. The R code for fitting a model to estimate the relative risk associated with air pollution in this case is as follows: # Fitting a model to estimate the relative risk associated with air pollution summary(glm(Y ~ offset(log(E)) + X1, family=&quot;poisson&quot;, data=data_df)) In this case, the effect of air pollution is highly significant and the associated relative risk will be \\(\\exp(\\beta_1) = \\exp(0.07972) = 1.082\\), indicating an increase in risk of 8.2% associated with every increase of one unit in air pollution (in this case, the units are 10\\(\\mu\\)gm\\(^{-3}\\)). Using a quasi-likelihood approach again results in the same estimate but with a larger standard error. # Fitting a model to estimate the relative risk associated with air pollution # using a Quasi-Poisson approach summary(glm(Y ~ offset(log(E)) + X1, family = &quot;quasipoisson&quot;, data = data_df)) The 95% CIs are 1.0615 – 1.1049 for the Poisson case and 1.0434 – 1.1241 when using quasi-likelihood; both indicating that the increase in risk is significant, with the wider intervals in the quasi-likelihood case again reflecting the extra uncertainty associated with the over-dispersion. Adding the deprivation score, \\(X_2\\), to the model might be expected to reduce the risk associated with air pollution as areas which are highly polluted are likely to also be deprived and deprived areas, with some exceptions, have higher rates of disease. It is therefore a confounder in the relationship between air pollution and health. The R code for a model with both air pollution and deprivation is asfollows: #Fitting a Poisson GLM with air pollution and deprivation summary(glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;poisson&quot;, data = data_df)) It can be seen that adding deprivation to the model has resulted in a reduction in the size of the effect associated with air pollution, for which the RR has changed from 1.083 to 1.026 (95% CI; 1.004 – 1.049). The effect of deprivation is also significant, with an increase in risk of 5.3% (RR = \\(\\exp(0.051302)=1.053\\)) associated with a unit increase in Carstairs score. When using quasi-likelihood, the estimates of relative risk are the same, but again they have wider confidence intervals. # Fitting a Quasi-Poisson GLM with air pollution and deprivation summary(glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data_df)) This gives a RR for air pollution of 1.026 (95% CI; 0.993 – 1.060) and for deprivation a RR of 1.053 (95% CI; 1.045 – 1.061) which in this case leads to the effect of air pollution being non-significant. This suggests that it is deprivation that is playing a large part in the differences in SMRs observed in the different areas. Note the amount of the widening of the intervals is reduced as there is less over-dispersion; some of the extra-Poisson variability has thus been `explained’ by deprivation. The effect of adding deprivation to the model can be assessed by calculating the change in deviance between two models; (i) with air pollution and (ii) with both air pollution and deprivation. A significant difference in deviance will indicate that deprivation is a significant risk factor. The R code to perform a test between the deviances of the two models is as follows: ## Test 1: Effect on Quasi-Poisson models with and without deprivation anova( glm(Y ~ offset(log(E)) + X1, family = &quot;quasipoisson&quot;, data = data), # Model 1 glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data), # Model2 test = &quot;Chisq&quot; ) # Chi-Squared test This shows that deprivation has a highly significant effect on the risk of respiratory mortality. Using this method, the effect of taking air pollution out of the model can also be assessed, which proves to have a non-significant change in deviance; this indicates that when deprivation is included in the model the estimated risk associated with air pollution is non-significant. ## Test 2: Effect on Quasi-Poisson models with and without air pollution anova( glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data), # Model 1 glm(Y ~ offset(log(E)) + X2, family = &quot;quasipoisson&quot;, data = data), # Model 2 test = &quot;Chisq&quot; ) # Chi-Squared test Example 2.10: Modelling the risks associated with lagged effects of air pollution Following on from the previous example, we might fit the annual averages from the previous three years, \\(X_{it}, X_{i(t-1)}, X_{i(t-2)}\\). The R code to do this is as follows: ## Fitting quasi-poisson model glm( formula = Y ~ offset(log(E)) + X1 + X1t1 + X1t2, family = &quot;quasipoisson&quot;, data = data ) Example 2.10: Estimating the odds ratio in a case-control study using a logistic model In a study of asthma of whether children living near main roads require higher levels of asthma treatment than those who live further away, cases and controls were grouped according to whether or not they lived within 150 m of a main road . Of the 1066 cases, 172 lived within 150m of a main road with the corresponding number of controls being 464 (out of 6233). The MLE of the probability that an individual is a case can be found using R as follows: ## Fitting Odds Ratio glm(formula = Y ~ 1, family = &quot;binomial&quot;, data = data_df) The estimate \\(-1.76594\\) is on the log–odds scale. This can be converted back to the probability scale as, \\(\\frac{\\exp(-1.76594)}{(1 + \\exp(-1.76594)} = 0.146\\) which is the same as the proportion of cases (1066/6233). Example 2.11: Estimating the odds ratio of asthma associated with proximity to roads We now estimate the effects of living near to a main road on asthma. The R code to do this is as follows: ## Fitting Odds Ratio glm(Y ~ X, family = &quot;binomial&quot;, data = data) Here, the odds ratio associated with living close to a main road is \\(\\exp(0.87216) = 2.391\\) (95% CI; 1.981 – 2.889. This indicates that there is a significant increase in risk of asthma in the children under study associated with their living close to a main road. Of course there may be confounders, such as parental smoking, which may affect this. If available, these confounders could be added to the model in the same way as seen in the Poisson example. "],["uncertainty.html", "Chapter 3 The importance of uncertainty: assessment and quantification Supplementary Material", " Chapter 3 The importance of uncertainty: assessment and quantification This chapter contains a discussion of uncertainty, both in terms of statistical modelling and quantification, but also in the wider setting of sources of uncertainty outside those normally encountered in statistics. From this chapter, the reader will have gained an understanding of the following topics: Uncertainty can be dichotomized as either qualitative or quantitative, with the former allowing consideration of a wide variety of sources of uncertainty that would be difficult, if not impossible, to quantify mathematically. Quantitative uncertainty can be thought of as comprising both aleatory and epistemic components, the former representing stochastic uncertainty and the latter subjective uncertainty. Methods for assessing uncertainty including eliciting prior information from experts and sensitivity analysis. Indexing quantitative uncertainty using the variance and entropy of the distribution of a random quantity. Uncertainty in post-normal science derives from a wide variety of issues and can lead to high levels of that uncertainty with serious consequences. Understanding uncertainty is therefore a vital feature of modern environmental epidemiology. Supplementary Material More on qualitative uncertainty The NUSAP matrix consists of five columns, each of which represents one of five generic assessment criteria . The first three of these refer mainly to quantitative elements involved in generating the knowledge base: (i) numerical; (ii) units and (ii) spread. The latter two primarily represent qualitative uncertainty: (iv) assessment and (v) pedigree. Each row of the matrix contains the basic objects involved in the uncertainty analysis. Entries under (i) numerical might for example, the relative risk associated with short–term changes in air pollution for instance with the appropriate units given in (ii) the unit column. Spread is a measure of uncertainty in an estimate or prediction for the quantity defined in columns (i) and (ii) , and in the air pollution example might be the standard error or confidence interval associated with the risk estimate. Columns (i), (ii) and (iii) therefore capture the information that is available from standard statistical approaches. The final two columns act as an aid to recording this more general uncertainty with entries in the (iv) assessment column usually be a numerical quantity summarizing the result of a pedigree analysis. This pedigree analysis would consist of an organized discussion amongst experts, possibly from a wide variety of subject areas and interests, charged with determining the uncertainty in the relevant knowledge base related to the overall objectives of the analysis. Example S.1: The health impact of VOCs The NUSAP matrix would be designed created to fit the specific context and process component being assessed, for example the quality of the data or the quality of a complex model. Here we will focus on the pedigree and assessment in an example of the health impact of VOCS. We concentrate on the pedigree analysis and show how the example of such a consultation can itself be expressed as a pedigree matrix. Based on interest in emissions of volatile organic compound (VOC) emissions from paint, a study was undertaken to assess the uncertainties associated with annual emissions in the Netherlands, . As part of this study a pedigree matrix was used in order to allow qualitative uncertainties to be incorporated into the assessment of monitoring data. An abbreviated version of the pedigree matrix appears in Table 1 which shows how scores are to be assigned by experts according to four different pedigree criteria when considering a particular type of paint. In this case, ‘Proxy’ might relate to inference of VOC emissions based on national paint sales, ‘Empirical’ to the degree to which direct measurements were used for the assessment, ‘Method’ to methodological rigour in making the measurements and ‘validation’ the comparison between measurements over a long time frame. Differences in emissions will arise from the different uses of paint, which can be classified into five categories including SHI for ship construction and DIY for do–it–yourself (DIY) construction. There will also be variations based on whether paint is produced locally or imported. In addition, thinner which is added to the paint is also a source of emissions and will thus lead to differences in emissions. This would lead to four emission estimates for each class for which for SHI and DIY have an annual VOC totals for 1998 of 2.4 and 11.3 ktonnes respectively with the total across the five classes being 32.2 ktonnes of emissions from paint purchased nationally. The overall sum of annual VOC emission for both national and imported paints is 51.6 ktonnes. However the components of these total emissions are found in a variety of ways and some will be more certain than others and here the pedigree analysis can help in assessing the relative uncertainties and how they might combine in the aggregation. For example, calculations made on national sales data of VOC emissions for paint resulting in an estimate of 6.6 ktonnes. In this case a uncertainty score of 4 was assigned for Method reflecting the relative robustness of this approach. In contrast the estimate of the proportion of paint that had thinner added, 28%, in DIY construction was imputed rather than measured and was scored only 2 for Method since the value was considered to be no more than an educated guess. Such an analysis was done for each each source of VOCs for a total of fifteen pedigree analyses on each of the four pedigree criteria above. There were only fifteen and not 5 × 4 = 20 since ten of the twenty fell into into just six categories. The result was a 15 × 4 (pedigree) matrix with a number between 0 and 4 appearing in each cell. Expressing the row columns as a percentage of the maximum value of 16 gives a measure of the overall strength of evidence for each the categories . In this analysis, National-DIY category got the highest score of 0.8 with the National-SHI getting a lower score of 0.7. The results led to a number of conclusions. For example the assumed VOC percentage for imported paint had one of the two weakest uncertainty assessments pointing to a lack of requisite knowledge and the need for improvements in the way these VOC emissions are assessed. Diagnostic analyses of the scores are also possible. The diagnostic diagram is one tool for doing this. It aims to compare qualitative and quantitative uncertainties in a search for discrepancies. More precisely, it plots sensitivity to inputs against strengths. Each input parameter was randomly varied around its specified value, independently of the other parameters, and for each simulation the contribution of the component VOC’s contribution to a paint class’s total VOC was determined. It was found that the contribution from the imported VOC percentage was by far the greatest at 82.0%. When plotted against its strength score of 0.3, it indicated a strong outlier and singled out this component as needing further information and analysis. Overall the NUSAP analysis led to a number of important insights into the paint monitoring program. (#fig:Table 1)The pedigree matrix for the VOC data. It gives the scoring criteria based on evaluators’ assessments with respect to the various criteria. "],["data.html", "Chapter 4 Extracting information from data Getting to know the structure of dataframes Extracting and creating variables Simple manipulations using Tidyverse Example 4.1: Health impacts associated with outdoor air pollution Example 4.3. Mapping cancer incidence in Southern and Eastern Serbia", " Chapter 4 Extracting information from data There is a rich history of collecting environmental data and recently there has been an explosion in quantity and complexity of data related to the environment, from monitoring, satellite remote sensing, numerical modelling and many other sources. This chapter provides an introduction to the variety of different sources of data that are available and methods for obtaining, manipulating and processing data using the Tidyverse in R so that it is a form that can be readily used for analysis. The reader will have gained an understanding of the following topics: How the Tidyverse can be used for data wrangling; The importance of visualization in communication and understanding; How R can be used to summarize and visualize data; The use of shape files to produce maps in R; How to calculate expected numbers and SMRs in practice; How to perform health impact analysis. Getting to know the structure of dataframes Once a dataframe has been loaded into R we can examine it and perform analysis. Initially, we can understand our dataset by finding the number of observations and variables in data frames by using the and functions, respectively. We will load and analyse data from the World Health Organization’s Global Air Pollution database. The data is open source and can be downloaded from the WHO’s website in format. It contains over 11,000 measurements of fine particulate matter air pollution (PM2.5 and PM10) for the years 2010-2019 and details of the locations of monitoring sites. We can import this into R and convert it to a dataframe either by using the # import the dataset from a .csv file WHO_GM_Database &lt;- read.csv(&quot;data/WHOGMDatabase.csv&quot;) # Viewing the structure of the variables within the WHO Air Pollution dataset str(WHO_GM_Database) ## &#39;data.frame&#39;: 65428 obs. of 34 variables: ## $ ISO3 : chr &quot;AFG&quot; &quot;AFG&quot; &quot;AFG&quot; &quot;ALB&quot; ... ## $ CountryName : chr &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Albania&quot; ... ## $ Year : chr &quot;2009&quot; &quot;2009&quot; &quot;2019&quot; &quot;2015&quot; ... ## $ StationID : chr &quot;AFG1&quot; &quot;AFG2&quot; &quot;AFG3&quot; &quot;ALB1&quot; ... ## $ StationIDOrig : chr NA NA &quot;[US Diplomatic Post: Kabul]&quot; &quot;AL0205A&quot; ... ## $ StationIDOldDatabase: int 1 1 0 1 1 1 1 1 1 1 ... ## $ City : chr &quot;Mazar-e Sharif&quot; &quot;Kabul&quot; &quot;[Kabul]&quot; NA ... ## $ CityReverseGeocoded : chr &quot;Mazar-i-Sharif&quot; &quot;Kabul&quot; &quot;Kabul&quot; &quot;Durrës&quot; ... ## $ CityGiulia : chr NA NA NA NA ... ## $ CityClean : chr &quot;Mazar-e Sharif&quot; &quot;Kabul&quot; &quot;Kabul&quot; &quot;Durrës&quot; ... ## $ Longitude : num 67.1 69.2 69.2 19.4 19.4 ... ## $ Latitude : num 36.7 34.5 34.5 41.3 41.3 ... ## $ PM25 : num 68 86 119.8 NA 14.3 ... ## $ PM25PercCoverage : num 0.0384 0.0384 0.18 NA 0.7533 ... ## $ PM25Grading : int 3 3 3 NA 1 NA NA NA 1 1 ... ## $ PM10 : num 334 260 NA 17.6 24.6 ... ## $ PM10PercCoverage : num NA NA NA 0.794 0.839 ... ## $ PM10Grading : int 4 4 NA 1 1 1 1 1 NA 1 ... ## $ LocationInfo : chr &quot;NA, NA&quot; &quot;NA, NA&quot; NA &quot;AL0205A, NA&quot; ... ## $ Source : chr &quot;Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldier&quot;| __truncated__ &quot;Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldier&quot;| __truncated__ &quot;OpenAQ&quot; &quot;European Environment Agency (EEA)&quot; ... ## $ MonitorType : chr &quot;Urban&quot; &quot;Urban&quot; &quot;Unknown&quot; &quot;Traffic&quot; ... ## $ MonitorTypeOrig : chr &quot;urban&quot; &quot;urban&quot; NA &quot;Traffic&quot; ... ## $ PM25Conv : int 0 0 0 1 0 1 1 1 0 0 ... ## $ UnspecifiedType : int 0 0 1 0 0 0 0 0 0 0 ... ## $ WebLink : chr NA NA &quot;[[[\\&quot;EPA AirNow DOS\\&quot;,\\&quot;http://airnow.gov/index.cfm?action=airnow.global_summary\\&quot;]]]&quot; &quot;https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2&quot; ... ## $ Version : chr &quot;2016 Release&quot; &quot;2016 Release&quot; &quot;2021 Release&quot; &quot;Redownloaded for 2021 Release, replacing 2018 Release&quot; ... ## $ WHOStatus : chr &quot;Member state&quot; &quot;Member state&quot; &quot;Member state&quot; &quot;Member state&quot; ... ## $ WHORegion : chr &quot;EMR&quot; &quot;EMR&quot; &quot;EMR&quot; &quot;EUR&quot; ... ## $ WHOIncomeRegion : chr &quot;EMR LMI&quot; &quot;EMR LMI&quot; &quot;EMR LMI&quot; &quot;EUR LMI&quot; ... ## $ SDG1Region : chr &quot;Central Asia and Southern Asia&quot; &quot;Central Asia and Southern Asia&quot; &quot;Central Asia and Southern Asia&quot; &quot;Northern America and Europe&quot; ... ## $ SDG2Region : chr &quot;Southern Asia&quot; &quot;Southern Asia&quot; &quot;Southern Asia&quot; &quot;Europe&quot; ... ## $ SDG3Region : chr &quot;Southern Asia&quot; &quot;Southern Asia&quot; &quot;Southern Asia&quot; &quot;Southern Europe&quot; ... ## $ GBDRegion : chr &quot;Asia, South&quot; &quot;Asia, South&quot; &quot;Asia, South&quot; &quot;Europe, Central&quot; ... ## $ GBDSuperRegion : chr &quot;South Asia&quot; &quot;South Asia&quot; &quot;South Asia&quot; &quot;Central Europe, Eastern Europe and Central Asia&quot; ... A quick way of viewing the dataset to see the data are using the names(), str() and head() functions. The names() function will display the variable names within a dataframe. The str() function will display the structure of the dataset, and the head() function will display the first 6 rows in the dataframe Extracting and creating variables # Extracting the variable Year from WHO_GM_Database and assign to a new variable called Year YearOfMeasurement &lt;- WHO_GM_Database[,&#39;Year&#39;] # show the first 5 entries in YearOfMeasurement YearOfMeasurement[1:5] ## [1] &quot;2009&quot; &quot;2009&quot; &quot;2019&quot; &quot;2015&quot; &quot;2016&quot; # Extracting the row (or observation) from WHO_GM_Database and assign to a new variable called FirstRow FirstRow &lt;- WHO_GM_Database[1,] # Show the first 10 entries in FirstRow FirstRow[1:10] ## ISO3 CountryName Year StationID StationIDOrig StationIDOldDatabase ## 1 AFG Afghanistan 2009 AFG1 &lt;NA&gt; 1 ## City CityReverseGeocoded CityGiulia CityClean ## 1 Mazar-e Sharif Mazar-i-Sharif &lt;NA&gt; Mazar-e Sharif # Extracting the 3rd row for Year from WHO_GM_Database WHO_GM_Database[3,&#39;Year&#39;] ## [1] &quot;2019&quot; Alternatively, you can extract variables from the dataframes by using the operator. We first specify the dataset, then give the name of the variable that we want. Let’s extract the variable from . # Extracting the variable a from WHO_GM_Database, and show the first 3 entries WHO_GM_Database$Year[1:3] ## [1] &quot;2009&quot; &quot;2009&quot; &quot;2019&quot; Creating a new variable within a data frame is straightforward. Let’s create a variable within which is the difference between and 2010. For this we ex- tract the variable from and subtract 2010. In the dataframe, is a character variable (which you can see using ) and we will need to convert this to a numeric variable before performing the calculation. # convert Year to a numeric variable WHO_GM_Database$Year &lt;- as.numeric(WHO_GM_Database$Year) ## Warning: NAs introduced by coercion # Extracting Year a from WHO_GM_Database,subtract 2000 and make a new variable in the WHO_GM_Database dataframe. For clarity, we will only show the first 3 entries WHO_GM_Database$TimeSince2010 &lt;- WHO_GM_Database$Year - 2000 WHO_GM_Database$TimeSince2010[1:3] ## [1] 9 9 19 Simple manipulations using Tidyverse Again, we will use the data frame and will start by looking at some basic operations, such as subsetting, sorting and adding new columns. One operation we often want to do is to extract a subset of rows according to some criterion. For example, we may want to extract all rows of the iris dataset that correspond to the versicolor species. In Tidyverse, we can use a function called . For clarity, we will show the first 3 rows of the output Filter rows in Tidyverse require(&quot;tidyverse&quot;) ## Loading required package: tidyverse ## ── Attaching packages ──────────────────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.1 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## Warning: package &#39;stringr&#39; was built under R version 4.2.3 ## ── Conflicts ─────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() filter(WHO_GM_Database, Year == 2019)[1:3,] ## ISO3 CountryName Year StationID StationIDOrig ## 1 AFG Afghanistan 2019 AFG3 [US Diplomatic Post: Kabul] ## 2 ALB Albania 2019 ALB2 AL0204A ## 3 ARE United Arab Emirates 2019 ARE22 Hamdan Street ## StationIDOldDatabase City CityReverseGeocoded CityGiulia CityClean ## 1 0 [Kabul] Kabul &lt;NA&gt; Kabul ## 2 1 &lt;NA&gt; Vlorë &lt;NA&gt; Vlorë ## 3 0 Abu dhabi Abu Dhabi &lt;NA&gt; Abu dhabi ## Longitude Latitude PM25 PM25PercCoverage PM25Grading PM10 ## 1 69.19051 34.53581 119.77360 0.1800000 3 NA ## 2 19.48620 40.40309 10.31525 0.7510274 1 NA ## 3 54.37730 24.45390 NA NA NA 100 ## PM10PercCoverage PM10Grading LocationInfo ## 1 NA NA &lt;NA&gt; ## 2 NA NA AL0204A, NA ## 3 NA 4 Hamdan Street, Abu dhabi, United Arab Emirates ## Source MonitorType ## 1 OpenAQ Unknown ## 2 European Environment Agency (EEA) Background ## 3 Statistics Center - Abu Dhabi from Environment Agency - Abu Dhabi Urban ## MonitorTypeOrig PM25Conv UnspecifiedType ## 1 &lt;NA&gt; 0 1 ## 2 Background 0 0 ## 3 traffic 1 0 ## WebLink ## 1 [[[&quot;EPA AirNow DOS&quot;,&quot;http://airnow.gov/index.cfm?action=airnow.global_summary&quot;]]] ## 2 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 3 &lt;NA&gt; ## Version WHOStatus WHORegion ## 1 2021 Release Member state EMR ## 2 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 3 2021 Release Member state EMR ## WHOIncomeRegion SDG1Region SDG2Region ## 1 EMR LMI Central Asia and Southern Asia Southern Asia ## 2 EUR LMI Northern America and Europe Europe ## 3 EMR HI Western Asia and Northern Africa Western Asia ## SDG3Region GBDRegion ## 1 Southern Asia Asia, South ## 2 Southern Europe Europe, Central ## 3 Western Asia North Africa / Middle East ## GBDSuperRegion TimeSince2010 ## 1 South Asia 19 ## 2 Central Europe, Eastern Europe and Central Asia 19 ## 3 North Africa / Middle East 19 Sorting rows in Tidyverse The function will sort the data by (alphabetically) and then by (numerically). Again, for clarify we will show only the first few rows of the data (9 rows) arrange(WHO_GM_Database,CountryName, Year)[1:9,] ## ISO3 CountryName Year StationID StationIDOrig ## 1 AFG Afghanistan 2009 AFG1 &lt;NA&gt; ## 2 AFG Afghanistan 2009 AFG2 &lt;NA&gt; ## 3 AFG Afghanistan 2019 AFG3 [US Diplomatic Post: Kabul] ## 4 ALB Albania 2011 ALB4 AL0201A ## 5 ALB Albania 2011 ALB5 AL0202A ## 6 ALB Albania 2012 ALB4 AL0201A ## 7 ALB Albania 2012 ALB5 AL0202A ## 8 ALB Albania 2013 ALB4 AL0201A ## 9 ALB Albania 2014 ALB2 AL0204A ## StationIDOldDatabase City CityReverseGeocoded CityGiulia ## 1 1 Mazar-e Sharif Mazar-i-Sharif &lt;NA&gt; ## 2 1 Kabul Kabul &lt;NA&gt; ## 3 0 [Kabul] Kabul &lt;NA&gt; ## 4 1 &lt;NA&gt; Tirana &lt;NA&gt; ## 5 1 &lt;NA&gt; Tirana &lt;NA&gt; ## 6 1 &lt;NA&gt; Tirana &lt;NA&gt; ## 7 1 &lt;NA&gt; Tirana &lt;NA&gt; ## 8 1 &lt;NA&gt; Tirana &lt;NA&gt; ## 9 1 &lt;NA&gt; Vlorë &lt;NA&gt; ## CityClean Longitude Latitude PM25 PM25PercCoverage PM25Grading ## 1 Mazar-e Sharif 67.11667 36.70000 68.00000 0.03835616 3 ## 2 Kabul 69.19128 34.53076 86.00000 0.03835616 3 ## 3 Kabul 69.19051 34.53581 119.77360 0.18000000 3 ## 4 Tirana 19.82177 41.33027 27.53000 0.97991000 1 ## 5 Tirana 19.85167 41.34560 37.49400 0.94475000 1 ## 6 Tirana 19.82177 41.33027 20.20700 0.97359000 1 ## 7 Tirana 19.85167 41.34560 24.52900 0.97848000 1 ## 8 Tirana 19.82177 41.33027 16.06237 0.77477169 1 ## 9 Vlorë 19.48620 40.40309 NA NA NA ## PM10 PM10PercCoverage PM10Grading LocationInfo ## 1 334.00000 NA 4 NA, NA ## 2 260.00000 NA 4 NA, NA ## 3 NA NA NA &lt;NA&gt; ## 4 52.36900 0.9847000 1 AL0201A, NA ## 5 112.44400 0.9172400 1 AL0202A, NA ## 6 33.99500 0.9833800 1 AL0201A, NA ## 7 37.85900 0.9788300 1 AL0202A, NA ## 8 31.61542 0.7990868 1 AL0201A, NA ## 9 15.25370 0.8649543 1 AL0204A, NA ## Source ## 1 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 2 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 3 OpenAQ ## 4 European Environment Agency (EEA) ## 5 European Environment Agency (EEA) ## 6 European Environment Agency (EEA) ## 7 European Environment Agency (EEA) ## 8 European Environment Agency (EEA) ## 9 European Environment Agency (EEA) ## MonitorType MonitorTypeOrig PM25Conv UnspecifiedType ## 1 Urban urban 0 0 ## 2 Urban urban 0 0 ## 3 Unknown &lt;NA&gt; 0 1 ## 4 Traffic Traffic 0 0 ## 5 Traffic Traffic 0 0 ## 6 Traffic Traffic 0 0 ## 7 Traffic Traffic 0 0 ## 8 Traffic Traffic 0 0 ## 9 Background Background 1 0 ## WebLink ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 [[[&quot;EPA AirNow DOS&quot;,&quot;http://airnow.gov/index.cfm?action=airnow.global_summary&quot;]]] ## 4 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 5 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 6 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 7 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 8 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 9 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## Version WHOStatus WHORegion ## 1 2016 Release Member state EMR ## 2 2016 Release Member state EMR ## 3 2021 Release Member state EMR ## 4 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 5 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 6 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 7 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 8 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 9 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## WHOIncomeRegion SDG1Region SDG2Region SDG3Region ## 1 EMR LMI Central Asia and Southern Asia Southern Asia Southern Asia ## 2 EMR LMI Central Asia and Southern Asia Southern Asia Southern Asia ## 3 EMR LMI Central Asia and Southern Asia Southern Asia Southern Asia ## 4 EUR LMI Northern America and Europe Europe Southern Europe ## 5 EUR LMI Northern America and Europe Europe Southern Europe ## 6 EUR LMI Northern America and Europe Europe Southern Europe ## 7 EUR LMI Northern America and Europe Europe Southern Europe ## 8 EUR LMI Northern America and Europe Europe Southern Europe ## 9 EUR LMI Northern America and Europe Europe Southern Europe ## GBDRegion GBDSuperRegion TimeSince2010 ## 1 Asia, South South Asia 9 ## 2 Asia, South South Asia 9 ## 3 Asia, South South Asia 19 ## 4 Europe, Central Central Europe, Eastern Europe and Central Asia 11 ## 5 Europe, Central Central Europe, Eastern Europe and Central Asia 11 ## 6 Europe, Central Central Europe, Eastern Europe and Central Asia 12 ## 7 Europe, Central Central Europe, Eastern Europe and Central Asia 12 ## 8 Europe, Central Central Europe, Eastern Europe and Central Asia 13 ## 9 Europe, Central Central Europe, Eastern Europe and Central Asia 14 Select columns in Tidyverse Now let’s say that we wish to select just the , and columns from the data set and assign it to a new dataset, . In Tidyverse we can use the function: WHO_GM_Database_selectcolumns &lt;- select(WHO_GM_Database, CountryName, Year, PM25) head(WHO_GM_Database_selectcolumns) ## CountryName Year PM25 ## 1 Afghanistan 2009 68.00000 ## 2 Afghanistan 2009 86.00000 ## 3 Afghanistan 2019 119.77360 ## 4 Albania 2015 NA ## 5 Albania 2016 14.32325 ## 6 Albania 2014 NA There is even a set of functions to help extract columns based on pattern matching, e.g. WHO_GM_Database_selectcolumns2 &lt;- select(WHO_GM_Database, starts_with(&quot;Country&quot;)) head(WHO_GM_Database_selectcolumns2) ## CountryName ## 1 Afghanistan ## 2 Afghanistan ## 3 Afghanistan ## 4 Albania ## 5 Albania ## 6 Albania Note that we can also remove columns using an operator, e.g. WHO_GM_Database_selectcolumns3 &lt;- select(WHO_GM_Database, -starts_with(&quot;SDG&quot;)) head(WHO_GM_Database_selectcolumns3) ## ISO3 CountryName Year StationID StationIDOrig ## 1 AFG Afghanistan 2009 AFG1 &lt;NA&gt; ## 2 AFG Afghanistan 2009 AFG2 &lt;NA&gt; ## 3 AFG Afghanistan 2019 AFG3 [US Diplomatic Post: Kabul] ## 4 ALB Albania 2015 ALB1 AL0205A ## 5 ALB Albania 2016 ALB1 AL0205A ## 6 ALB Albania 2014 ALB2 AL0204A ## StationIDOldDatabase City CityReverseGeocoded CityGiulia ## 1 1 Mazar-e Sharif Mazar-i-Sharif &lt;NA&gt; ## 2 1 Kabul Kabul &lt;NA&gt; ## 3 0 [Kabul] Kabul &lt;NA&gt; ## 4 1 &lt;NA&gt; Durrës &lt;NA&gt; ## 5 1 &lt;NA&gt; Durrës &lt;NA&gt; ## 6 1 &lt;NA&gt; Vlorë &lt;NA&gt; ## CityClean Longitude Latitude PM25 PM25PercCoverage PM25Grading ## 1 Mazar-e Sharif 67.11667 36.70000 68.00000 0.03835616 3 ## 2 Kabul 69.19128 34.53076 86.00000 0.03835616 3 ## 3 Kabul 69.19051 34.53581 119.77360 0.18000000 3 ## 4 Durrës 19.44920 41.31990 NA NA NA ## 5 Durrës 19.44920 41.31990 14.32325 0.75330146 1 ## 6 Vlorë 19.48620 40.40309 NA NA NA ## PM10 PM10PercCoverage PM10Grading LocationInfo ## 1 334.0000 NA 4 NA, NA ## 2 260.0000 NA 4 NA, NA ## 3 NA NA NA &lt;NA&gt; ## 4 17.6483 0.7937215 1 AL0205A, NA ## 5 24.5591 0.8385701 1 AL0205A, NA ## 6 15.2537 0.8649543 1 AL0204A, NA ## Source ## 1 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 2 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 3 OpenAQ ## 4 European Environment Agency (EEA) ## 5 European Environment Agency (EEA) ## 6 European Environment Agency (EEA) ## MonitorType MonitorTypeOrig PM25Conv UnspecifiedType ## 1 Urban urban 0 0 ## 2 Urban urban 0 0 ## 3 Unknown &lt;NA&gt; 0 1 ## 4 Traffic Traffic 1 0 ## 5 Traffic Traffic 0 0 ## 6 Background Background 1 0 ## WebLink ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 [[[&quot;EPA AirNow DOS&quot;,&quot;http://airnow.gov/index.cfm?action=airnow.global_summary&quot;]]] ## 4 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 5 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 6 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## Version WHOStatus WHORegion ## 1 2016 Release Member state EMR ## 2 2016 Release Member state EMR ## 3 2021 Release Member state EMR ## 4 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 5 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 6 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## WHOIncomeRegion GBDRegion ## 1 EMR LMI Asia, South ## 2 EMR LMI Asia, South ## 3 EMR LMI Asia, South ## 4 EUR LMI Europe, Central ## 5 EUR LMI Europe, Central ## 6 EUR LMI Europe, Central ## GBDSuperRegion TimeSince2010 ## 1 South Asia 9 ## 2 South Asia 9 ## 3 South Asia 19 ## 4 Central Europe, Eastern Europe and Central Asia 15 ## 5 Central Europe, Eastern Europe and Central Asia 16 ## 6 Central Europe, Eastern Europe and Central Asia 14 WHO_GM_Database_selectcolumns4 &lt;- select(WHO_GM_Database, -SDG1Region, -SDG2Region, SDG3Region) head(WHO_GM_Database_selectcolumns4) ## ISO3 CountryName Year StationID StationIDOrig ## 1 AFG Afghanistan 2009 AFG1 &lt;NA&gt; ## 2 AFG Afghanistan 2009 AFG2 &lt;NA&gt; ## 3 AFG Afghanistan 2019 AFG3 [US Diplomatic Post: Kabul] ## 4 ALB Albania 2015 ALB1 AL0205A ## 5 ALB Albania 2016 ALB1 AL0205A ## 6 ALB Albania 2014 ALB2 AL0204A ## StationIDOldDatabase City CityReverseGeocoded CityGiulia ## 1 1 Mazar-e Sharif Mazar-i-Sharif &lt;NA&gt; ## 2 1 Kabul Kabul &lt;NA&gt; ## 3 0 [Kabul] Kabul &lt;NA&gt; ## 4 1 &lt;NA&gt; Durrës &lt;NA&gt; ## 5 1 &lt;NA&gt; Durrës &lt;NA&gt; ## 6 1 &lt;NA&gt; Vlorë &lt;NA&gt; ## CityClean Longitude Latitude PM25 PM25PercCoverage PM25Grading ## 1 Mazar-e Sharif 67.11667 36.70000 68.00000 0.03835616 3 ## 2 Kabul 69.19128 34.53076 86.00000 0.03835616 3 ## 3 Kabul 69.19051 34.53581 119.77360 0.18000000 3 ## 4 Durrës 19.44920 41.31990 NA NA NA ## 5 Durrës 19.44920 41.31990 14.32325 0.75330146 1 ## 6 Vlorë 19.48620 40.40309 NA NA NA ## PM10 PM10PercCoverage PM10Grading LocationInfo ## 1 334.0000 NA 4 NA, NA ## 2 260.0000 NA 4 NA, NA ## 3 NA NA NA &lt;NA&gt; ## 4 17.6483 0.7937215 1 AL0205A, NA ## 5 24.5591 0.8385701 1 AL0205A, NA ## 6 15.2537 0.8649543 1 AL0204A, NA ## Source ## 1 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 2 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 3 OpenAQ ## 4 European Environment Agency (EEA) ## 5 European Environment Agency (EEA) ## 6 European Environment Agency (EEA) ## MonitorType MonitorTypeOrig PM25Conv UnspecifiedType ## 1 Urban urban 0 0 ## 2 Urban urban 0 0 ## 3 Unknown &lt;NA&gt; 0 1 ## 4 Traffic Traffic 1 0 ## 5 Traffic Traffic 0 0 ## 6 Background Background 1 0 ## WebLink ## 1 &lt;NA&gt; ## 2 &lt;NA&gt; ## 3 [[[&quot;EPA AirNow DOS&quot;,&quot;http://airnow.gov/index.cfm?action=airnow.global_summary&quot;]]] ## 4 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 5 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 6 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## Version WHOStatus WHORegion ## 1 2016 Release Member state EMR ## 2 2016 Release Member state EMR ## 3 2021 Release Member state EMR ## 4 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 5 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 6 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## WHOIncomeRegion SDG3Region GBDRegion ## 1 EMR LMI Southern Asia Asia, South ## 2 EMR LMI Southern Asia Asia, South ## 3 EMR LMI Southern Asia Asia, South ## 4 EUR LMI Southern Europe Europe, Central ## 5 EUR LMI Southern Europe Europe, Central ## 6 EUR LMI Southern Europe Europe, Central ## GBDSuperRegion TimeSince2010 ## 1 South Asia 9 ## 2 South Asia 9 ## 3 South Asia 19 ## 4 Central Europe, Eastern Europe and Central Asia 15 ## 5 Central Europe, Eastern Europe and Central Asia 16 ## 6 Central Europe, Eastern Europe and Central Asia 14 Adding columns Finally, let’s add a new column representing the different between and 2000 as before, but using the . To avoid over-riding the first version we will call it and for clarity will only show the first three rows. WHO_GM_Database$Year &lt;- as.numeric(WHO_GM_Database$Year) mutate(WHO_GM_Database, TimeSince2010_tidy = Year - 2000)[1:2,] ## ISO3 CountryName Year StationID StationIDOrig StationIDOldDatabase ## 1 AFG Afghanistan 2009 AFG1 &lt;NA&gt; 1 ## 2 AFG Afghanistan 2009 AFG2 &lt;NA&gt; 1 ## City CityReverseGeocoded CityGiulia CityClean Longitude ## 1 Mazar-e Sharif Mazar-i-Sharif &lt;NA&gt; Mazar-e Sharif 67.11667 ## 2 Kabul Kabul &lt;NA&gt; Kabul 69.19128 ## Latitude PM25 PM25PercCoverage PM25Grading PM10 PM10PercCoverage PM10Grading ## 1 36.70000 68 0.03835616 3 334 NA 4 ## 2 34.53076 86 0.03835616 3 260 NA 4 ## LocationInfo ## 1 NA, NA ## 2 NA, NA ## Source ## 1 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## 2 Magnusson et al., Broad Exposure Screening of Air Pollutants in the Occupational Environment of Swedish Soldiers Deployed in Afghanistan; MILITARY MEDICINE, 177, 3:318, 2012 ## MonitorType MonitorTypeOrig PM25Conv UnspecifiedType WebLink Version ## 1 Urban urban 0 0 &lt;NA&gt; 2016 Release ## 2 Urban urban 0 0 &lt;NA&gt; 2016 Release ## WHOStatus WHORegion WHOIncomeRegion SDG1Region ## 1 Member state EMR EMR LMI Central Asia and Southern Asia ## 2 Member state EMR EMR LMI Central Asia and Southern Asia ## SDG2Region SDG3Region GBDRegion GBDSuperRegion TimeSince2010 ## 1 Southern Asia Southern Asia Asia, South South Asia 9 ## 2 Southern Asia Southern Asia Asia, South South Asia 9 ## TimeSince2010_tidy ## 1 9 ## 2 9 Pipes Piping comes from Unix scripting, and simply means a chain of commands, such that the results from each command feed into the next one. It can be helpful in making code more succinct, and uses the pipe operator %&gt;% to chain functions together. For example, the following will filter the dataframe to extract rows when the year is 2019 and then how the first 6 rows using the head function. WHO_GM_Database %&gt;% filter(Year == 2019) %&gt;% head() ## ISO3 CountryName Year StationID StationIDOrig ## 1 AFG Afghanistan 2019 AFG3 [US Diplomatic Post: Kabul] ## 2 ALB Albania 2019 ALB2 AL0204A ## 3 ARE United Arab Emirates 2019 ARE22 Hamdan Street ## 4 ARE United Arab Emirates 2019 ARE22 Khadejah School ## 5 ARE United Arab Emirates 2019 ARE22 Khalifa School ## 6 ARE United Arab Emirates 2019 ARE22 Al Maqta ## StationIDOldDatabase City CityReverseGeocoded CityGiulia CityClean ## 1 0 [Kabul] Kabul &lt;NA&gt; Kabul ## 2 1 &lt;NA&gt; Vlorë &lt;NA&gt; Vlorë ## 3 0 Abu dhabi Abu Dhabi &lt;NA&gt; Abu dhabi ## 4 0 Abu dhabi Abu Dhabi &lt;NA&gt; Abu dhabi ## 5 0 Abu dhabi Abu Dhabi &lt;NA&gt; Abu dhabi ## 6 0 Abu dhabi Abu Dhabi &lt;NA&gt; Abu dhabi ## Longitude Latitude PM25 PM25PercCoverage PM25Grading PM10 ## 1 69.19051 34.53581 119.77360 0.1800000 3 NA ## 2 19.48620 40.40309 10.31525 0.7510274 1 NA ## 3 54.37730 24.45390 NA NA NA 100.00 ## 4 54.37730 24.45390 NA NA NA 102.29 ## 5 54.37730 24.45390 NA NA NA 104.69 ## 6 54.37730 24.45390 NA NA NA 112.58 ## PM10PercCoverage PM10Grading LocationInfo ## 1 NA NA &lt;NA&gt; ## 2 NA NA AL0204A, NA ## 3 NA 4 Hamdan Street, Abu dhabi, United Arab Emirates ## 4 NA 4 Khadejah School, Abu dhabi, United Arab Emirates ## 5 NA 4 Khalifa School, Abu dhabi, United Arab Emirates ## 6 NA 4 Al Maqta, Abu dhabi, United Arab Emirates ## Source MonitorType ## 1 OpenAQ Unknown ## 2 European Environment Agency (EEA) Background ## 3 Statistics Center - Abu Dhabi from Environment Agency - Abu Dhabi Urban ## 4 Statistics Center - Abu Dhabi from Environment Agency - Abu Dhabi Urban ## 5 Statistics Center - Abu Dhabi from Environment Agency - Abu Dhabi Urban ## 6 Statistics Center - Abu Dhabi from Environment Agency - Abu Dhabi Urban ## MonitorTypeOrig PM25Conv UnspecifiedType ## 1 &lt;NA&gt; 0 1 ## 2 Background 0 0 ## 3 traffic 1 0 ## 4 down town 1 0 ## 5 Urban/Residential 1 0 ## 6 Urban/Residential 1 0 ## WebLink ## 1 [[[&quot;EPA AirNow DOS&quot;,&quot;http://airnow.gov/index.cfm?action=airnow.global_summary&quot;]]] ## 2 https://www.eea.europa.eu/data-and-maps/Data/aqereporting-2 ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; ## Version WHOStatus WHORegion ## 1 2021 Release Member state EMR ## 2 Redownloaded for 2021 Release, replacing 2018 Release Member state EUR ## 3 2021 Release Member state EMR ## 4 2021 Release Member state EMR ## 5 2021 Release Member state EMR ## 6 2021 Release Member state EMR ## WHOIncomeRegion SDG1Region SDG2Region ## 1 EMR LMI Central Asia and Southern Asia Southern Asia ## 2 EUR LMI Northern America and Europe Europe ## 3 EMR HI Western Asia and Northern Africa Western Asia ## 4 EMR HI Western Asia and Northern Africa Western Asia ## 5 EMR HI Western Asia and Northern Africa Western Asia ## 6 EMR HI Western Asia and Northern Africa Western Asia ## SDG3Region GBDRegion ## 1 Southern Asia Asia, South ## 2 Southern Europe Europe, Central ## 3 Western Asia North Africa / Middle East ## 4 Western Asia North Africa / Middle East ## 5 Western Asia North Africa / Middle East ## 6 Western Asia North Africa / Middle East ## GBDSuperRegion TimeSince2010 ## 1 South Asia 19 ## 2 Central Europe, Eastern Europe and Central Asia 19 ## 3 North Africa / Middle East 19 ## 4 North Africa / Middle East 19 ## 5 North Africa / Middle East 19 ## 6 North Africa / Middle East 19 Chaining pipes Pipes can be chained together multiple times. For example: WHO_GM_Database$Year &lt;- as.numeric(WHO_GM_Database$Year) WHO_GM_Database %&gt;% filter(Year == 2019) %&gt;% select(CountryName, Year, PM25, -starts_with(&quot;SDG&quot;)) %&gt;% mutate(TimeSince2010_tidy = Year - 2000) %&gt;% arrange(CountryName, Year) %&gt;% head() ## CountryName Year PM25 TimeSince2010_tidy ## 1 Afghanistan 2019 119.77360 19 ## 2 Albania 2019 10.31525 19 ## 3 Algeria 2019 21.53338 19 ## 4 Australia 2019 NA 19 ## 5 Australia 2019 NA 19 ## 6 Australia 2019 NA 19 Grouping and summarizing A common thing we might want to do is to produce summaries of some variable for different subsets of the data. For example, we might want the mean values of for each . The dplyr package (within ) provides a function group_by() that allows us to group data, and summarize() that allows us to summarize data. In this case, we can think of what we want to do as the data by and then averaging the values within each group. Note that there are missing values in as in some locations only is measured, and vice-versa. We use to exclude missing values when calculating the mean. WHO_GM_Database %&gt;% group_by(CountryName) %&gt;% summarize(mn = mean(PM25, na.rm=TRUE)) ## # A tibble: 127 × 2 ## CountryName mn ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 91.3 ## 2 Albania 22.3 ## 3 Algeria 21.5 ## 4 Andorra 11.0 ## 5 Argentina 10.2 ## 6 Australia 8.11 ## 7 Austria 14.2 ## 8 Bahamas 4.16 ## 9 Bahrain 55.4 ## 10 Bangladesh 76.8 ## # ℹ 117 more rows Summarize The summarize() function applies a function to a dataframe or subsets of a data frame. For example, we can produce a table of estimates for the mean and variance of both PM25 lengths and PM10, within each CountryName. WHO_GM_Database %&gt;% group_by(CountryName) %&gt;% summarize(MeanPM25 = mean(PM25, na.rm=TRUE), MeanPM10 = mean(PM10, na.rm=TRUE), VarPM25 = var(PM25, na.rm=TRUE), VarPM10 = var(PM10, na.rm=TRUE)) ## # A tibble: 127 × 5 ## CountryName MeanPM25 MeanPM10 VarPM25 VarPM10 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 91.3 297 691. 2738 ## 2 Albania 22.3 36.4 77.9 655. ## 3 Algeria 21.5 NaN NA NA ## 4 Andorra 11.0 19.2 0.276 1.09 ## 5 Argentina 10.2 27.4 0.0103 1.45 ## 6 Australia 8.11 17.7 54.2 58.4 ## 7 Austria 14.2 20.4 11.2 25.8 ## 8 Bahamas 4.16 4.65 1.04 1.19 ## 9 Bahrain 55.4 176. 103. 4844. ## 10 Bangladesh 76.8 142. 690. 1784. ## # ℹ 117 more rows Example 4.1: Health impacts associated with outdoor air pollution We now demonstrate how using dataframes and the can allow us to perform a health impact analysis of air pollution very efficiently. We will be calculating the annual number of deaths attributable to PM\\(_{2.5}\\). We wish to estimate the annual number of deaths attributable to PM\\(_{2.5}\\) air pollution. In order to do this, we need a relative risk (RR), the population at risk for the areas of interest, the overall mortality rate (OMR), and a baseline value for air pollution (for which there is no associated increase in risk). In this example, we use a RR of 1.06 per 10\\(\\mu gm^{-3}\\), the population at risk is 1 million and an overall mortality rate of 80 per 10000. We first enter this information into by assigning these values to a series of variables. We first enter this information into R by assigning these values to a series of variables. # Relative Risk RR &lt;- 1.06 # Size of population Population &lt;- 1000000 # Unit for the Relative Risk RR_unit &lt;- 10 # Overall mortality count, used for calculating the overall mortality rate OMR_count &lt;- 80 # Denominator (population at risk), used for calculating the overall mortality rate. OMR_pop &lt;- 10000 # Mortality rate OMR = OMR_count/OMR_pop OMR ## [1] 0.008 # Baseline value of PM2.5 for which there is no increased risk baseline &lt;- 5 # Population attributable fraction #PAF = (Proportion of population exposed*(RR-1))/(Proportion of population exposed*(RR-1)+1). #In this case the proportion of the population exposed is one. PAF = (RR-1)/RR PAF ## [1] 0.05660377 In this example, we will calculate the attributable deaths for increments of 10\\(\\mu gm^{-3}\\), however the following code is general and will work for any increments. # PM2.5 categories PM2.5.cats &lt;- c(5,15,25,35,45,55,65,75,85,95,105) # Create a dataframe containing the PM2.5 categoriess Impacts &lt;- data.frame(PM2.5.cats) We now calculate the increases in risk for each category of PM\\(_{2.5}\\). For each category, we find the increase in risk compared to the baseline. For the second category, with PM\\(_{2.5}\\) = 15, the risk will be 1.06 (the original RR) as this is 10\\(\\mu gm^{-3}\\) (one unit) greater than the baseline. For the next category, PM\\(_{2.5}\\) is 10\\(\\mu\\)gm\\(^{-3}\\) higher than the previous category (one unit in terms of the RR) and so the risk in that category again be increased by a factor of 1.06 (on that of the previous category). In this case, the relative risk (with respect to baseline) is therefore . For the next category, PM\\(_{2.5}\\) = 25 which is again 10\\(\\mu\\)gm\\(^{-3}\\) (one unit in terms of the RR) higher, and so the relative risk is 1.06 multiplies by the previous value, i.e.~. We can calculate the relative risks for each category (relative to baseline) in . For each category, we find the number of units from baseline and repeatedly multiply the RR by this number. This is equivalent to raising the RR to the power of (Category-Baseline)/Units, e.g. \\[\\mbox{RR}^{\\left( \\frac{\\mbox{Category-Baseline}}{\\mbox{Units}}\\right)}\\] We add another column to the Impacts dataframe containing these values. # Calculating Relative Risks Impacts &lt;- mutate(Impacts, RR = RR^((Impacts$PM2.5.cats - baseline)/RR_unit)) Once we have the RR for each pollution level, we can calculate the rate for each category. This is found by applying the risks to the overall rate. Again, we add these numbers to the Impacts dataframe as an additional column using . To use this function, we need to add columns which contains replications of the and . # Create an additional column containing replication of the OMR Impacts$OMR &lt;- rep(OMR, nrow(Impacts)) Impacts$Population &lt;- rep(Population, nrow(Impacts)) # Calculating the rates in each category Impacts &lt;- mutate(Impacts, Rate = RR * OMR) # Add the PAFs for each category Impacts &lt;- mutate(Impacts, PAF = RR * (RR-1)/RR) # Add the number of (expected) deaths per year for each category Impacts &lt;- mutate(Impacts, DeathsPerYear = Rate * Population) For each category, we need to calculate the extra deaths (with reference to the overall rate). The number of deaths for the reference category is the first number in the column. # The reference number of deaths Impacts$DeathsPerYear[1] ## [1] 8000 # make into a vector by using the rep (replicate) function and add to the dataframe Impacts$ReferenceDeaths &lt;- rep(Impacts$DeathsPerYear[1], nrow(Impacts)) # We can then calculate the excess numbers of deaths for each category Impacts &lt;- mutate(Impacts, ExtraDeaths = DeathsPerYear - ReferenceDeaths) For each category, we then want to calculate the number of deaths gained. These are the difference between the values in each category. We can find these using the function. This will produce a set of differences for which the length is one less than the number of rows in our Impacts dataframe. We need to add a zero to this to ensure that they line up when we add them as another column. # Calculate the number of deaths gained diff(Impacts$ExtraDeaths) ## [1] 480.0000 508.8000 539.3280 571.6877 605.9889 642.3483 680.8892 721.7425 ## [9] 765.0471 810.9499 # We can now add these gains to the main Impacts dataframe Impacts$Gain &lt;- c(0,diff(Impacts$ExtraDeaths)) # Show the results Impacts ## PM2.5.cats RR OMR Population Rate PAF DeathsPerYear ## 1 5 1.000000 0.008 1e+06 0.008000000 0.0000000 8000.000 ## 2 15 1.060000 0.008 1e+06 0.008480000 0.0600000 8480.000 ## 3 25 1.123600 0.008 1e+06 0.008988800 0.1236000 8988.800 ## 4 35 1.191016 0.008 1e+06 0.009528128 0.1910160 9528.128 ## 5 45 1.262477 0.008 1e+06 0.010099816 0.2624770 10099.816 ## 6 55 1.338226 0.008 1e+06 0.010705805 0.3382256 10705.805 ## 7 65 1.418519 0.008 1e+06 0.011348153 0.4185191 11348.153 ## 8 75 1.503630 0.008 1e+06 0.012029042 0.5036303 12029.042 ## 9 85 1.593848 0.008 1e+06 0.012750785 0.5938481 12750.785 ## 10 95 1.689479 0.008 1e+06 0.013515832 0.6894790 13515.832 ## 11 105 1.790848 0.008 1e+06 0.014326782 0.7908477 14326.782 ## ReferenceDeaths ExtraDeaths Gain ## 1 8000 0.000 0.0000 ## 2 8000 480.000 480.0000 ## 3 8000 988.800 508.8000 ## 4 8000 1528.128 539.3280 ## 5 8000 2099.816 571.6877 ## 6 8000 2705.805 605.9889 ## 7 8000 3348.153 642.3483 ## 8 8000 4029.042 680.8892 ## 9 8000 4750.785 721.7425 ## 10 8000 5515.832 765.0471 ## 11 8000 6326.782 810.9499 Example 4.3. Mapping cancer incidence in Southern and Eastern Serbia In this example, we will see how to use to create maps and then map the values of data within a dataframe. We will create a map of South and East Serbia. and creating expected number of cases and SIRs of cancer in City of Bor. To create maps, we use something called `shapefiles’. Shapefiles contain location, shape, and attributes of geographic features such as country borders. The files SE_Serbia.shp, and SE_Serbia.dbf contain the location, shape, and attributes of South and East Serbia by district. These were obtained from . On this website you can download administrative boundaries for almost every country in the world. We will use the following files: shapefiles and information for South and East Serbia split by administrative district (, ) population counts and density for South and East Serbia split by administrative district () population counts and incidence rates of all cancers, by age group and sex in City of Bor (), observed counts of all cancers cancer, by age group and sex in City of Bor () These need to be in the working directory, which can be set using the function. For this example, we need the following packages: : Package to use spatial objects. : Package to use spatial objects. : Package to load and manipulate spatial data. : Package to fit spatial GLMMs which contains some useful functions for manipulating spatial data : Package to give scaled colours for plots. : Package to work with rasters. : Package to plot maps. Use the function or the packages window in the bottom right pane of RStudio to download and install the packages that we need. We use the function to load the required packages into the library. # Loading required packages library(spdep) ## Loading required package: sp ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` ## Loading required package: sf ## Linking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE library(shapefiles) ## Loading required package: foreign ## ## Attaching package: &#39;shapefiles&#39; ## The following objects are masked from &#39;package:foreign&#39;: ## ## read.dbf, write.dbf library(sp) library(CARBayes) ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## Loading required package: Rcpp ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 library(rgdal) ## Please note that rgdal will be retired during 2023, ## plan transition to sf/stars/terra functions using GDAL and PROJ ## at your earliest convenience. ## See https://r-spatial.org/r/2022/04/12/evolution.html and https://github.com/r-spatial/evolution ## rgdal: version: 1.6-2, (SVN revision 1183) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.5.2, released 2022/09/02 ## Path to GDAL shared files: C:/Users/saraz/AppData/Local/R/win-library/4.2/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821] ## Path to PROJ shared files: C:/Users/saraz/AppData/Local/R/win-library/4.2/rgdal/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.5-1 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading sp or rgdal. library(RColorBrewer) library(raster) ## ## Attaching package: &#39;raster&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select library(rworldmap) ## Warning: package &#39;rworldmap&#39; was built under R version 4.2.3 ## Please note that &#39;maptools&#39; will be retired during October 2023, ## plan transition at your earliest convenience (see ## https://r-spatial.org/r/2023/05/15/evolution4.html and earlier blogs ## for guidance);some functionality will be moved to &#39;sp&#39;. ## Checking rgeos availability: TRUE ## ### Welcome to rworldmap ### ## For a short introduction type : vignette(&#39;rworldmap&#39;) # a function from a previous version of CARBayes that we use here source(&quot;./functions/combine.data.shapefile.R&quot;) Creating maps of Southern and Eastern Serbia To create maps, we use something called `shapefiles’. Shapefiles contain location, shape, and attributes of geographic features such as country borders. The files , and contain the location, shape, and attributes of South and East Serbia by district. These were obtained from . The functions and will read these shapefiles into . # Reading in borders shp_Serbia &lt;- read.shp(shp.name = &quot;./data/SE_Serbia.shp&quot;) dbf_Serbia &lt;- read.dbf(dbf.name = &quot;./data/SE_Serbia.dbf&quot;) # Read population data for Serbia pop_Serbia &lt;- read.csv(&#39;./data/SE_Serbia.csv&#39;) To check that the data has been read into correctly, we can use the and function, which prints the first six rows of a dataset. # Printing first six rows head(pop_Serbia) ## CountryName ISO3 District Area_KM2 Pop_2011 Pop_Per_KM2_2011 ## 1 Serbia SRB Bor 3510 124992 35.61026 ## 2 Serbia SRB Branicevo 3865 183625 47.50970 ## 3 Serbia SRB Jablanica 2770 216304 78.08809 ## 4 Serbia SRB Nisava 2727 376319 137.99743 ## 5 Serbia SRB Pcinja 3520 159081 45.19347 ## 6 Serbia SRB Pirot 2761 92479 33.49475 # Combining population data and the shapefile Serbia &lt;- combine.data.shapefile ( data = pop_Serbia , #Dataset to attach shp = shp_Serbia,#Shapefile dbf = dbf_Serbia ) #Database file # Scaling population counts ( to 1000 s ) Serbia $ Pop_2011 &lt;- Serbia$Pop_2011 / 1000 # Creating map of population counts in Serbia spplot (obj = Serbia , # Spatial object to be plotted zcol = c ( &quot;Pop_2011&quot; ) , # Choice of the column the object you are plotting . main = &quot; Population ( in 1000 s ) &quot; , # Plot title at= seq (0 ,400 , length.out =20), # Break points for legend col = &#39;black&#39;, # Colour for borders col.regions = colorRampPalette(brewer.pal(9, &#39;Blues&#39;))(20)) # Create a set of colours Example 4.4 Cancer in Bor We will work through an example of creating expected counts and a standarized morbidity ratio (SMR) using data on all types of cancer (except skin) in the Muncipality of Bor, Serbia between 2001 and 2015. ###Expected Numbers {-} In order to calculate SMRs, we need to estimate the number of cases we expect in Bor per year, based on their age-sex profiles. To calculate expected numbers, we use indirect standardization. For indirect standardization, we take the age and sex-specific rates from the reference population (in this example, Serbia) and convert them into the mortality rate we would observe if those reference rates were true for the age and sex-structure of the population of interest (in this example, Bor). Therefore, we require population counts by age and sex for Bor the age and sex-specific incidence rates for cancer in Serbia. The file contain the populations and incidence rates required by age and sex. These are in csv format, so we use the function. # Reading in observed numbers of hospital admissions in England by local authority Bor_Rates &lt;- read.csv(file=&quot;./data/Bor_Rates.csv&quot;) To check that the data has been read into correctly, and to familiarise ourselves with the data, we can summarize it using the function. This will allow us to check for anomalies in our data.\\ # Summarising first six rows of the Rates and populations of Bor summary(Bor_Rates) ## City Sex AgeGroup Pop ## Length:32 Length:32 Length:32 Min. : 506.0 ## Class :character Class :character Class :character 1st Qu.: 853.5 ## Mode :character Mode :character Mode :character Median :1101.0 ## Mean :1067.5 ## 3rd Qu.:1232.0 ## Max. :1482.0 ## Incidence_Serbia Incidence_SE_Serbia ## Min. :0.0001243 Min. :0.0001088 ## 1st Qu.:0.0002873 1st Qu.:0.0002470 ## Median :0.0015951 Median :0.0015305 ## Mean :0.0044163 Mean :0.0039337 ## 3rd Qu.:0.0080899 3rd Qu.:0.0072949 ## Max. :0.0173886 Max. :0.0149553 We can see that has the following variables: - Name of City, - Sex category, - Age categories, in 5-yearly groups, - Population count, - Incidence rates of cancer in per year - Incidence rates of cancer in per year Now that we have read in the population and the incidence rates required, we calculate the expected number as follows \\[E = \\sum_{k}N_k \\times r_k\\] where \\(r_k\\) are the age- and sex-specific rates of obtaining cancer in Serbia and \\(N_k\\) are the population counts by age and sex in Bor. In we can calculate each of the \\(N_k \\times r_k\\) by multiplying the columns containing the Serbian incidence rates and the population profile of Bor. We add another column to the dataframe containing these values. Remember that to extract and assign columns in a dataframe we use the operator. # Calculating the expected number by Settlement, age and sex # using the Serbian incidence rates and Bor population profiles Bor_Rates$Expected &lt;- Bor_Rates$Incidence_Serbia * Bor_Rates$Pop Calculating SMRs The observed number of cases of cancer by sex in the Municipality of Bor need to be read into . These are in csv format, so we use the function. # Reading in observed numbers of hospital admissions in England by local authority Bor_Observed &lt;- read.csv(file=&quot;./data/Bor_Observed.csv&quot;) ## Warning in read.table(file = file, header = header, sep = sep, quote = quote, : ## incomplete final line found by readTableHeader on &#39;./data/Bor_Observed.csv&#39; To check that the data has been read into correctly, we can use the function, which prints the first six rows of a dataset. # Printing first six rows of the observed counts head(Bor_Observed) ## City Sex Observed ## 1 Bor Males 1581 ## 2 Bor Females 1540 We can see that has the following variables: - Name of City, - Sex category, - Observed number of cases of cancer between 2001 and 2015 # Summing all expected cases by Settlement and Sex Bor_Expected &lt;- aggregate(Expected ~ City + Sex, # Variable to sum over ~ Variables to Stratify by data = Bor_Rates, # Dataset name sum) # Function to summarize over Remember, we calculated expected numbers for one specific year, whereas the observed counts are over 10 years (2001-2015). For the SMRs, we assume that the population remains the same across that time period and multiply the expected cases by 15. # Multiplying the number of cases by 15 to obtain expected cases between 2001 and 2015 Bor_Expected$Expected &lt;- 15 * Bor_Expected$Expected To compare the observed and expected counts, we need to merge the two datasets and together. We do this using the function. # Merging files together Bor &lt;- merge(Bor_Expected, # First file to merge Bor_Observed, # Second file to merge by = c(&#39;City&#39;,&#39;Sex&#39;)) # Variables to merge Now that we have observed and expected numbers of cancer cases, we can calculate raw SMRs. Remember that \\[ \\mbox{SMR} = \\frac{\\mbox{observed}}{\\mbox{expected}} \\] We add another column to the dataframe containing these values. # Calculating SMR by sex Bor$SMR &lt;- Bor$Observed / Bor$Expected # Printing the results Bor ## City Sex Expected Observed SMR ## 1 Bor Females 1024.503 1540 1.503167 ## 2 Bor Males 1071.878 1581 1.474982 "],["embracing.html", "Chapter 5 Embracing uncertainty: the Bayesian approach", " Chapter 5 Embracing uncertainty: the Bayesian approach This chapter introduces the Bayesian approach, which provides a natural framework for dealing with uncertainty and also for fitting the models that will be encountered later in the book. The reader will have gained an understanding of the following topics: use of prior distributions to capture beliefs before data are observed; combining prior beliefs and information from data to obtain posterior beliefs; manipulation of prior distributions with likelihoods to formulate posterior distributions and why conjugate priors are useful in this regard; the differences between informative and non-informative priors; use of the posterior distribution for inference and methods for calculating summary measures. "],["computation.html", "Chapter 6 Approaches to Bayesian Computation Example 6.2: Gibbs sampling with a Poisson-Gamma model - hospital admissions for chronic obstructive pulmonary disease (COPD) for England between 2001-2010 Code for the Poisson-Gamma model in Nimble Example 6.3: Fitting a Poisson regression model", " Chapter 6 Approaches to Bayesian Computation This chapter describes methods for implementing Bayesian models when their complexity means that simple, analytic solutions may not be available. The reader will have gained an understanding of the following topics: analytical approximations to the posterior distribution; use of samples from a posterior distribution for inference and Monte Carlo integration; methods for direct sampling such as importance and rejection sampling; Markov Chain Monte Carlo (MCMC) and methods for obtaining samples from the required posterior distribution including Metropolis–Hastings and Gibbs algorithms; use of NIMBLE and RStan packages to fit Bayesian models using Gibbs sampling; Integrated Nested Laplace Approximations (INLA) as a method for performing efficient Bayesian inference including the use of R-INLA to implement a wide variety of latent Gaussian models. Example 6.2: Gibbs sampling with a Poisson-Gamma model - hospital admissions for chronic obstructive pulmonary disease (COPD) for England between 2001-2010 We now look at example into the hospital admission rates for chronic obstructive pulmonary disease (COPD) in England between 2001–2010. In England, there are 324 local authority administrative areas each with an observed and expected number of cases. The expected numbers were calculated using indirect standardization by applying the age–sex specific rates for the whole of England to the age–sex population profile of each of the areas. We show the full conditional distributions for a Poisson model where the prior distribution on the rates is Gamma, that is, \\(y_i \\mid \\theta_, E_i \\sim Poi(\\theta_i E_i)\\) where \\(E_i\\) is the expected number of cases in area \\(i\\). In particular, let \\(\\mathbf{y} = (y_1, ..., y_N)\\) be a \\(N\\)-dimensional vector with observed counts on the total number of hospital admissions for chronic obstructive pulmonary disease (COPD) for England between 2001 and 2010. The expected numbers of hospital admissions by local authority were computed as described in Chapter 2, Section 2.1. If we assign a Gamma prior to the random effects \\(\\theta_i\\), i.e., \\(\\theta_i \\sim Gamma(a, b)\\) for \\(i=1, \\dots N\\), and independent exponential distributions to the hyperparameters \\(a\\) and \\(b\\) then we can find the full conditional distributions required for Gibbs sampling. The joint posterior is proportional to \\[\\begin{eqnarray} \\nonumber p(a, b, \\mathbf{\\theta} | \\mathbf{y}) &amp;\\propto&amp; \\prod^N _{i=1} \\frac{\\left(\\theta_i E_i\\right)^{y_i}}{y_i!} \\exp(-\\theta_i E_i) \\, \\frac{b^{a}}{\\Gamma (a) }\\theta_i^{a-1} e^{-b \\theta_i} \\\\ \\nonumber &amp;&amp; \\times \\lambda_{a} \\exp(-\\lambda_{a}) \\, \\lambda_{b}\\exp(-\\lambda_{b}), \\end{eqnarray}\\] and the full conditionals are: Posterior full conditional for each \\(\\theta_i\\), \\(i=1,\\dots,N\\): \\[\\begin{eqnarray} p(\\theta_i | \\mathbf{\\theta}_{-i}, a, b, \\mathbf{y}) \\propto \\theta_i^{y_i+a-1} \\exp[-(E_i+b)\\theta_i], \\end{eqnarray}\\] which is the kernel of a Gamma distribution with parameters \\(y_i+a\\) and \\(E_i+b\\); Posterior full conditional for \\(b\\): \\[\\begin{eqnarray} p(b | \\mathbf{\\theta}, a, \\mathbf{y}) \\propto b^{N a} \\exp\\left[-\\left(\\sum^N_{i=1}\\theta_i + \\lambda_{b}\\right)b\\right], \\end{eqnarray}\\] which is the kernel of a Gamma distribution with parameters \\(Na+1\\) and \\((\\sum^N_{i=1}\\theta_i + \\lambda_{b})\\); Posterior full conditional for \\(a\\): \\[\\begin{eqnarray} p(a | \\mathbf{\\theta}, b, \\mathbf{y}) \\propto \\frac{\\left(b^N \\prod^N_{i=1}\\theta_i\\right)^{a-1}}{\\Gamma (a)^N }, \\end{eqnarray}\\] which does not have a closed form. We propose to sample from \\(p(a | \\mathbf{\\theta}, b, \\mathbf{y})\\) through a random walk, Metropolis-Hastings step. As \\(a\\) must be strictly positive, the proposal is a log-normal distribution whose associated normal distribution has mean at the logarithm of the current value and some fixed variance, say \\(u\\), that needs to be tuned. For this example, the following packages are needed ggplot2 and sf. Load the necessary packages. library(ggplot2) library(sf) To create SMR maps, we need to load the relevant shapefiles into the R session. englandlocalauthority.shp and englandlocalauthority.dbf contain the location, shape, and attributes of English local authorities. The function read_sf() from the sf package will read these shapefiles into R. copdmortalityobserved.csv contains the observed number of hospital admissions in England by local authority. You can find this data by clicking. copdmortalityexpected.csv contains the expected number of hospital admissions in England by local authority. # Reading in the borders of the shape file england &lt;- read_sf(&quot;data/copd/englandlocalauthority.shp&quot;) # Reading the data observed &lt;-read.csv(file = &quot;data/copd/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;-read.csv(file = &quot;data/copd/copdmortalityexpected.csv&quot;, row.names = 1) Print summaries of the observed and expected counts. # Printing first six rows of the observed counts head(observed) ## name Y2001 Y2002 Y2003 Y2004 Y2005 Y2006 Y2007 Y2008 ## 00AA City of London LB 2 0 3 1 1 1 5 1 ## 00AB Barking and Dagenham LB 100 100 122 93 136 97 91 96 ## 00AC Barnet LB 110 102 106 89 99 97 72 84 ## 00AD Bexley LB 109 113 113 96 113 97 94 89 ## 00AE Brent LB 69 89 70 59 61 48 53 46 ## 00AF Bromley LB 120 129 135 124 128 117 120 106 ## Y2009 Y2010 ## 00AA 0 1 ## 00AB 101 78 ## 00AC 78 89 ## 00AD 93 93 ## 00AE 55 43 ## 00AF 107 113 # Printing first six rows of the expected counts head(expected) ## E2001 E2002 E2003 E2004 E2005 E2006 ## 00AA 2.648915 2.68106 2.727112 2.749562 2.808655 2.915977 ## 00AB 63.946730 63.41700 62.567863 61.444884 60.677119 59.678672 ## 00AC 121.795213 121.91534 122.451050 123.201898 124.449563 125.982868 ## 00AD 90.201336 91.24645 91.949050 92.754781 93.674540 94.598593 ## 00AE 76.876437 77.18529 78.017980 78.967493 80.422828 81.785325 ## 00AF 131.182934 132.30521 133.257442 134.520920 136.441229 137.382528 ## E2007 E2008 E2009 E2010 ## 00AA 3.021586 3.114696 3.237998 3.237998 ## 00AB 58.487583 57.701932 57.250524 57.250524 ## 00AC 127.088805 128.825149 131.374946 131.374946 ## 00AD 95.447131 96.832061 97.651369 97.651369 ## 00AE 83.651266 85.265264 87.089119 87.089119 ## 00AF 138.634021 139.508507 140.634084 140.634084 # Summarising the observed counts summary(observed) ## name Y2001 Y2002 Y2003 ## Length:324 Min. : 2.00 Min. : 0.00 Min. : 3.00 ## Class :character 1st Qu.: 35.00 1st Qu.: 38.00 1st Qu.: 38.00 ## Mode :character Median : 50.00 Median : 52.00 Median : 52.00 ## Mean : 68.01 Mean : 69.63 Mean : 73.44 ## 3rd Qu.: 83.50 3rd Qu.: 80.75 3rd Qu.: 83.25 ## Max. :445.00 Max. :438.00 Max. :480.00 ## Y2004 Y2005 Y2006 Y2007 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 5.00 ## 1st Qu.: 35.00 1st Qu.: 37.00 1st Qu.: 35.00 1st Qu.: 37.00 ## Median : 49.50 Median : 51.00 Median : 49.00 Median : 50.00 ## Mean : 66.67 Mean : 69.37 Mean : 67.07 Mean : 68.17 ## 3rd Qu.: 81.25 3rd Qu.: 80.50 3rd Qu.: 81.00 3rd Qu.: 79.00 ## Max. :428.00 Max. :395.00 Max. :428.00 Max. :456.00 ## Y2008 Y2009 Y2010 ## Min. : 1.00 Min. : 0.00 Min. : 1.00 ## 1st Qu.: 37.00 1st Qu.: 36.00 1st Qu.: 38.00 ## Median : 51.00 Median : 50.00 Median : 51.00 ## Mean : 71.40 Mean : 67.04 Mean : 68.81 ## 3rd Qu.: 84.25 3rd Qu.: 78.00 3rd Qu.: 81.25 ## Max. :463.00 Max. :394.00 Max. :441.00 # Summarising the expected counts summary(expected) ## E2001 E2002 E2003 E2004 ## Min. : 2.649 Min. : 2.681 Min. : 2.727 Min. : 2.75 ## 1st Qu.: 39.066 1st Qu.: 39.456 1st Qu.: 39.849 1st Qu.: 40.60 ## Median : 51.766 Median : 52.671 Median : 53.487 Median : 54.29 ## Mean : 62.944 Mean : 63.589 Mean : 64.139 Mean : 64.72 ## 3rd Qu.: 74.292 3rd Qu.: 74.974 3rd Qu.: 74.701 3rd Qu.: 74.02 ## Max. :370.913 Max. :371.271 Max. :369.861 Max. :368.87 ## E2005 E2006 E2007 E2008 ## Min. : 2.809 Min. : 2.916 Min. : 3.022 Min. : 3.115 ## 1st Qu.: 41.646 1st Qu.: 42.497 1st Qu.: 43.203 1st Qu.: 44.262 ## Median : 54.765 Median : 55.506 Median : 56.552 Median : 57.522 ## Mean : 65.440 Mean : 66.180 Mean : 67.022 Mean : 67.950 ## 3rd Qu.: 75.003 3rd Qu.: 75.260 3rd Qu.: 75.790 3rd Qu.: 76.935 ## Max. :368.565 Max. :367.838 Max. :368.026 Max. :368.291 ## E2009 E2010 ## Min. : 3.238 Min. : 3.238 ## 1st Qu.: 45.062 1st Qu.: 45.062 ## Median : 58.077 Median : 58.077 ## Mean : 68.901 Mean : 68.901 ## 3rd Qu.: 78.166 3rd Qu.: 78.166 ## Max. :368.940 Max. :368.940 Modelling the raw standardized mortality rates (SMRs) Calculate the raw SMRs as \\[ \\text{SMR} = \\frac{observed}{expected}\\] SMR_raw &lt;- observed[, -1] / expected # Rename columns names(SMR_raw) &lt;- c(&quot;SMR2001&quot;, &quot;SMR2002&quot;, &quot;SMR2003&quot;, &quot;SMR2004&quot;, &quot;SMR2005&quot;, &quot;SMR2006&quot;, &quot;SMR2007&quot;, &quot;SMR2008&quot;, &quot;SMR2009&quot;, &quot;SMR2010&quot; ) # Printing first six rows of raw SMRs head(SMR_raw) ## SMR2001 SMR2002 SMR2003 SMR2004 SMR2005 SMR2006 SMR2007 ## 00AA 0.7550261 0.0000000 1.1000648 0.3636943 0.3560423 0.3429382 1.6547601 ## 00AB 1.5638016 1.5768644 1.9498828 1.5135516 2.2413721 1.6253713 1.5558858 ## 00AC 0.9031554 0.8366462 0.8656520 0.7223915 0.7955030 0.7699460 0.5665330 ## 00AD 1.2084078 1.2384043 1.2289415 1.0349871 1.2063043 1.0253852 0.9848384 ## 00AE 0.8975442 1.1530694 0.8972291 0.7471429 0.7584911 0.5869024 0.6335828 ## 00AF 0.9147531 0.9750183 1.0130766 0.9217897 0.9381329 0.8516367 0.8655884 ## SMR2008 SMR2009 SMR2010 ## 00AA 0.3210586 0.0000000 0.3088328 ## 00AB 1.6637225 1.7641760 1.3624329 ## 00AC 0.6520466 0.5937205 0.6774503 ## 00AD 0.9191171 0.9523676 0.9523676 ## 00AE 0.5394928 0.6315370 0.4937471 ## 00AF 0.7598103 0.7608397 0.8035037 # Summarising raw SMRs summary(SMR_raw) ## SMR2001 SMR2002 SMR2003 SMR2004 ## Min. :0.3883 Min. :0.0000 Min. :0.3616 Min. :0.2778 ## 1st Qu.:0.7900 1st Qu.:0.8272 1st Qu.:0.8519 1st Qu.:0.7636 ## Median :0.9496 Median :1.0168 Median :1.0209 Median :0.9266 ## Mean :1.0349 Mean :1.0508 Mean :1.0895 Mean :0.9812 ## 3rd Qu.:1.2526 3rd Qu.:1.2364 3rd Qu.:1.3071 3rd Qu.:1.1858 ## Max. :1.9861 Max. :2.2181 Max. :2.2483 Max. :1.9811 ## SMR2005 SMR2006 SMR2007 SMR2008 ## Min. :0.3326 Min. :0.3429 Min. :0.3509 Min. :0.3211 ## 1st Qu.:0.7592 1st Qu.:0.7415 1st Qu.:0.7533 1st Qu.:0.7695 ## Median :0.9573 Median :0.9101 Median :0.9305 Median :0.9404 ## Mean :1.0126 Mean :0.9726 Mean :0.9743 Mean :1.0069 ## 3rd Qu.:1.2083 3rd Qu.:1.1586 3rd Qu.:1.1679 3rd Qu.:1.1979 ## Max. :2.2414 Max. :2.0805 Max. :1.8528 Max. :2.0567 ## SMR2009 SMR2010 ## Min. :0.0000 Min. :0.3088 ## 1st Qu.:0.7452 1st Qu.:0.7682 ## Median :0.8777 Median :0.9337 ## Mean :0.9328 Mean :0.9639 ## 3rd Qu.:1.0934 3rd Qu.:1.1335 ## Max. :1.8507 Max. :2.3856 Attach the values of the raw SMRs to the shapefiles. The function merge() allows us to combine a data frame with a shapefile to plot later. # Convert row names to ID column SMR_raw &lt;- tibble::rownames_to_column(SMR_raw, &quot;ID&quot;) # Combine raw SMRs and shapefiles SMRspatial_raw &lt;- merge(england, SMR_raw, by = &quot;ID&quot;) Use ggplot() and geom_sf() to create a choropleth map such that local authorities are colored by the raw SMR estimate. # Creating breaks for legend in plot range &lt;-seq(min(SMR_raw$SMR2010) - 0.01, max(SMR_raw$SMR2010) + 0.01, length.out = 11) # Creating the map of Raw SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = SMRspatial_raw, aes(fill = SMR2010)) + # Break points for colours scale_y_continuous(breaks = range) + # Clear background and plot borders theme( axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), rect = element_blank() ) MCMC implementation of the Poisson-Gamma model in R The following code shows how to implement the MCMC using R for the COPD example. We start by defining the constants and the vectors that will be used in the code. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 # Number of MCMC iterations L &lt;- 20000 ## Initialize objects used in MCMC # Matrix for sampled values of parameter theta_i theta &lt;-matrix(ncol = length(y), nrow = L) # Matrix for fitted values fitted &lt;-theta # Vector for sampled values of hyper-parameter a a &lt;- c() # Vector for sampled values of hyper-parameter b b &lt;- c() ## Define constants # Sample size N &lt;- length(y) # Parameter of exponential prior for a lambda_a &lt;- 1 # Parameter of exponential prior for b lambda_b &lt;- 1 # standard deviation of the proposal distribution of log a u &lt;- 0.1 # Initialize theta theta[1, ] &lt;- y / E # Initial value sampled from the prior for a # REVIEW: In the example of the book theta ~ Ga(a,a) not Ga(a,b) a &lt;- rexp(1, lambda_a) # Initial value sampled from the prior for b b &lt;- rexp(1, lambda_b) fitted[1, ] &lt;- rpois(N, E * theta[1, ]) # Once all the constants and initial values are set we can run the MCMC. # The following code shows the MCMC implementation of a Poisson-Gamma model using only `R`. # Starting from l=2 as l=1 contains the initial values for(l in 2:L) { # Sampling from the posterior full conditional of each theta_i for (i in 1:N) theta[l, i] &lt;- rgamma(1, (y[i] + a[(l - 1)]), rate = (E[i] + b[(l - 1)])) # Sampling from the posterior full conditional of b b[l] &lt;- rgamma(1, (N * a[(l - 1)] + 1), rate = (sum(theta[l, ]) + lambda_b)) # Metropolis-Hastings step to sample from the full conditional of &quot;a&quot; # the new value receives the current value in case the proposed # value is rejected a[l] &lt;- a[l - 1] # Proposal in the log-scale laprop &lt;- rnorm(1, log(a[l - 1]), u) aprop &lt;- exp(laprop) #computing the numerator of the M-H step num &lt;- N * (aprop * (log(b[l])) - lgamma(aprop)) + (aprop - 1) * sum(log(theta[l, ])) - aprop * lambda_a + log(aprop) #computing the denominator of the M-H step den &lt;- N * (a[l - 1] * (log(b[l])) - lgamma(a[l - 1])) + (a[(l - 1)] - 1) * sum(log(theta[l, ])) - a[(l - 1)] * lambda_a + log(a[(l - 1)]) #computing the M-H ratio ratio &lt;- exp(num - den) unif &lt;- runif(1) # Change the current value if the proposed value is accepted if (unif &lt; ratio) a[l] &lt;- aprop fitted[l,] &lt;- rpois(N, E * theta[l,]) } After running the MCMC, check convergence of the chains using trace plots, autocorrelation functions and functions from the coda package. # Number of burn in samples and thinning burnin &lt;- 10000 thin &lt;- 10 # MCMC samples, setting the burnin and thinning seqaux &lt;- seq(burnin, L, by = thin) # Trace-plots of the parameters xx &lt;- seq(0, 20, length = 3000) par(mfrow = c(3, 2),pty=&quot;m&quot;) # Plot for &quot;a&quot; #traceplot for the posterior sample of parameters &#39;a&#39; and &#39;b&#39; plot(a[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;,xlab=&quot;Iterations&quot;,ylab=&quot;a&quot;) plot(b[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;,xlab=&quot;Iterations&quot;,ylab=&quot;b&quot;) #histogram of the posterior distribution of parameter &#39;a&#39; hist(a[seqaux], prob = 1, main = &quot;&quot;,) #prior distribution of parameter &quot;a&#39; lines(xx, dexp(xx, lambda_a), col = 2, lwd = 2) hist(b[seqaux], prob = 1, main = &quot;&quot;) #prior distribution of parameter &quot;a&#39; lines(xx, dexp(xx, lambda_b), col = 2, lwd = 2) #autocorrelation function of the sampled values of parameter &#39;a&#39; acf(a[seqaux],main=&quot;ACF for a&quot;) acf(b[seqaux],main=&quot;ACF for b&quot;) # Traceplots of the posterior samples of some of the theta&#39;s par(mfrow = c(3, 3)) for (i in 1:9) plot(theta[seqaux, i], type = &quot;l&quot;, bty = &quot;n&quot;,xlab=&#39;Iteration&#39;,ylab=expression(paste(theta,i))) Other diagnostic tools that can be assessed are the effective sample size (ESS) and Rhat. This can be done using the package coda. paste0(&quot;ESS a: &quot;, coda::effectiveSize(a[seqaux])) ## [1] &quot;ESS a: 32.126114421714&quot; paste0(&quot;ESS b: &quot;, coda::effectiveSize(b[seqaux])) ## [1] &quot;ESS b: 30.2610882715984&quot; paste0(&quot;ESS theta[1]: &quot;, coda::effectiveSize(theta[seqaux, 1])) ## [1] &quot;ESS theta[1]: 1001&quot; paste0(&quot;ESS theta[10]: &quot;,coda::effectiveSize(theta[seqaux, 10])) ## [1] &quot;ESS theta[10]: 1001&quot; From the trace-plots and the ESS of the sampled values for parameters a and b it looks like the sampled values are highly auto-correlated. We can run the MCMC longer to get a bigger or ESS, or we can tune the variance of the proposal of a to decrease the auto-correlation among sampled values. To this end, we use the algorithm proposed by Roberts and Rosenthal (2001) # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 # Number of MCMC iterations L &lt;- 30000 burnin&lt;-10000 check&lt;-50 attempt&lt;-0 accept&lt;-0 ## Initialize objects used in MCMC # Matrix for sampled values of parameter theta_i theta &lt;-matrix(ncol = length(y), nrow = L) # Matrix for fitted values fitted &lt;-theta # Vector for sampled values of hyper-parameter a a &lt;- c() # Vector for sampled values of hyper-parameter b b &lt;- c() ## Define constants # Sample size N &lt;- length(y) # Parameter of exponential prior for a lambda_a &lt;- 1 # Parameter of exponential prior for b lambda_b &lt;- 1 # standard deviation of the proposal distribution of log a u &lt;- 0.1 # Initialize theta theta[1, ] &lt;- y / E # Initial value sampled from the prior for a # REVIEW: In the example of the book theta ~ Ga(a,a) not Ga(a,b) a &lt;- rexp(1, lambda_a) # Initial value sampled from the prior for b b &lt;- rexp(1, lambda_b) fitted[1, ] &lt;- rpois(N, E * theta[1, ]) k&lt;-0 # Once all the constants and initial values are set we can run the MCMC. # The following code shows the MCMC implementation of a Poisson-Gamma model using only `R`. # Starting from l=2 as l=1 contains the initial values for(l in 2:L) { # Sampling from the posterior full conditional of each theta_i for (i in 1:N) theta[l, i] &lt;- rgamma(1, shape=(y[i] + a[(l - 1)]), rate =(E[i] + b[(l - 1)])) # Sampling from the posterior full conditional of b b[l] &lt;- rgamma(1, shape=(N * a[(l - 1)] + 1), rate=(sum(theta[l, ]) + lambda_b)) # Metropolis-Hastings step to sample from the full conditional of &quot;a&quot; # the new value receives the current value in case the proposed # value is rejected a[l] &lt;- a[l - 1] # Proposal in the log-scale laprop &lt;- rnorm(1, log(a[l - 1]), u) aprop &lt;- exp(laprop) #computing the numerator of the M-H step num &lt;- N * ((aprop) * (log(b[l])) - lgamma(aprop)) + (aprop - 1) * sum(log(theta[l, ])) - aprop * lambda_a + log(aprop) #computing the denominator of the M-H step den &lt;- N * ((a[l - 1]) * (log(b[l])) - lgamma(a[l - 1])) + (a[(l - 1)] - 1) * sum(log(theta[l, ])) - a[(l - 1)] * lambda_a + log(a[(l - 1)]) #computing the M-H ratio attempt&lt;-attempt+1 ratio &lt;- exp(num - den) unif &lt;- runif(1) # Change the current value if the proposed value is accepted if (unif &lt; ratio){ accept&lt;-accept+1 a[l] &lt;- aprop } # TUNING! if(l&lt;burnin &amp; attempt==check){ K&lt;-k+1 print(paste0(&quot;Can sd of &quot;, round(u,3), &quot; for a gave acc rate &quot;,accept/attempt)) delta&lt;-min(0.01,1/sqrt(k+1)) if(accept/attempt&gt;0.44){u&lt;-u*exp(delta)} if(accept/attempt&lt;0.44){u&lt;-u*exp(-delta)} accept &lt;- attempt &lt;- 0 # if(accept/attempt&lt;0.2){u&lt;-u*0.8} # if(accept/attempt&gt;0.6){u&lt;-u*1.2} # accept &lt;- attempt &lt;- 0 } fitted[l,] &lt;- rpois(N, E * theta[l,]) } # Number of burn in samples and thinning burnin &lt;- 10000 thin &lt;- 1 # MCMC samples, setting the burnin and thinning seqaux &lt;- seq(burnin, L, by = thin) # Trace-plots of the parameters xx &lt;- seq(0, 20, length = 3000) par(mfrow = c(3, 2),pty=&quot;m&quot;) # Plot for &quot;a&quot; #traceplot for the posterior sample of parameters &#39;a&#39; and &#39;b&#39; plot(a[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;,xlab=&quot;Iterations&quot;,ylab=&quot;a&quot;) plot(b[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;,xlab=&quot;Iterations&quot;,ylab=&quot;b&quot;) #histogram of the posterior distribution of parameter &#39;a&#39; hist(a[seqaux], prob = 1, main = &quot;&quot;,) #prior distribution of parameter &quot;a&#39; lines(xx, dexp(xx, lambda_a), col = 2, lwd = 2) hist(b[seqaux], prob = 1, main = &quot;&quot;) #autocorrelation function of the sampled values of parameter &#39;a&#39; acf(a[seqaux],main=&quot;ACF for a&quot;) acf(b[seqaux],main=&quot;ACF for b&quot;) #prior distr paste0(&quot;ESS a: &quot;, coda::effectiveSize(a[seqaux])) ## [1] &quot;ESS a: 123.907829891743&quot; paste0(&quot;ESS b: &quot;, coda::effectiveSize(b[seqaux])) ## [1] &quot;ESS b: 134.452182190428&quot; paste0(&quot;ESS theta[1]: &quot;, coda::effectiveSize(theta[seqaux, 1])) ## [1] &quot;ESS theta[1]: 19286.3773745989&quot; paste0(&quot;ESS theta[10]: &quot;,coda::effectiveSize(theta[seqaux, 10])) ## [1] &quot;ESS theta[10]: 20001.0000000001&quot; The ESS of a and b have increased a bit. Another way to increase the ESS is to run the algorithm for multiple chains starting from different values. # Posterior summaries of theta_i meantheta &lt;- apply(theta, 2, mean) q025theta &lt;- apply(theta, 2, function(x) quantile(x, 0.025)) q975theta &lt;- apply(theta, 2, function(x) quantile(x, 0.975)) # Plot the mean and 95% CIs for the thetas par(mfrow = c(1, 1)) plot( meantheta, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(q025theta), max(q975theta)) ) for (i in 1:N) segments(i, q025theta[i], i, q975theta[i]) abline(h = 1, lwd = 2, lty = 2) # Posterior summary of fitted values meanfit &lt;- apply(fitted, 2, mean) q025fit &lt;- apply(fitted, 2, function(x) quantile(x, 0.025)) q975fit &lt;- apply(fitted, 2, function(x) quantile(x, 0.975)) # Plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, meanfit, ylim = c(min(q025fit), max(q975fit)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], q025fit[i], y[i], q975fit[i]) abline(a = 0, b = 1) We now show how to run the same model using Nimble. Code for the Poisson-Gamma model in Nimble require(&quot;nimble&quot;) # Define the model Example6_2Code &lt;- nimbleCode({ for (i in 1:N) { Y[i] ~ dpois(mu[i]) mu[i] &lt;- log(E[i]) * theta[i] theta[i] ~ dgamma(a,b) } # Priors a ~ dexp(1) b ~ dexp(1) }) # Read the data and define the constants, data and initials lists for the `Nimble` model. ex.const &lt;- list( E = expected$E2010, N=length(observed$Y2010)) ex.data &lt;- list(Y = observed$Y2010) inits &lt;- function() list(theta = rgamma(length(y),1,1),a=rexp(1,1),b=rexp(1,1)) # Define parameters to monitor and run the model params &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;theta&quot;) mcmc.out &lt;- nimbleMCMC( code = Example6_2Code, data = ex.data, constants = ex.const, inits = inits, monitors = params, niter = 22000, nburnin = 2000, thin = 10, WAIC = TRUE, nchains = 2, samplesAsCodaMCMC = TRUE ) mvSamples &lt;- mcmc.out$samples #trace plots of beta1 plot(mvSamples[, c(&quot;a&quot;)]) plot(mvSamples[, c(&quot;b&quot;)]) Example 6.3: Fitting a Poisson regression model In this example, we consider the Poisson log-linear model seen in Chapter 2, Section 2.7. We consider the observed and expected number of cases of respiratory mortality in small areas in the UK from a study examining the long-term effects of air pollution (Elliott, Shaddick, Wakefield, de Hoogh, &amp; Briggs, 2007). \\[\\begin{eqnarray} \\label{eqn:cum1c2} \\log \\mu_{l} = \\beta_0 + \\beta_1 X_{l} + \\beta_{d}X_{l} \\end{eqnarray}\\] where \\(\\beta_1\\) represents the effect of exposure and \\(\\beta_d\\) is the effect of the area-level covariate. Nimble Load nimble package library(&quot;nimble&quot;) The NIMBLE code to fit this model is as follows: # Define the model Example6_3Code &lt;- nimbleCode({ for (i in 1:N) { Y[i] ~ dpois(mu[i]) log(mu[i]) &lt;- log(E[i]) + beta0 + beta1 * X1[i] + beta2 * X2[i] } # Priors beta0 ~ dnorm (0 , sd = 100) beta1 ~ dnorm (0 , sd = 100) beta2 ~ dnorm (0 , sd = 100) # Functions of interest: base &lt;- exp(beta0) RR &lt;- exp(beta1) }) # Read the data and define the constants, data and initials lists for the `Nimble` model. # REVIEW: Is this another version of the COPD data? Is there a data dictionary for this dataset? data &lt;- read.csv(&quot;data/DataExample53.csv&quot;, sep = &quot;,&quot;) ex.const &lt;- list( N = nrow(data), E = data$exp_lungc65pls, X1 = as.vector(scale(data$k3)), X2 = as.vector(scale(data$k2)) ) ex.data &lt;- list(Y = data$lungc65pls) inits &lt;- function() list(beta0 = rnorm(1), beta1 = rnorm(1), beta2 = rnorm(1)) # Define parameters to monitor and run the model params &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;base&quot;, &quot;RR&quot;) mcmc.out &lt;- nimbleMCMC( code = Example6_3Code, data = ex.data, constants = ex.const, inits = inits, monitors = params, niter = 22000, nburnin = 2000, thin = 10, WAIC = TRUE, nchains = 3, samplesAsCodaMCMC = TRUE ) Checking the trace plots and posterior summaries for each of the parameters. mvSamples &lt;- mcmc.out$samples #trace plots of beta1 plot(mvSamples[, c(&quot;beta1&quot;)]) #trace plots of base plot(mvSamples[, c(&quot;base&quot;)]) #trace plots of RR plot(mvSamples[, c(&quot;RR&quot;)]) #posterior summary of base and RR summary(mvSamples[, c(&quot;base&quot;,&quot;RR&quot;)]) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 3 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## base 1.129 0.01260 0.0001627 0.0001666 ## RR 1.015 0.01744 0.0002251 0.0003443 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## base 1.1046 1.121 1.129 1.138 1.154 ## RR 0.9805 1.003 1.014 1.026 1.048 The baseline relative risk is estimated at 1.105 with limits of the 95% posterior credible interval equal to 1.12 and 1.154. Stan We now run the same example in Stan. Load stan package with options library(rstan) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) data { int&lt;lower=0&gt; N; vector[N] E; // expected cases? vector[N] X1; // what are these covariates? vector[N] X2; int Y[N] ; } parameters { real beta0; real beta1; real beta2; } transformed parameters{ real base = exp(beta0); real RR = exp(beta1); } model { vector[N] mu; for(i in 1:N){ mu[i] = log(E[i])+ beta0 + beta1*X1[i] + beta2*X2[i]; Y[i] ~ poisson_log(mu[i]); } beta0 ~ normal(0 , 100); beta1 ~ normal(0 , 100); beta2 ~ normal(0 , 100); } data &lt;- read.csv(&quot;data/DataExample53.csv&quot;, sep = &quot;,&quot;) stan_data &lt;- list( N = nrow(data), E = data$exp_lungc65pls, X1 = as.vector(scale(data$k3)), X2 = as.vector(scale(data$k2)), Y = data$lungc65pls ) Example6_3_Stan &lt;- stan( file = &quot;functions/Example6_3.stan&quot;, data = stan_data, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) Checking the traceplots and posterior summaries of the parameters. rstan::traceplot(Example6_3_Stan, pars = c(&quot;beta1&quot;, &quot;base&quot;, &quot;RR&quot;)) stan_summary &lt;- summary(Example6_3_Stan, pars = c(&quot;RR&quot;, &quot;base&quot;)) stan_summary$summary ## mean se_mean sd 2.5% 25% 50% 75% ## RR 1.014958 0.0002226978 0.01751280 0.9808189 1.003024 1.015022 1.026792 ## base 1.128715 0.0001341104 0.01258458 1.1044455 1.120106 1.128723 1.137275 ## 97.5% n_eff Rhat ## RR 1.049442 6184.139 1.0000989 ## base 1.153651 8805.473 0.9998812 "],["Strategies.html", "Chapter 7 Strategies for modelling Example 7.6 Variable selection in land use regression using lasso and horseshoe priors", " Chapter 7 Strategies for modelling This chapter considers both some wider issues related to modelling and the generalisability of results, together with more technical material on the effect of covariates and model selection. The reader will have gained an understanding of the following topics: why having contrasts in the variables of interest is important in assessing the effects they have on the response variable; biases that may arise in the presence of covariates and how covariates can affect variable selection and model choice; hierarchical models and how that can be used to acknowledge dependence between observations; the issues associated with the use of p-values as measures of evidence against a null hypothesis and why basing scientific conclusions on it can lead to non-reproducible results; use of predictions from exposure models, including acknowledging the additional uncertainty involved when using predictions as inputs to a health model; methods for performing model selection, including the pros and cons of automatic selection procedures; model selection within the Bayesian setting, and how the models themselves can be incorporated into the estimation process using Bayesian Model Averaging. Example 7.6 Variable selection in land use regression using lasso and horseshoe priors This example is based on the Volatile Organic Compound (VOC) data analyzed by Zapata-Marin et al. (2022). VOCs are components of the complex mixture of air pollutants within cities and can cause various adverse health effects. VOCs concentrations were measured over 2-week periods, for three monitoring campaigns between 2005 and 2006 across over 130 locations in Montreal, QC, Canada. The data analyzed in this example is ethylbenzene observed during April 2006. Land-use variables were available as a proportion of the area of each buffer covered by each specific variable. In this example, we explore the land-use variables using circular buffers at 1,000-m radii around each monitoring location. The land use variables include Building_100m,Government and Institutional, Residential, Population,Roads,Total NOx. Below we read the data and plot the monitoring locations with solid circles proportional to the observed value of ethylbenzene. library(dplyr) library(geoR) library(GGally) library(leaflet) library(ggplot2) library(sp) library(spdep) library(nimble) # Load data on benzene concentration in Montreal ethylbenzene &lt;- read.csv(&quot;data/montreal_ethylbenzene.csv&quot;) We start by mapping the locations of the monitoring sites across Montreal. The diameter of the circles are proportional to the observed concentration of ethylbenzene. range_values &lt;- range(log(ethylbenzene$Ethylbenzene)) # Define bins based on the range of values mybins &lt;- seq(range_values[1], range_values[2], length.out = 5) mypalette &lt;- colorBin(palette=&quot;inferno&quot;, domain=ethylbenzene$Ethylbenzene, na.color=&quot;transparent&quot;, bins=mybins) leaflet( data = ethylbenzene) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ lon, ~ lat, fillOpacity = 0.8, color=&quot;orange&quot;, fillColor = ~ mypalette(log(Ethylbenzene)), opacity = 5, radius = ~log(Ethylbenzene)*4, stroke = FALSE) |&gt; addLegend( pal=mypalette, values=~log(Ethylbenzene), opacity=0.9, title = &quot;log(Ethylbenzene)&quot;, position = &quot;bottomright&quot; ) We now perform a simple exploratory data analysis using the ggpairs command. It provides the empirical distribution of each of the variables as well as pairwise correlations among them. We transform two variables, Building and Total NOx to obtain a more symmetric empirical distributions. The greatest observed correlation among the variables is 0.614, between Roads and Population. lur &lt;- read.csv(&quot;data/VOC_predictors.csv&quot;) colnames(lur) ## [1] &quot;X&quot; &quot;Y&quot; ## [3] &quot;ID&quot; &quot;Building_100m&quot; ## [5] &quot;Government.and.Institutional_1000m&quot; &quot;Residential_1000m&quot; ## [7] &quot;Pop_1000m&quot; &quot;Roads_1000m&quot; ## [9] &quot;Tot_NOx_1000m&quot; &quot;Ethylbenzene&quot; lur$X &lt;- lur$X/1000 lur$Y &lt;- lur$Y/1000 lur$lbuild&lt;-sqrt(lur$Building_100m) lur$lNOx&lt;-log(lur$Tot_NOx_1000m) #removing X, Y, ID, Building_100m and Tot_NOx_1000m from the plot ggpairs(lur[,-c(1:3,4,9)], title=&quot;correlogram with ggpairs()&quot;, progress = FALSE) We now fit a linear model to the log(Ethylbenzene) that includes the log of buildings, government and institutional buildings, residential areas, population, roads and log of NOx: #lur_ethyl &lt;- lur |&gt; select( ends_with(&quot;0m&quot;), &quot;X&quot;, &quot;Y&quot;,&quot;Ethylbenzene&quot;) # Fit a glm fit_glm&lt;-glm(log(Ethylbenzene) ~ X + Y + lbuild + Government.and.Institutional_1000m + Residential_1000m + Pop_1000m + Roads_1000m+lNOx, data = lur ) summary(fit_glm) ## ## Call: ## glm(formula = log(Ethylbenzene) ~ X + Y + lbuild + Government.and.Institutional_1000m + ## Residential_1000m + Pop_1000m + Roads_1000m + lNOx, data = lur) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.70723 -0.18976 0.00957 0.17620 0.59346 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.416e+00 2.229e+01 0.198 0.8434 ## X -6.006e-03 5.088e-03 -1.180 0.2409 ## Y -2.087e-04 4.729e-03 -0.044 0.9649 ## lbuild 2.542e-03 9.912e-04 2.565 0.0120 * ## Government.and.Institutional_1000m 5.939e-09 1.410e-07 0.042 0.9665 ## Residential_1000m 1.061e-07 8.989e-08 1.180 0.2410 ## Pop_1000m 9.442e-06 4.908e-06 1.924 0.0575 . ## Roads_1000m -8.235e-06 5.967e-06 -1.380 0.1710 ## lNOx 9.982e-02 4.689e-02 2.129 0.0359 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.06620174) ## ## Null deviance: 8.4070 on 99 degrees of freedom ## Residual deviance: 6.0244 on 91 degrees of freedom ## AIC: 22.852 ## ## Number of Fisher Scoring iterations: 2 Variables log(building), population and lNOx are positively associated with log(Ethylbenzene). We now fit a linear model in Nimble using a Lasso prior for the coefficients of the variables. Nimble Ex7_6LassoCode &lt;- nimbleCode({ # Likelihood for (i in 1:n) { y[i] ~ dnorm(mu[i], sd = sigma) mu[i] &lt;- beta0 + inprod(X[i, ], beta[1:p]) } # Prior specification for (j in 1:p) { beta[j] ~ ddexp(0, scale = 1/(taub * taue)) } taue ~ dgamma(0.1, 0.1) sigma &lt;- 1/taue taub ~ dgamma(0.1, 0.1) beta0 ~ dnorm(0, 10) }) X &lt;- lur |&gt; dplyr::select( &quot;lbuild&quot;, &quot;Government.and.Institutional_1000m&quot;,&quot;Residential_1000m&quot;, &quot;Pop_1000m&quot;,&quot;Roads_1000m&quot;,&quot;lNOx&quot;, &quot;X&quot;, &quot;Y&quot;) constants &lt;- list(n = nrow(lur), p = ncol(X)) ex.data &lt;- list(y = log(lur$Ethylbenzene), X = scale(X)) params &lt;- c( &quot;beta0&quot;, &quot;beta&quot;, &quot;taub&quot;,&quot;taue&quot;, &quot;sigma&quot;) inits &lt;- list( sigma = 0.1, tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out.lasso &lt;- nimbleMCMC( code = Ex7_6LassoCode, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 100000, nburnin = 50000, thin = 15, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time min(coda::effectiveSize(mcmc.out.lasso$samples)) ## [1] 4028.613 plot(mcmc.out.lasso$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[1]&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[2]&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[3]&quot;)], bty = &quot;n&quot;, main = &quot;beta3&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[4]&quot;)], bty = &quot;n&quot;, main = &quot;beta4&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[5]&quot;)], bty = &quot;n&quot;, main = &quot;beta5&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[6]&quot;)], bty = &quot;n&quot;, main = &quot;beta6&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[7]&quot;)], bty = &quot;n&quot;, main = &quot;beta7&quot;) plot(mcmc.out.lasso$samples[, c(&quot;beta[8]&quot;)], bty = &quot;n&quot;, main = &quot;beta8&quot;) plot(mcmc.out.lasso$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out.lasso$samples[, c(&quot;taue&quot;)], bty = &quot;n&quot;, main = &quot;taue&quot;) plot(mcmc.out.lasso$samples[, c(&quot;taub&quot;)], bty = &quot;n&quot;, main = &quot;taub&quot;) beta_var &lt;- paste0(&quot;beta[&quot;, 1:ncol(X), &quot;]&quot;) #posterior summaries of all the parameters in the model post_summary_lasso &lt;- mcmc.out.lasso$summary$all.chains |&gt; as.data.frame() post_summary_lasso &lt;- post_summary_lasso[beta_var,] post_summary_lasso$variable &lt;- colnames(X) ggplot(post_summary_lasso, aes(x = variable)) + geom_pointrange(aes(y = Mean, ymin =`95%CI_low`, ymax =`95%CI_upp`)) + geom_hline(yintercept = 0) + theme_classic() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) Under the Lasso prior specification, only the logarithm of Building_100m is associated with the logarithm of ethylbenzene. Next, we perform the analysis assuming a horseshoe prior for the coefficient.: Ex7_6HorseshoeCode &lt;- nimbleCode({ # Likelihood for(i in 1:n){ y[i] ~ dnorm(mu[i], sd = sigma) mu[i] &lt;- beta0 + inprod(X[i,], beta[1:p]) } # Prior specification for(j in 1:p){ beta[j] ~ dnorm(0, sd = lambda[j]*tau) lambda[j] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) } tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) beta0 ~ dnorm(0,sd=10) }) set.seed(321) constants &lt;- list(n = nrow(lur), p = ncol(X)) #using the scaled covariates ex.data &lt;- list(y = log(lur$Ethylbenzene), X = scale(X)) params &lt;- c( &quot;beta0&quot;, &quot;beta&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;lambda&quot;) inits &lt;- list( sigma = 0.1, tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out.hs &lt;- nimbleMCMC( code = Ex7_6HorseshoeCode, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 100000, nburnin = 50000, thin = 15, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time min(coda::effectiveSize(mcmc.out.hs$samples)) ## [1] 661.5456 plot(mcmc.out.hs$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[1]&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[2]&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[3]&quot;)], bty = &quot;n&quot;, main = &quot;beta3&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[4]&quot;)], bty = &quot;n&quot;, main = &quot;beta4&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[5]&quot;)], bty = &quot;n&quot;, main = &quot;beta5&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[6]&quot;)], bty = &quot;n&quot;, main = &quot;beta6&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[7]&quot;)], bty = &quot;n&quot;, main = &quot;beta7&quot;) plot(mcmc.out.hs$samples[, c(&quot;beta[8]&quot;)], bty = &quot;n&quot;, main = &quot;beta8&quot;) plot(mcmc.out.hs$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out.hs$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) beta_var &lt;- paste0(&quot;beta[&quot;, 1:ncol(X), &quot;]&quot;) post_summary_hs &lt;- mcmc.out.hs$summary$all.chains |&gt; as.data.frame() post_summary_hs &lt;- post_summary_hs[beta_var,] post_summary_hs$variable &lt;- colnames(X) ggplot(post_summary_hs, aes(x = variable)) + geom_pointrange(aes(y = Mean, ymin =`95%CI_low`, ymax =`95%CI_upp`)) + geom_hline(yintercept = 0) + theme_classic() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) "],["trusted.html", "Chapter 8 The challenges of working with real-world data Solutions to Selected Exercises", " Chapter 8 The challenges of working with real-world data This chapter considers some of the issues that will arise when dealing with ‘real data’. Data will commonly have missing values and may be measured with error. This error might be random or may be due to systematic patterns in how it was collected. From this chapter, the reader will have gained an understanding of the following topics: Classification of missing values into missing at random or not at random. Methods for imputing missing values. Various measurement models including classical and Berkson. The attenuation of regression coefficients under measurement error. Preferential sampling, where the process that determines the locations of monitoring sites and the process being modelled are in some ways dependent. How preferential sampling can bias the measurements that arise from environmental monitoring networks Solutions to Selected Exercises PLACEHOLDER: Repeat the Black Smoke analysis conducted in Watson et al. 2019. in the “naive” model. If INLA is not already installed, it needs to be done manually (https://www.r-inla.org/download-install): install.packages( &quot;INLA&quot;, repos = c(getOption(&quot;repos&quot;), INLA = &quot;https://inla.r-inla-download.org/R/stable&quot;), dep = TRUE ) After INLA is installed load all the necessary packages. library(INLA) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loading required package: parallel ## This is INLA_22.12.16 built 2022-12-23 13:24:10 UTC. ## - See www.r-inla.org/contact-us for how to get help. library(mvtnorm) library(boot) ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:nimble&#39;: ## ## logit library(geoR) library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths library(sp) library(ggplot2) This example uses the BlackSmokePrefData.csv file. BlackSmokePrefData &lt;- read.csv(&quot;data/black_smoke/BlackSmokePrefData.csv&quot;) head(BlackSmokePrefData) ## site east north AMEAN66 AMEAN67 AMEAN68 AMEAN69 AMEAN70 AMEAN71 ## 1 ADDLESTONE 1 505200 164600 92 103 68 47 47 45 ## 2 BARNSLEY 8 434800 409400 NA NA NA NA NA NA ## 3 BARNSLEY 9 437000 405500 NA NA NA NA NA NA ## 4 BOLTON 24 371500 409200 132 108 75 77 78 74 ## 5 BRADFORD 6 416300 432900 202 151 122 110 90 118 ## 6 CARDIFF 12 319300 177300 NA NA NA NA NA NA ## AMEAN72 AMEAN73 AMEAN74 AMEAN75 AMEAN76 AMEAN77 AMEAN78 AMEAN79 AMEAN80 ## 1 52 41 36 31 33 24 20 17 11 ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA 23 18 19 19 12 ## 4 149 137 NA NA NA NA NA NA NA ## 5 172 191 74 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA 24 19 12 ## AMEAN81 AMEAN82 AMEAN83 AMEAN84 AMEAN85 AMEAN86 AMEAN87 AMEAN88 AMEAN89 ## 1 15 9 8 15 12 10 13 7 9 ## 2 NA NA NA NA NA NA NA NA NA ## 3 19 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 10 11 16 19 18 14 11 9 11 ## AMEAN90 AMEAN91 AMEAN92 AMEAN93 AMEAN94 AMEAN95 AMEAN96 ## 1 11 6 NA NA NA NA NA ## 2 NA 13 6 11 6 6 5 ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 12 12 9 11 9 19 11 ## standardize location coordinates sd_x &lt;- sd(BlackSmokePrefData[, c(2)]) sd_y &lt;- sd(BlackSmokePrefData[, c(3)]) BlackSmokePrefData[, 2] &lt;- BlackSmokePrefData[, 2] / sd_x BlackSmokePrefData[, 3] &lt;- BlackSmokePrefData[, 3] / sd_y We need to reshape the data to have one observation per row, as required by INLA. yearmeans = colMeans(BlackSmokePrefData[, -c(1, 2, 3)], na.rm = T) ## save means for each year BlackSmokePrefData &lt;- melt( BlackSmokePrefData, id.vars = c(1, 2, 3), variable.name = &#39;year&#39;, value.name = &#39;bsmoke&#39; ) BlackSmokePrefData$year = as.numeric(as.character(factor(BlackSmokePrefData$year, labels = 66:96))) ## standardize by mean of mean of each year to make it unitless BlackSmokePrefData$bsmoke = BlackSmokePrefData$bsmoke / mean(yearmeans) ## log-transform to eliminate right skew BlackSmokePrefData$bsmoke = log(BlackSmokePrefData$bsmoke) Producing the 2-d mesh of the UK: no_sites = as.numeric(length(unique(BlackSmokePrefData$site))) # number of sites no_T = as.numeric(length(unique(BlackSmokePrefData$year))) # number of years ncol = 100 # grid for projection nrow = 100 L = nrow * ncol # number of grid sites # Form the regular mesh # # Create a rough convex boundary for the UK # # Form the grid independently from the sites to avoid preferential grid selection # UK_domain = cbind(c(2, 7.7, 7.7, 6, 4, 1), c(0.5, 0.5, 6, 13.5, 13.5, 12)) hull = inla.nonconvex.hull(cbind(BlackSmokePrefData$east, BlackSmokePrefData$north)) cutoff_dist = 16000 / sd_x # 16km as min edge max.edge = 100000 / sd_x # 100km max edge as in Shaddick and Zidek # NOTE: JOHNNY, I HAD TO COMMENT THE CUTOFF VARIABLE CAUSE OTHERWISE IT WOULDN&#39;T WORK mesh = inla.mesh.2d( loc = cbind(BlackSmokePrefData$east, BlackSmokePrefData$north), boundary = hull, offset = c(0.1, 0.2), max.edge = c(cutoff_dist, 0.5), #cutoff = c(cutoff_dist, 1), min.angle = 26 ) plot(mesh) points(BlackSmokePrefData$east, BlackSmokePrefData$north, col = &quot;red&quot;) Now we define and fit our INLA model. spde_obj = inla.spde2.pcmatern( mesh = mesh, alpha = 2, prior.range = c(0.04, 0.05), prior.sigma = c(1, 0.01), constr = T ) A_proj = inla.spde.make.A( mesh = mesh, loc = as.matrix(cbind( BlackSmokePrefData$east, BlackSmokePrefData$north )), group = BlackSmokePrefData$year - 65, # group membership needs to be 1:no_T n.group = no_T ) s_index = inla.spde.make.index(name = &quot;spatial.field&quot;, n.spde = spde_obj$n.spde, n.group = no_T) time = (1:no_T) / no_T time2 = time ^ 2 # create the stack object for estimating observation process y # cov_y = data.frame( year = rep(time, each = no_sites), year_2 = rep(time2, each = no_sites), spatial_ind = rep(1:no_sites, times = no_T), spatial_ind2 = no_sites + rep(1:no_sites, times = no_T) ) # site-specific random intercepts # Needs copies to include covariate twice s_index_copy = s_index names(s_index_copy) = c(&#39;spatial.field.copy&#39;, &quot;spatial.field.group.copy&quot;, &quot;spatial.field.repl.copy&quot;) s_index_copy2 = s_index names(s_index_copy2) = c(&#39;spatial.field.copy2&#39;, &quot;spatial.field.group.copy2&quot;, &quot;spatial.field.repl.copy2&quot;) stack_y_est = inla.stack( data = list( y = BlackSmokePrefData$bsmoke, #single model alldata = cbind(BlackSmokePrefData$bsmoke, NA, NA), Ntrials = rep(0, times = length(BlackSmokePrefData$bsmoke)) ), #joint model A = list(A_proj, A_proj, A_proj, 1), effects = list( c(s_index, list(Intercept = 1)), c(s_index_copy, list(Intercept_copy = 1)), c(s_index_copy2, list(Intercept_copy2 = 1)), cov_y ), tag = &#39;y_est&#39; ) formula_naive = y ~ -1 + Intercept + f(spatial.field, model = spde_obj) + f(spatial.field.copy, I(spatial.field.group.copy / no_T), model = spde_obj) + f(spatial.field.copy2, I((spatial.field.group.copy2 / no_T) ^ 2), model = spde_obj) + I(spatial.field.group / no_T) + I((spatial.field.group / no_T) ^ 2) + f(year, model = &#39;ar1&#39;) + f(spatial_ind, model = &quot;iid2d&quot;, n = no_sites * 2, constr = TRUE) + #random site-specific intercepts f(spatial_ind2, year, copy = &quot;spatial_ind&quot;) theta.ini = c( 1.597900,-1.277423,-0.443820,-1.441220, 0.036510,-1.441336, 0.016919, 4.462918, 1.437147, 4, 4, 4) out.naive = inla( formula_naive, family = &#39;gaussian&#39;, data = inla.stack.data(stack_y_est), control.predictor = list(A = inla.stack.A(stack_y_est), compute = F), control.compute = list(dic = F, config = T, cpo = F), control.inla = list(strategy = &quot;gaussian&quot;, int.strategy = &#39;eb&#39;), control.mode = list(theta = theta.ini, restart = T), verbose = T, num.threads = 2 ) We can now take posteriors from the model. m_samples = 50 samp &lt;- inla.posterior.sample(m_samples, out.naive) for (i in 1:m_samples) { samp[[i]]$latent = samp[[i]]$latent[-c(grep(&#39;Predictor&#39;, rownames(samp[[i]]$latent), fixed = T)),] } Finally, summarize and plot the outputs: stepsize = 5000 / sd_x nxy = round(c(diff(range( BlackSmokePrefData$east )), diff(range( BlackSmokePrefData$north ))) / stepsize) ## how many points? proj_grid = inla.mesh.projector( mesh, xlim = range(BlackSmokePrefData$east), ylim = range(BlackSmokePrefData$north), dims = nxy ) A.grid = inla.spde.make.A(mesh, loc = cbind(BlackSmokePrefData$east, BlackSmokePrefData$north)) ## define placeholders tmp2 = matrix(0, nrow = no_T, ncol = mesh$n) year = (1:no_T) / no_T year2 = year ^ 2 Post_Array2 = array(0, dim = c(m_samples, no_T, mesh$n)) Post_Sites_Array2 = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) residuals_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) RI_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) # random intercepts RS_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) # random slopes for (i in 1:m_samples) { # Creating predictions for Y process for (j in 1:no_T) # loop over the years { Post_Array2[i, j,] &lt;- samp[[i]]$latent[&#39;Intercept:1&#39;] + # Intercept samp[[i]]$latent[which(base::startsWith(prefix = &#39;year:&#39;, x = names(samp[[i]]$latent)))][j] * year[j] + # linear fixed effect #samp[[i]]$latent[&#39;year_2&#39;] * year2[j] + # quadratic fixed effect as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial.field:&#39;, x = names(samp[[i]]$latent)))]) + # Spatial Intercept as.numeric(samp[[i]]$latent[base::which(startsWith(prefix = &#39;spatial.field.copy:&#39;, x = names(samp[[i]]$latent)))]) * year[j] + # Spatial linear slope as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial.field.copy2:&#39;, x = names(samp[[i]]$latent)))]) * year2[j] # Spatial quadratic slope # Project onto the site locations and add random site-specific effects # Post_Sites_Array2[i, j,] = as.numeric(A.grid %*% Post_Array2[i, j,]) + as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial_ind:&#39;, x = names(samp[[i]]$latent)))[1:no_sites]]) + # random Intercept as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial_ind2:&#39;, x = names(samp[[i]]$latent)))[no_sites + (1:no_sites)]]) * year[j] # random slope # Extract the random intercepts # RI_array[i, j,] = as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial_ind:&#39;, x = names(samp[[i]]$latent)))[1:no_sites]]) # Extract the random slopes # RS_array[i, j,] = as.numeric(samp[[i]]$latent[which(base::startsWith(prefix = &#39;spatial_ind2:&#39;, x = names(samp[[i]]$latent)))[no_sites + (1:no_sites)]]) } } ## summarizing Post_mean2 &lt;- t(apply(Post_Array2, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) Post_sd2 &lt;- t(apply(Post_Array2, c(2, 3), function(x) { sd(x, na.rm = TRUE) })) Post_LCL2 &lt;- apply((apply(Post_Array2, c(1, 2), function(x) { mean(x, na.rm = TRUE) })), 2, quantile, probs = c(0.025)) Post_UCL2 &lt;- apply((apply(Post_Array2, c(1, 2), function(x) { mean(x, na.rm = TRUE) })), 2, quantile, probs = c(0.975)) RI_array &lt;- t(apply(RI_array, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) RS_array &lt;- t(apply(RS_array, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) ## placeholders for uniform grid posterior blacksmoke values (not necessarily in dataset) post_Matrix_grid_mean2 = array(NA, dim = c(nxy[1], nxy[2], no_T)) post_Matrix_grid_sd2 = array(NA, dim = c(nxy[1], nxy[2], no_T)) for (i in 1:no_T) { post_Matrix_grid_mean2[, , i] = inla.mesh.project(proj_grid, Post_mean2[, i]) post_Matrix_grid_sd2[, , i] = inla.mesh.project(proj_grid, Post_sd2[, i]) } ## placeholders for observed (R1 = active, R0 = inactive) site blacksmoke values R1_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(R1_results2) = c(&#39;prediction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) R0_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(R0_results2) = c(&#39;prediction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) Grid_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(Grid_results2) = c(&#39;prediction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) BlackSmokePrefData_original &lt;- read.csv(&quot;data/black_smoke/BlackSmokePrefData.csv&quot;) ## need original columns here for (j in 1:no_T) { R1_results2[j, 1] = mean(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], na.rm = T) R1_results2[j, 2] = sd(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], c(1), mean, na.rm = T)) R1_results2[j, 3] = quantile(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.025) R1_results2[j, 4] = quantile(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.975) R0_results2[j, 1] = mean(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], na.rm = T) R0_results2[j, 2] = sd(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], c(1), mean, na.rm = T)) R0_results2[j, 3] = quantile(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.025) R0_results2[j, 4] = quantile(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.975) } for (i in 1:no_T) { Grid_results2[i, 1] = mean(post_Matrix_grid_mean2[, , i], na.rm = T) Grid_results2[i, 2] = mean(post_Matrix_grid_sd2[, , i], na.rm = T) } Grid_results2[, 3] = Post_LCL2 Grid_results2[, 4] = Post_UCL2 ## Revert to the original scale (exponentiating and scaling up) Grid_df_2 = data.frame( y = exp(c( Grid_results2[, 1], R1_results2[, 1], R0_results2[, 1] )) * mean(yearmeans), x = rep(66:96, times = 3), ymin = exp(c(Post_LCL2, R1_results2[, 3], R0_results2[, 3])) * mean(yearmeans), ymax = exp(c(Post_UCL2, R1_results2[, 4], R0_results2[, 4])) * mean(yearmeans), group = c( rep(&#39;Whole UK&#39;, times = 31), rep(&#39;R1&#39;, times = 31), rep(&#39;R0&#39;, times = 31) ) ) ## plotting Grid_plot_2 = ggplot(aes( x = x, y = y, ymin = ymin, ymax = ymax, alpha = 0.05 ), data = Grid_df_2) + geom_ribbon(aes( colour = group, alpha = 0.05, fill = group )) + xlab(&#39;year&#39;) + ylab(&#39;posterior mean bs on transformed scale&#39;) + ggtitle( &#39;Naive posterior means of BS at the selected, unselected sites and across the UK (R1, R0 and Whole UK resp.). &#39; ) + theme_classic() Grid_plot_2 "],["Disease.html", "Chapter 9 Disease-spatial patterns Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 Example 9.3: Fitting a conditional spatial model in NIMBLE and RStan Example 9.4: Fitting a conditional spatial model using CARBayes Example 9.5: Fitting a conditional model using INLA Supplementary Example: Fitting a Leroux model to the COPD dataset using Nimble, Stan and CARBayes", " Chapter 9 Disease-spatial patterns This chapter contains an introduction to different types of spatial data, the theory of spatial lattice processes and introduces disease mapping and models for performing smoothing of risks over space. The reader will have gained an understanding of the following topics: disease mapping and how to improve estimates of risk by borrowing strength from adjacent regions which can reduce the instability inherent in risk estimates based on small (expected) numbers; how smoothing can be performed using either the empirical Bayes or fully Bayesian approaches; computational methods for handling areal data; Besag’s seminal contributions to the field of spatial statistics including the very important concept of a Markov random field; approaches to modelling areal data including conditional autoregressive models; how Bayesian spatial models for lattice data can be fit using NIMBLE, RStan and R–INLA. Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 Here we consider hospitalization for a respiratory condition, chronic obstructive pulmonary disease (COPD), in England in 2010. There are \\(N_l=324\\) local authority administrative areas each with an observed, \\(Y_l\\) and expected, \\(E_l\\), number of cases, \\(l=1,...,324\\). As described in Section 2.4 the expected numbers were calculated using indirect standardization by applying the age–sex specific rates for the whole of England to the age–sex population profile of each of the areas. In order to perform empirical Bayes smoothing, we will use the eBayes function in the SpatialEpi package. This example uses the englandlocalauthority.shp, copdmortalityobserved.csv, and copdmortalityexpected.csv datasets. 9.0.1 Empirical Bayes estimate using SpatialEpi # requires SpatialEpi package library (SpatialEpi) library(sf) # package used to read shapefiles # Laoding the borders of England england &lt;- read_sf(&quot;data/copd/englandlocalauthority.shp&quot;) # Loading the data observed &lt;- read.csv(file = &quot;data/copd/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copd/copdmortalityexpected.csv&quot;, row.names = 1) #observed data Y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 RRs = eBayes (Y , E ) plot ( RRs $ SMR , RRs $ RR , xlim = c (0 ,2) , ylim = c (0 ,2) , xlab = &quot; SMRs &quot; , ylab = &quot; RRs &quot; ) abline ( a =0 , b =1 , col = &quot; red &quot; , lwd =3) The smoothing can be seen with lower and higher SMRs being brought close to the overall average, \\(\\mu\\) which in this case is \\(\\exp(-0.0309) = 0.9696\\). Note that in this example, the areas are relatively large and would be expected to have substantial populations, so the effect of the smoothing is limited. In this example, \\(\\alpha\\) was estimated to be 14.6. For a fully Bayesian, rather than empirical Bayes, analysis we can use NIMBLE; and this is what we present next. Nimble library(nimble) Example9_1Nimble &lt;- nimbleCode({ for (i in 1:N) { Y[i] ~ dpois(mu[i]) # REVIEW: There is an intercept in the book, # but then we wouldn&#39;t be able to compare it to example 5.2 mu[i] &lt;- E[i]*exp(beta0)* theta[i] #mu[i] &lt;- E[i]* theta[i] # REVIEW: Same as before, the example in the book has theta[i] ~ Ga(a,a) theta[i] ~ dgamma(a, a) Y.fit[i] ~ dpois(mu[i]) } # Priors a ~ dgamma(1,1) beta0 ~ dnorm(0, 10) }) # Define the constants, data and initials lists for the `nimble` model. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 N &lt;- length(y) # constants list constants &lt;- list( N = N, E = E ) # data list ex.data &lt;- list(Y = y) # initial values list inits &lt;- list( theta = rgamma(N, 1, 1), a = rgamma(1, 1), beta0 = 0, Y.fit = rpois(N, E) ) # parameters to monitor params &lt;- c(&quot;theta&quot;, &quot;a&quot;, &quot;beta0&quot;, &quot;Y.fit&quot;) # Run model in nimble mcmc.out &lt;- nimbleMCMC( code = Example9_1Nimble, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 50000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) Effective sample size is checked through the minimum value among all the samples available. This is to check if this minimum is acceptable. We also provide trace plots of the chains for some of the parameters. We provide the command to obtain WAIC in case model comparison will be performed in a later stage. min(coda::effectiveSize(mcmc.out$samples)) ## [1] 948.4589 mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] 2422.992 ## Field &quot;lppd&quot;: ## [1] -1061.364 ## Field &quot;pWAIC&quot;: ## [1] 150.132 #storing the samples from the posterior distribution in mvSamples mvSamples &lt;- mcmc.out$samples # trace plot of a plot(mvSamples[, c(&quot;a&quot;)], bty = &quot;n&quot;) # trace plot of beta0 plot(mvSamples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;) # trace plots of theta for (i in 1:3) plot(mvSamples[, c(paste(&quot;theta[&quot;, i, &quot;]&quot;, sep = &quot;&quot;))], bty = &quot;n&quot;) Now that we have checked the convergence of the chains we can plot the posterior mean and 95% CIs for each of the parameters. # Print posterior summary for parameters a and b summary(mvSamples[, c(&quot;a&quot;, &quot;beta0&quot;)]) ## ## Iterations = 1:2142 ## Thinning interval = 1 ## Number of chains = 2 ## Sample size per chain = 2142 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## a 12.97734 1.23282 0.0188354 0.0193663 ## beta0 -0.03137 0.01673 0.0002556 0.0005469 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## a 10.7242 12.1384 12.92600 13.75815 15.546804 ## beta0 -0.0642 -0.0427 -0.03132 -0.02042 0.001347 # posterior summaries of theta_i post_summary &lt;- mcmc.out$summary$all.chains |&gt; as.data.frame() |&gt; tibble::rownames_to_column(&quot;variable&quot;) # plot the mean and 95% CIs for the thetas post_theta &lt;- post_summary[grepl(&quot;theta\\\\[&quot;, post_summary$variable),] par(mfrow = c(1, 1)) plot( post_theta$Mean, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(post_theta$`95%CI_low`), max(post_theta$`95%CI_upp`)) ) for (i in 1:N) segments(i, post_theta$`95%CI_low`[i], i, post_theta$`95%CI_upp`[i]) abline(h = 1, lwd = 2, lty = 2) # posterior summary of fitted values post_fitted &lt;- post_summary[grepl(&quot;Y.fit\\\\[&quot;, post_summary$variable),] # plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, post_fitted$Mean, ylim = c(min(post_fitted$`95%CI_low`), max(post_fitted$`95%CI_upp`)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], post_fitted$`95%CI_low`[i], y[i], post_fitted$`95%CI_upp`[i]) abline(a = 0, b = 1) Now we present the code for the same model in {}. We start by cleaning the ´R environment`. Stan library(rstan) library(sf) # to read shapefile library(loo) # To calculate WAIC options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Load data # Reading in borders england &lt;- read_sf(&quot;data/copd/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copd/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copd/copdmortalityexpected.csv&quot;, row.names = 1) The Stan model is in a separate file called Example9_1.stan that will be called later. data { int&lt;lower=0&gt; N; real&lt;lower=0&gt; E[N]; // need to indicate that variable is strictly positive int&lt;lower=0&gt; Y[N]; } parameters { real&lt;lower=0&gt; theta[N]; real&lt;lower=0&gt; a; real beta0; } transformed parameters{ real &lt;lower=0&gt; mu[N]; for(i in 1:N){ mu[i]=E[i]*theta[i]*exp(beta0); } } model { // likelihood function and prior for theta for(i in 1:N){ Y[i] ~ poisson(mu[i]); theta[i]~gamma(a,a); } a~gamma(1, 1); beta0~normal(0,10); } generated quantities { vector [N] log_lik; int&lt;lower=0&gt; yfit [N]; //computing the log_likelihood for each value of the mean mu and the fitted values for(i in 1:N){ log_lik[i]=poisson_lpmf(Y[i] |mu[i]); yfit[i]=poisson_rng(mu[i]); } } Similarly to NIMBLE, we now define the different objects that will contain information about the data. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 N &lt;- length(y) # data list ex.data &lt;- list( N = length(y), Y = y, E = E ) # Run the model in Stan Ex9_1Stan &lt;- stan( file = &quot;functions/Example9_1.stan&quot;, data = ex.data, chains = 3, iter = 10000, warmup = 3000, thin = 14, # QUESTION: should we explain this? control = list(adapt_delta = 0.8, max_treedepth = 15), init = &quot;random&quot;, pars = c(&quot;a&quot;,&quot;beta0&quot;,&quot;theta&quot;, &quot;log_lik&quot;, &quot;yfit&quot;), include = TRUE ) We now check the trace plots of some of the parameters, check the effective sample size. Once convergence has been checked we obtain the posterior summaries of some of the parameters. # traceplots of parameters a and b rstan::traceplot(Ex9_1Stan, pars = c(&quot;a&quot;,&quot;beta0&quot;)) # traceplots of parameter theta rstan::traceplot(Ex9_1Stan, pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;, &quot;theta[3]&quot;)) # posterior summaries together with Rhat rstan::traceplot(Ex9_1Stan, pars = c(&quot;a&quot;,&quot;theta[1]&quot;, &quot;theta[2]&quot;, &quot;theta[3]&quot;)) rstan::summary(Ex9_1Stan, pars = c(&quot;a&quot;,&quot;theta[1]&quot;, &quot;theta[2]&quot;, &quot;theta[3]&quot;)) ## $summary ## mean se_mean sd 2.5% 25% 50% ## a 12.9490131 0.030133132 1.21097052 10.7279153 12.1160782 12.8953865 ## theta[1] 0.8681535 0.005872796 0.22967964 0.4789593 0.6960057 0.8597968 ## theta[2] 1.3269027 0.004045516 0.14569941 1.0538612 1.2254400 1.3209029 ## theta[3] 0.7293979 0.002208173 0.07439276 0.5962105 0.6774943 0.7211274 ## 75% 97.5% n_eff Rhat ## a 13.729623 15.3851313 1615.023 0.9998414 ## theta[1] 1.016351 1.3539740 1529.520 0.9999455 ## theta[2] 1.419336 1.6207442 1297.083 1.0004835 ## theta[3] 0.777150 0.8885225 1134.998 1.0007149 ## ## $c_summary ## , , chains = chain:1 ## ## stats ## parameter mean sd 2.5% 25% 50% 75% ## a 12.9391260 1.20688131 10.6647963 12.1367051 12.8610229 13.7066434 ## theta[1] 0.8544009 0.21847768 0.4610477 0.7025649 0.8516174 0.9842713 ## theta[2] 1.3309053 0.14915119 1.0589718 1.2319590 1.3220494 1.4277758 ## theta[3] 0.7328573 0.07826966 0.5966028 0.6769945 0.7249705 0.7835412 ## stats ## parameter 97.5% ## a 15.5179697 ## theta[1] 1.3197144 ## theta[2] 1.6162752 ## theta[3] 0.8917676 ## ## , , chains = chain:2 ## ## stats ## parameter mean sd 2.5% 25% 50% 75% ## a 12.9631607 1.23242884 10.7069049 12.1322796 12.9113706 13.7432376 ## theta[1] 0.8706192 0.23177021 0.4891381 0.6946233 0.8573132 1.0206591 ## theta[2] 1.3338165 0.14394937 1.0460653 1.2392597 1.3249056 1.4239841 ## theta[3] 0.7297883 0.07502032 0.5907728 0.6779665 0.7205219 0.7788053 ## stats ## parameter 97.5% ## a 15.3983920 ## theta[1] 1.3767840 ## theta[2] 1.6308883 ## theta[3] 0.8904337 ## ## , , chains = chain:3 ## ## stats ## parameter mean sd 2.5% 25% 50% 75% ## a 12.9447526 1.19560290 10.7837245 12.0674517 12.9678053 13.7452054 ## theta[1] 0.8794403 0.23811021 0.4884869 0.6921386 0.8671821 1.0340745 ## theta[2] 1.3159863 0.14359380 1.0602166 1.2078908 1.3137105 1.4073229 ## theta[3] 0.7255480 0.06959774 0.6008815 0.6771501 0.7190926 0.7718208 ## stats ## parameter 97.5% ## a 15.2901383 ## theta[1] 1.3849875 ## theta[2] 1.6040231 ## theta[3] 0.8703692 # To be able to compute the WAIC in Stan, we need to define the log_lik quantity in the generated quantities block # Compute WAIC loglik0 &lt;- extract_log_lik(Ex9_1Stan) waic0 &lt;- waic(loglik0) waic0 ## ## Computed from 1500 by 324 log-likelihood matrix ## ## Estimate SE ## elpd_waic -1210.4 8.0 ## p_waic 149.3 3.9 ## waic 2420.8 16.0 ## ## 176 (54.3%) p_waic estimates greater than 0.4. We recommend trying loo instead. Note that the ´summaryfunction of aRStan` object provides the effective sample size and Rhat. We now plot the posterior summaries of \\(\\theta_i\\) and each of the fitted values with their respective 95% posterior credible intervals. summary_theta &lt;- summary(Ex9_1Stan, pars = c(&quot;theta&quot;), probs = c(0.05, 0.95))$summary |&gt; as.data.frame() par(mfrow = c(1, 1)) plot( summary_theta$mean, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(summary_theta$`5%`), max(summary_theta$`95%`)) ) for (i in 1:N) segments(i, summary_theta$`5%`[i], i, summary_theta$`95%`[i]) abline(h = 1, lwd = 2, lty = 2) # Posterior summary of fitted values summary_fit &lt;- summary(Ex9_1Stan, pars = c(&quot;yfit&quot;), probs = c(0.05, 0.95))$summary |&gt; as.data.frame() # Plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, summary_fit$mean, ylim = c(min(summary_fit$`5%`), max(summary_fit$`95%`)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], summary_fit$`5%`[i], y[i], summary_fit$`95%`[i]) abline(a = 0, b = 1) Example 9.3: Fitting a conditional spatial model in NIMBLE and RStan In this example, we see how to fit the Poisson log–normal model seen in Section 9.4 with spatial random effects coming from the ICAR model described in Section 9.3.1. This example uses the englandlocalauthority.shp, copdmortalityobserved.csv, and copdmortalityexpected.csv datasets. Nimble In this example, we see how to fit the Poisson log–normal model seen in Section 9.4 with spatial random effects coming from the ICAR model described in Section 9.3.1. We start by loading some packages and the data. library(ggplot2) # to plot map library(spdep) # read the shapefile (read_sf) and build neighbors list (poly2nb) library(nimble) # Load data # Reading in borders england &lt;- read_sf(&quot;data/copd/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copd/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copd/copdmortalityexpected.csv&quot;) covariates &lt;- read.csv(file = &quot;data/copd/copdavgscore.csv&quot;) # Merge everything into one data frame copd_df &lt;- cbind(observed, expected) |&gt; merge( covariates, by.x = &quot;code&quot;, by.y = &quot;LA.CODE&quot;, all.x = TRUE, all.y = FALSE ) The following command provides a scatter plot of the average deprivation score and the estimated SMR for 2010. The plot suggests that as deprivation increases so does the SMR for 2010. copd_df$SMR2010 = copd_df$Y2010 / copd_df$E2010 ggplot(copd_df) + geom_point(aes(x = Average.Score, y = SMR2010)) + theme_classic() + ylab(&quot;SMR 2010&quot;) + xlab(&quot;Average deprivation score&quot;) To fit the ICAR model in Nimble we need to obtain the neighborhood matrix \\(W\\). We start by defining a function that computes the number of neighbors for each area. adjlist = function(W, N) { adj = 0 for (i in 1:N) { for (j in 1:N) { if (W[i, j] == 1) { adj = append(adj, j) } } } adj = adj[-1] return(adj) } Next we obtain the adjacency matrix, W,Define the adjacency matrix and indexes for stan using the functions nb2mat and adjlist from package spdep. # Create the neighborhood W.nb &lt;-poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) # Define the spatial structure to use in stan N &lt;- length(unique(england$ID)) #creating a vector listing the ID numbers of the adjacent areas for each area neigh &lt;- adjlist(W.mat, N) # computing a vector of length N (the total number of areas) giving the number of neighbors for each area. numneigh &lt;- apply(W.mat,2,sum) Example9_3Nimble &lt;- nimbleCode({ # Likelihood for (i in 1:N) { y[i] ~ dpois(lambda[i]) log(lambda[i]) &lt;- log(E[i]) + b[i] + beta0 + beta1*x[i] } # Priors beta0 ~ dnorm(0, sd = 1) beta1 ~ dnorm(0, sd = 1) b[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1) tau &lt;- 1 / (sigma_b ^ 2) #prior for sigma_b is a half-normal distribution sigma_b ~ T(dnorm(0, sd = 1), 0,) # Fitted values and likelihood for WAIC for (i in 1:N) { fitted[i] ~ dpois(lambda[i]) } }) # Define the constants, data and initial values lists and run the model. # constants list constants &lt;- list( N = N, E = copd_df$E2010, L = length(neigh), adj = neigh, weights = rep(1, length(neigh)), num = as.vector(numneigh), p = 3 ) # data list ex.data &lt;- list(y = copd_df$Y2010, x = as.vector(copd_df$Average.Score)) # vector of covariates inits &lt;- list( beta0 = rnorm(1), beta1 = rnorm(1), fitted = rpois(N, 2), sigma_b = 1, b = rnorm(N) ) params &lt;- c(&quot;beta0&quot;,&quot;beta1&quot;, &quot;fitted&quot;, &quot;b&quot;, &quot;tau&quot;) # Run model in nimble mcmc.out &lt;- nimbleMCMC( code = Example9_3Nimble, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 20, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) Next we double-check convergence of the chains through trace plots and the effective sample sizes for some of the parameters. min(coda::effectiveSize(mcmc.out$samples)) ## [1] 106.484 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;) plot(mcmc.out$samples[, c(&quot;b[2]&quot;)], bty = &quot;n&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;) Posterior summaries of the intercept, the fixed effect for average deprivation index and the precision of the ICAR prior distribution. # Extract samples variables &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;,&quot;tau&quot;) summary_CAR_nimble &lt;- mcmc.out$summary$all.chains summary_CAR_nimble[variables,] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 -0.48208299 -0.48319524 0.029720605 -0.54251409 -0.42199775 ## beta1 0.02158382 0.02160515 0.001446477 0.01868493 0.02446601 ## tau 15.38297154 15.18218615 2.462509582 11.19231669 20.87252479 Map of the posterior mean of the latent effects. samples_CAR_b &lt;- summary_CAR_nimble[grepl(&quot;b\\\\[&quot;, rownames(summary_CAR_nimble)), ] |&gt; as.data.frame() observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) samples_CAR_b$ID &lt;- observed$ID CAR_nimble_merge &lt;- merge(england, samples_CAR_b, by = &quot;ID&quot;) ggplot() + # Choose spatial object and column for plotting geom_sf(data = CAR_nimble_merge, aes(fill = Mean)) + # Change legend&#39;s label labs(fill = &#39;Latent effects under Nimble&#39;) + # Clear background and plot borders theme_void() Now we perform the same analysis using Stan. Note that, different from Nimble, the sum-to-zero constraint in Stan is a soft one. Stan As Stan does not have a specific function for the ICAR prior we need to compute the neighborhood structure through the following functions which were obtained from this site: adjlist = function(W, N) { adj = 0 for (i in 1:N) { for (j in 1:N) { if (W[i, j] == 1) { adj = append(adj, j) } } } adj = adj[-1] return(adj) } mungeCARdata4stan = function(adjBUGS, numBUGS) { N = length(numBUGS) nn = numBUGS N_edges = length(adjBUGS) / 2 node1 = vector(mode = &quot;numeric&quot;, length = N_edges) node2 = vector(mode = &quot;numeric&quot;, length = N_edges) iAdj = 0 iEdge = 0 for (i in 1:N) { for (j in 1:nn[i]) { iAdj = iAdj + 1 if (i &lt; adjBUGS[iAdj]) { iEdge = iEdge + 1 node1[iEdge] = i node2[iEdge] = adjBUGS[iAdj] } } } return (list( &quot;N&quot; = N, &quot;N_edges&quot; = N_edges, &quot;node1&quot; = node1, &quot;node2&quot; = node2 )) } This model is in a separate file called Example9_3.stan that will be called later. data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; N_edges; int&lt;lower=1&gt; p; matrix[N,p] X; int&lt;lower=1, upper=N&gt; node1[N_edges]; // node1[i] adjacent to node2[i] int&lt;lower=1, upper=N&gt; node2[N_edges]; // and node1[i] &lt; node2[i] int&lt;lower=0&gt; y[N]; // count outcomes vector&lt;lower=0&gt;[N] E; // exposure } transformed data { vector[N] log_E = log(E); } parameters { real beta0; // intercept vector[p] beta; vector[N] s; // spatial effects real&lt;lower=0&gt; sigma_s; // marginal standard deviation of spatial effects } transformed parameters { vector[N] b; // latent effect real &lt;lower=0&gt; riskdep = exp(beta[1]); b = sigma_s*s; } model { y ~ poisson_log(log_E + beta0 + X*beta + b); // This is the prior for s! (up to proportionality) target += -0.5 * dot_self(s[node1] - s[node2]); //soft sum-to-zero constraint sum(s) ~ normal(0, 0.001 * N); beta0 ~ normal(0.0, 10.0); for(j in 1:p){ beta[j] ~ normal(0.0, 10.0); } sigma_s ~ normal(0.0,1.0); } generated quantities { vector[N] mu=exp(log_E + beta0 + X*beta + b); vector[N] lik; vector[N] log_lik; for(i in 1:N){ lik[i] = exp(poisson_lpmf(y[i] | mu[i] )); log_lik[i] = poisson_lpmf(y[i] | mu[i] ); } } Define the adjacency matrix and the set of indices of the neighboring areas using functions nb2mat and adjlist: # Create the neighborhood W.nb &lt;- poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) # Define the spatial structure to use in stan N &lt;- length(unique(england$ID)) neigh &lt;- adjlist(W.mat, N) numneigh &lt;- apply(W.mat, 2, sum) nbs &lt;- mungeCARdata4stan(neigh, numneigh) # Define data and variables for Stan model y &lt;- copd_df$Y2010 E &lt;- copd_df$E2010 X &lt;- as.numeric(copd_df$Average.Score) ex.data &lt;- list( N = nbs$N, y = y, E = E, p = 1, X = as.matrix(X), N_edges = nbs$N_edges, node1 = nbs$node1, node2 = nbs$node2 ) Example9_3Stan &lt;- stan( file = &quot;functions/Example9_3.stan&quot;, data = ex.data, warmup = 2000, iter = 10000, chains = 2, thin = 8, pars = c(&quot;beta0&quot;, &quot;beta&quot;,&quot;sigma_s&quot;, &quot;riskdep&quot;, &quot;b&quot;, &quot;log_lik&quot;), include = TRUE ) Trace plots of the posterior samples for some of the parameters. #traceplots of some parameters rstan::traceplot(Example9_3Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma_s&quot;)) We now obtain the posterior summaries of the intercept, the fixed effect associated with the deprivation score and the standard deviation of the random effect. The quantity riskdep shows the relative risk associated with an one unit increase in deprivation. # Extract samples summary_CAR_stan &lt;- summary( Example9_3Stan, pars = c(&quot;beta0&quot;, &quot;beta&quot;,&quot;sigma_s&quot;,&quot;riskdep&quot;), probs = c(0.025, 0.975) ) summary_CAR_stan$summary ## mean se_mean sd 2.5% 97.5% n_eff ## beta0 -0.4831513 7.614790e-04 0.029746646 -0.54094962 -0.42631806 1526.020 ## beta[1] 0.0216228 3.757970e-05 0.001446513 0.01884916 0.02450198 1481.623 ## sigma_s 0.2581699 5.230906e-04 0.021200471 0.21816947 0.30228148 1642.621 ## riskdep 1.0218593 3.840191e-05 0.001478135 1.01902793 1.02480462 1481.572 ## Rhat ## beta0 1.000210 ## beta[1] 1.000255 ## sigma_s 1.000640 ## riskdep 1.000257 The result suggests that an one unit increase in deprivation, increases the relative risk by 2.2%. Recall that the standard deviation of the ICAR prior refers to the standard deviation of a conditional distribution. We now plot the posterior means of the latent effects summary_CAR_stan_b &lt;- summary( Example9_3Stan, pars = c(&quot;b&quot;), probs = c(0.025, 0.975) ) observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) summary_CAR_stan_b$ID &lt;- observed$ID CAR_stan_merge &lt;- merge(england, summary_CAR_stan_b, by = &quot;ID&quot;) ggplot() + # Choose spatial object and column for plotting geom_sf(data = CAR_stan_merge, aes(fill = summary.mean)) + # Change legend&#39;s label labs(fill = &#39;Latent effects stan&#39;) + # Clear background and plot borders theme_void() Example 9.4: Fitting a conditional spatial model using CARBayes Here we consider fitting the Poisson log–normal model with spatial effects to the data for respiratory admissions seen in Example 9.3 using the R package CARBayes. The adjacency matrix is obtained using functions of the package spdep. CARBayes performs MCMC simulation similar to NIMBLE but has the distinct advantage that you just call specific functions to run the model. library(CARBayes) library(ggplot2) library(sf) library(spdep) # Reading in borders england &lt;- read_sf(&quot;data/copd/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copd/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copd/copdmortalityexpected.csv&quot;) covariates &lt;- read.csv(file = &quot;data/copd/copdavgscore.csv&quot;) # Merge everything into one data frame copd_df &lt;- cbind(observed, expected) |&gt; merge( covariates, by.x = &quot;code&quot;, by.y = &quot;LA.CODE&quot;, all.x = TRUE, all.y = FALSE ) copd_df$Average.Score &lt;- as.numeric(scale(copd_df$Average.Score)) Next we compute the neighborhood matrix through poly2nb() and nb2mat() from the spdep. # Create the neighborhood W.nb &lt;-poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) The function S.CARleroux() allows us to use the neighborhood structure and performs a Bayesian analysis assuming a CAR prior distribution to the random effects. This is obtained by setting the parameter rho equals 1 in the call of S.CARleroux(). # Running smoothing model Ex9_4 &lt;- S.CARleroux( # Model Formula formula = Y2010 ~ offset(log(E2010)) + Average.Score, # data frame with data data = copd_df, # Choosing Poisson Regression family = &quot;poisson&quot;, # Neighborhood matrix W = W.mat, # Number of burn in samples burnin = 20000, # Number of MCMC samples n.sample = 100000, thin = 10, #ICAR model given a 0-1 neighborhood structure rho = 1 ) Using ggplot() and geom_sf() to plot the map of the latent spatial effects. observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) phi_car &lt;- Ex9_4$samples$phi latent_car_df &lt;- data.frame( phi_mean = apply(phi_car, 2, mean), phi_sd = apply(phi_car, 2, sd) ) # Combine latent spatial effect with england dataset latent_car_df$ID &lt;- observed$ID latent_car_england &lt;- merge(england, latent_car_df, by = &quot;ID&quot;) # Creating map of smoothed SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = latent_car_england, aes(fill = phi_mean)) + labs(fill = &#39;Latent effects CARbayes&#39;) + # Clear background and plot borders theme_void() Example 9.5: Fitting a conditional model using INLA Now we fit the Poisson log–normal model with spatial effects to the data for respiratory admissions seen in Example 9.3 using R-INLA. Recall that R-INLA approximates the resultant posterior distribution using an integrated nested Laplace approximation. The call to fit the model using R-INLA is very similar to the one when fitting a glm. The latent effects are introduced through the function f(.). # run the INLA model Ex9_5 &lt;- inla( Y2010 ~ Average.Score + f(ID , model = &quot;besag&quot;, graph = &quot;UK.adj&quot;), family = &quot;poisson&quot;, E = E2010, data = copd_df, control.predictor = list(compute = TRUE) ) Summarize the results # Summarizing smoothed SMRs summary(Ex9_5) ## ## Call: ## c(&quot;inla.core(formula = formula, family = family, contrasts = contrasts, ## &quot;, &quot; data = data, quantiles = quantiles, E = E, offset = offset, &quot;, &quot; ## scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, ## &quot;, &quot; lp.scale = lp.scale, link.covariates = link.covariates, verbose = ## verbose, &quot;, &quot; lincomb = lincomb, selection = selection, control.compute ## = control.compute, &quot;, &quot; control.predictor = control.predictor, ## control.family = control.family, &quot;, &quot; control.inla = control.inla, ## control.fixed = control.fixed, &quot;, &quot; control.mode = control.mode, ## control.expert = control.expert, &quot;, &quot; control.hazard = control.hazard, ## control.lincomb = control.lincomb, &quot;, &quot; control.update = ## control.update, control.lp.scale = control.lp.scale, &quot;, &quot; ## control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, ## &quot;, &quot; inla.call = inla.call, inla.arg = inla.arg, num.threads = ## num.threads, &quot;, &quot; blas.num.threads = blas.num.threads, keep = keep, ## working.directory = working.directory, &quot;, &quot; silent = silent, inla.mode ## = inla.mode, safe = FALSE, debug = debug, &quot;, &quot; .parent.frame = ## .parent.frame)&quot;) ## Time used: ## Pre = 1.02, Running = 0.289, Post = 0.108, Total = 1.42 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) -0.069 0.008 -0.084 -0.069 -0.054 -0.069 0 ## Average.Score 0.182 0.012 0.158 0.182 0.206 0.182 0 ## ## Random effects: ## Name Model ## ID Besags ICAR model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for ID 15.94 2.62 11.52 15.70 21.75 15.23 ## ## Marginal log-Likelihood: -1493.27 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) phi_car &lt;- Ex9_5$summary.random$ID latent_car_df &lt;- data.frame( phi_mean = phi_car$mean, phi_sd = phi_car$sd ) # Combine latent spatial effect with england dataset latent_car_df$ID &lt;- observed$ID latent_car_england &lt;- merge(england, latent_car_df, by = &quot;ID&quot;) # Creating map of smoothed SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = latent_car_england, aes(fill = phi_mean)) + labs(fill = &#39;Latent effects INLA&#39;) + # Clear background and plot borders theme_void() Supplementary Example: Fitting a Leroux model to the COPD dataset using Nimble, Stan and CARBayes –&gt; –&gt; "],["hazards.html", "Chapter 10 Environmental hazards-spatial models Example 10.1 Spatial patterns in lead concentrations in the soil of the Meuse River flood plain Example 10.2: Examining the log concentrations of lead in the Meuse River flood plain Example 10.3: Mapping the locations of ozone monitoring sites in New York State Example 10.5: Spatial prediction of temperatures in California Example 10.6 Fitting a Bayesian exponential spatial model using NIMBLE Example 10.7 Spatial prediction of \\(PM_{2.5}\\) in Europe Example 10.10 Creating a mesh: black smoke monitoring locations in the UK Example 10.12 Plotting directional variograms for temperatures in California Example 10.14: Spatial modeling of malaria in Gambia Supplementary Material", " Chapter 10 Environmental hazards-spatial models This chapter contains the basic theory for spatial processes and a number of approaches to modelling point-referenced spatial data. From this chapter, the reader will have gained an understanding of the following topics: Visualization techniques needed for both exploring and analyzing spatial data and communicating its features through the use of maps. Exploring the underlying structure of spatial data and methods for characterizing dependence over space. Second-order theory for spatial processes including the covariance. The variogram for measuring spatial associations. Stationarity and isotropy. Methods for spatial prediction, using both classical methods (kriging) as well as modern methods (Bayesian kriging). Non-stationarity fields. Example 10.1 Spatial patterns in lead concentrations in the soil of the Meuse River flood plain library(geoR) library(leaflet) library(sp) # Load data data(meuse) coordinates(meuse)&lt;-~x+y # convert to SPDF # Assign Netherlands reference system proj4string(meuse) &lt;- CRS(&#39;+init=epsg:28992&#39;) # Convert to longlat reference system meuse_geo &lt;- spTransform(meuse,CRS(&quot;+proj=longlat +datum=WGS84&quot;)) ## Warning: PROJ support is provided by the sf and terra packages among others meuse_df &lt;- as.data.frame(meuse_geo) Plot Figure 10.1 Bubble map range_values &lt;- range(meuse_df$lead) # Define bins based on the range of values mybins &lt;- seq(range_values[1], range_values[2], length.out = 5) mypalette &lt;- colorBin(palette=&quot;inferno&quot;, domain=meuse_df$lead, na.color=&quot;transparent&quot;, bins=mybins) leaflet( data = meuse_df) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ x, ~ y, fillOpacity = 0.8, color=&quot;orange&quot;, fillColor = ~ mypalette(lead), opacity = 5, radius = ~lead/50, stroke = FALSE) |&gt; addLegend( pal=mypalette, values=~lead, opacity=0.9, title = &quot;Lead&quot;, position = &quot;bottomright&quot; ) meuse_geodata &lt;- as.geodata(as.data.frame(meuse), coords.col = 1:2, data.col = 5) plot(meuse_geodata) Example 10.2: Examining the log concentrations of lead in the Meuse River flood plain par(mfrow = c(1, 2)) log_histogram &lt;- hist(log(meuse$lead), main = &quot;&quot;, xlab = &quot;log(lead)&quot;) qqnorm(log(meuse$lead), bty = &quot;n&quot;) qqline(log(meuse$lead)) meuse_df &lt;- as.data.frame(meuse) coplot( lead ~ y | x, data = meuse_df) Example 10.3: Mapping the locations of ozone monitoring sites in New York State This example uses the coordinates in the NY_metadata.txt file. # Load the metadata giving the site coordinates ny_data &lt;- read.csv(&quot;data/ozone/NY_metadata.txt&quot;, sep = &quot;&quot;) # Now copy ny_data into ny_data_sp and convert data to &quot;sp&quot; format ny_data_sp &lt;- ny_data coordinates(ny_data_sp) &lt;- ~ Longitude + Latitude # assign a reference system to ny_data_sp proj4string(ny_data_sp) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) leaflet( data = ny_data) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ Longitude, ~ Latitude, fillOpacity = 0.8, color=&quot;orange&quot;, opacity = 5, radius = 4, stroke = FALSE) Example 10.5: Spatial prediction of temperatures in California # REVIEW: There is something wrong with this dataset, it is impossible to have this temperatures # most likely all the data has to be divided by 10 library(geoR) library(gstat) library(sp) ### First we load the data CAmetadata &lt;- read.table(&quot;data/metadataCA.txt&quot;, header = TRUE) CATemp &lt;- read.csv(&quot;data/MaxCaliforniaTemp.csv&quot;, header = TRUE) CATemp_20120901 &lt;- CATemp[CATemp$X == 20120901, -c(1,14)] CATemp_20120901 &lt;- t(CATemp_20120901) ### Change names of lat, long names(CAmetadata)[names(CAmetadata)==&quot;Long&quot;]&lt;-&quot;x&quot; names(CAmetadata)[names(CAmetadata)==&quot;Lat&quot;]&lt;-&quot;y&quot; ### Augment data file with coordinates CATemp_20120901 &lt;- cbind(CAmetadata, CATemp_20120901/10) names(CATemp_20120901)[names(CATemp_20120901) == &quot;245&quot;] &lt;- &quot;Temp&quot; coordinates(CATemp_20120901) &lt;- ~x +y CAgstat.vario &lt;- variogram(Temp ~ x+y, CATemp_20120901) #CAgstat.vario &lt;- variogram(Temp ~ 1, CATemp_20120501, print.SSE = TRUE) CATemp.variofit &lt;- fit.variogram(CAgstat.vario, vgm(10, &quot;Exp&quot;, 1, 0.1)) # Plot variogram and fit plot(CAgstat.vario, CATemp.variofit, bty = &quot;n&quot;) ### Examine the fit summary(CATemp.variofit) ## model psill range kappa ang1 ## Nug :1 Min. : 0.000 Min. :0.0000 Min. :0.000 Min. :0 ## Exp :1 1st Qu.: 3.713 1st Qu.:0.2813 1st Qu.:0.125 1st Qu.:0 ## Sph :0 Median : 7.425 Median :0.5627 Median :0.250 Median :0 ## Gau :0 Mean : 7.425 Mean :0.5627 Mean :0.250 Mean :0 ## Exc :0 3rd Qu.:11.138 3rd Qu.:0.8440 3rd Qu.:0.375 3rd Qu.:0 ## Mat :0 Max. :14.851 Max. :1.1254 Max. :0.500 Max. :0 ## (Other):0 ## ang2 ang3 anis1 anis2 ## Min. :0 Min. :0 Min. :1 Min. :1 ## 1st Qu.:0 1st Qu.:0 1st Qu.:1 1st Qu.:1 ## Median :0 Median :0 Median :1 Median :1 ## Mean :0 Mean :0 Mean :1 Mean :1 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:1 3rd Qu.:1 ## Max. :0 Max. :0 Max. :1 Max. :1 ## # Create the grid on which to make the spatial predictions CAPred.loci &lt;- expand.grid(seq(-125, -115, 0.1), seq(32, 42, 0.1)) names(CAPred.loci)[names(CAPred.loci) == &quot;Var1&quot;] &lt;- &quot;x&quot; names(CAPred.loci)[names(CAPred.loci) == &quot;Var2&quot;] &lt;- &quot;y&quot; coordinates(CAPred.loci) = ~ x + y gridded(CAPred.loci) = TRUE mod &lt;- vgm(14, &quot;Exp&quot;, 1.1, 0.01) # Get the monitoring locations monitor_loc &lt;- list(&#39;sp.points&#39;, CATemp_20120901, pch=19, cex=.8, col=&#39;cornsilk4&#39;) # ordinary kriging: x &lt;- krige(Temp ~ 1, CATemp_20120901, CAPred.loci, model = mod) ## [using ordinary kriging] spplot(x[&quot;var1.pred&quot;], main = &quot;ordinary kriging predictions&quot;, sp.layout = list(monitor_loc), ) spplot(x[&quot;var1.var&quot;], main = &quot;ordinary kriging variance&quot;, sp.layout = list(monitor_loc), ) Example 10.6 Fitting a Bayesian exponential spatial model using NIMBLE Example 10.7 Spatial prediction of \\(PM_{2.5}\\) in Europe library(gdata) library(leaflet) library(gstat) library(nimble) library(sp) library(tidybayes) library(tidyverse) ditch_the_axes &lt;- theme( axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.title = element_blank()) #perl &lt;- &quot;C:/Perl64/bin/perl5.28.1.exe&quot; # add location of perl exe for read.xls perl &lt;- (&quot;C:/Strawberry/perl/bin/perl5.32.1.exe&quot;) no2 &lt;- read.xls(&quot;data/no2_europe/NO2Europe.xlsx&quot;, perl = perl) no2$AirPollutionLevel &lt;- as.numeric(no2$AirPollutionLevel) colnames(no2) ## [1] &quot;Country&quot; &quot;CountryCode&quot; ## [3] &quot;City&quot; &quot;AirQualityNetwork&quot; ## [5] &quot;AirQualityNetworkName&quot; &quot;AirQualityStation&quot; ## [7] &quot;AirQualityStationEoICode&quot; &quot;AQStationName&quot; ## [9] &quot;AirPollutant&quot; &quot;AirPollutantCode&quot; ## [11] &quot;DataAggregationProcess&quot; &quot;YearOfStatistics&quot; ## [13] &quot;AirPollutionLevel&quot; &quot;UnitOfAirpollutionLevel&quot; ## [15] &quot;DataCoverage&quot; &quot;Verification&quot; ## [17] &quot;AirQualityStationType&quot; &quot;AirQualityStationArea&quot; ## [19] &quot;Longitude&quot; &quot;Latitude&quot; ## [21] &quot;Altitude&quot; &quot;CountryName&quot; ## [23] &quot;CountryType&quot; &quot;DataAggregationProcess_original&quot; ## [25] &quot;AirPollutantCode_original&quot; &quot;AirPollutant_original&quot; ## [27] &quot;cut&quot; summary_no2 &lt;- group_by(no2, Country) |&gt; summarize(total = n(), mean_country = mean(AirPollutionLevel, na.rm = TRUE)) |&gt; as.data.frame() summary_no2[order(summary_no2$total, decreasing = TRUE),] ## Country total mean_country ## 30 Spain 465 15.544086 ## 12 Germany 396 21.464646 ## 11 France 330 18.215152 ## 17 Italy 280 21.771429 ## 33 United Kingdom 153 22.581699 ## 25 Poland 147 16.319728 ## 2 Austria 141 18.787234 ## 3 Belgium 70 19.571429 ## 7 Czechia 63 16.761905 ## 26 Portugal 45 18.044444 ## 24 Norway 40 22.625000 ## 22 Netherlands 39 18.564103 ## 32 Switzerland 32 21.312500 ## 31 Sweden 29 19.793103 ## 28 Slovakia 25 17.600000 ## 10 Finland 23 11.173913 ## 4 Bulgaria 20 20.650000 ## 14 Hungary 19 22.052632 ## 19 Lithuania 14 15.142857 ## 5 Croatia 12 17.250000 ## 16 Ireland 11 15.818182 ## 8 Denmark 10 14.600000 ## 23 North Macedonia 10 18.600000 ## 9 Estonia 9 7.111111 ## 15 Iceland 9 5.888889 ## 29 Slovenia 9 18.777778 ## 13 Greece 8 30.375000 ## 20 Luxembourg 6 18.000000 ## 27 Serbia 5 21.200000 ## 6 Cyprus 3 19.666667 ## 18 Latvia 3 22.000000 ## 1 Andorra 1 31.000000 ## 21 Malta 1 3.000000 mypalette &lt;- colorFactor(palette=&quot;inferno&quot;, domain=no2$Country) leaflet( data = no2) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ Longitude, ~ Latitude, color = ~ mypalette(Country) , fillOpacity = 0.8, opacity = 5, radius = 2, stroke = FALSE) set.seed(321) no2_sample &lt;- no2 |&gt; filter(Longitude &lt; 30 &amp; Longitude &gt; -25 &amp; Latitude &gt; 30 &amp; Latitude &lt; 70) no2_sample &lt;- no2_sample |&gt; filter(Country == &quot;Spain&quot; | Country == &quot;Germany&quot; | Country == &quot;France&quot; | Country == &quot;Italy&quot; | Country == &quot;United Kingdom&quot; | Country == &quot;Poland&quot; | Country == &quot;Austria&quot; ) no2_subsample &lt;- no2_sample |&gt; group_by(Country) |&gt; slice_sample(n = 128) dim(no2_subsample) ## [1] 896 27 mypalette &lt;- colorFactor(palette=&quot;inferno&quot;, domain=no2_subsample$Country) leaflet( data = no2_subsample) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ Longitude, ~ Latitude, color = ~ mypalette(Country) , fillOpacity = 0.8, opacity = 5, radius = 2, stroke = FALSE) # Function to get utm coordinates since we have different utm zones # This function was obtained from: https://stackoverflow.com/questions/71573206/converting-latitude-and-longitude-data-to-utm-with-points-from-multiple-utm-zone get_utm &lt;- function(x, y, zone, loc){ points = SpatialPoints(cbind(x, y), proj4string = CRS(&quot;+proj=longlat +datum=WGS84&quot;)) points_utm = spTransform(points, CRS(paste0(&quot;+proj=utm +zone=&quot;, zone[1],&quot; +ellps=WGS84&quot;))) if (loc == &quot;x&quot;) { return(coordinates(points_utm)[,1]) } else if (loc == &quot;y&quot;) { return(coordinates(points_utm)[,2]) } } no2_utm &lt;- rename(no2_subsample, x = Longitude, y = Latitude) no2_utm &lt;- mutate(no2_utm, zone2 = (floor((x + 180)/6) %% 60) + 1, keep = &quot;all&quot;) |&gt; group_by(zone2) |&gt; mutate(utm_x = get_utm(x, y, zone2, loc = &quot;x&quot;), utm_y = get_utm(x, y, zone2, loc = &quot;y&quot;))|&gt; as.data.frame() |&gt; dplyr::select(utm_x, utm_y, Altitude, Country, AirPollutionLevel) |&gt; mutate( ID = c(1:896)) no2_utm_geo &lt;- no2_utm # change coordinates to kms and turn into a Spatial object no2_utm_geo[,c(&quot;utm_x&quot;, &quot;utm_y&quot;)] &lt;- no2_utm_geo[,c(&quot;utm_x&quot;, &quot;utm_y&quot;)]/1000 coordinates(no2_utm_geo) &lt;- ~ utm_x + utm_y # Estimate variogram intercept only no2_inter_vgm &lt;- variogram(log(AirPollutionLevel) ~ 1, data = no2_utm_geo, cutoff = 200,# cutoff distance width = 200 / 10) # bins width no2_vgm_inter_fit &lt;- fit.variogram(no2_inter_vgm, model = vgm(0.8, &quot;Exp&quot;, 15, 0.007)) no2_vgm_inter_fit ## model psill range ## 1 Nug 0.1260258 0.00000 ## 2 Exp 0.2119263 28.07435 # Estimate variogram with coordinates and altitude no2_utm_vgm &lt;- variogram(log(AirPollutionLevel) ~ utm_x + utm_y + Altitude, data = no2_utm_geo, cutoff = 200,# cutoff distance width = 200 / 10) # bins width no2_vgm_fit &lt;- fit.variogram(no2_utm_vgm, model = vgm(0.8, &quot;Exp&quot;, 15, 0.007)) no2_vgm_fit ## model psill range ## 1 Nug 0.1350376 0.00000 ## 2 Exp 0.1667001 36.06112 plot_inter_variog &lt;- plot(no2_inter_vgm, no2_vgm_inter_fit, ylim=c(0,0.4), bty = &quot;n&quot;) plot_coord_variog &lt;- plot(no2_utm_vgm, no2_vgm_fit, ylim=c(0,0.4), bty = &quot;n&quot;) cowplot::plot_grid(plot_inter_variog, plot_coord_variog, labels = &quot;auto&quot;) # Change coordinates to kilometers and observations to the log scale no2_df &lt;- data.frame( y = log(no2_utm$AirPollutionLevel), easting = no2_utm$utm_x / 1000, northing = no2_utm$utm_y / 1000, altitude = no2_utm$Altitude ) # Compute the distance matrix obsCoords &lt;- unname(as.matrix(no2_df[,c(&quot;easting&quot;, &quot;northing&quot;)])) obsDist &lt;- fields::rdist(obsCoords) easting_scaled &lt;- as.vector(scale(no2_df$easting)) northing_scaled &lt;- as.vector(scale(no2_df$northing)) altitude_scaled &lt;- as.vector(scale(no2_df$altitude)) WARNING Due to the number of locations this model can take a long time to run in a regular computer (more than 3 hours) NO2Code &lt;- nimbleCode ({ # Covariance matrix spatial effect Sigma[1:n, 1:n] &lt;- sigma_sq * exp(-distMatrix[1:n, 1:n] / phi) + tau_sq * identityMatrix(d = n) for (site in 1:n) { mean.site[site] &lt;- beta0 + beta1 * easting[site] + beta2 * northing[site] + beta3 * altitude[site] } y[1:n] ~ dmnorm(mean.site[1:n], cov = Sigma[1:n, 1:n]) # Set up the priors for the spatial model sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma^2 tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau_sq &lt;- tau^2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv # prior for the coefficients beta0 ~ dnorm (0, 10) beta1 ~ dnorm (0, 10) beta2 ~ dnorm (0, 10) beta3 ~ dnorm (0, 10) }) # Define the constants, data, parameters and initial values set.seed(1) constants &lt;- list(n = nrow(no2_df)) ex.data &lt;- list(y = no2_df$y, easting = easting_scaled, northing = northing_scaled, altitude = altitude_scaled, distMatrix = obsDist) params &lt;- c( &quot;beta0&quot;, &quot;beta1&quot;,&quot;beta2&quot;,&quot;beta3&quot; ,&quot;phi&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;tau_sq&quot;, &quot;sigma_sq&quot;) inits &lt;- list( sigma = 0.1, phi_inv = 6/max(obsDist), tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out &lt;- nimbleMCMC( code = NO2Code, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time min(coda::effectiveSize(mcmc.out$samples)) ## [1] 686.3596 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out$samples[, c(&quot;beta2&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out$samples[, c(&quot;beta3&quot;)], bty = &quot;n&quot;, main = &quot;beta3&quot;) plot(mcmc.out$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) plot(mcmc.out$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) mcmc.out$summary$all.chains[c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;beta3&quot;,&quot;sigma&quot;, &quot;tau&quot;, &quot;phi&quot;), ] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 0.39364392 0.39277941 0.35719331 -0.3016104 1.1088945 ## beta1 0.02804111 0.02476766 0.14864272 -0.2668795 0.3173510 ## beta2 -0.03237002 -0.02862977 0.23275470 -0.4791750 0.4258925 ## beta3 -0.27094361 -0.27115308 0.02261553 -0.3151008 -0.2270682 ## sigma 1.31724434 1.27765984 0.24708799 0.9322191 1.8713042 ## tau 0.43624410 0.43606721 0.01611994 0.4056233 0.4675887 ## phi 966.33919625 874.35628803 421.02076899 446.1835680 1979.0718225 library(rgdal) grid &lt;- sf::st_read(&quot;data/no2_europe/10km_grid.shp&quot;) covs_grid &lt;- read.csv(&quot;data/no2_europe/centroids_altitude.csv&quot;) covs_grid_utm &lt;- rename(covs_grid, x = lon, y = lat) covs_grid_utm &lt;- mutate(covs_grid_utm, zone2 = (floor((x + 180)/6) %% 60) + 1, keep = &quot;all&quot;) |&gt; group_by(zone2) |&gt; mutate(utm_x = get_utm(x, y, zone2, loc = &quot;x&quot;), utm_y = get_utm(x, y, zone2, loc = &quot;y&quot;))|&gt; as.data.frame() |&gt; dplyr::select(id, utm_x, utm_y, starts_with(&quot;eu_dem_v11&quot;)) |&gt; gather(layer, altitude, -c(id, utm_x, utm_y), na.rm = TRUE) # Scale covariates grid covs_grid_sc &lt;- covs_grid_utm[,c(&quot;utm_x&quot;, &quot;utm_y&quot;, &quot;altitude&quot;)] predCoords &lt;- covs_grid_utm[,c(&quot;utm_x&quot;, &quot;utm_y&quot;)]/1000 covs_grid_sc$utm_x &lt;- ((covs_grid_sc$utm_x/1000) - mean(no2_df$easting))/sd(no2_df$easting) covs_grid_sc$utm_y &lt;- ((covs_grid_sc$utm_y/1000) - mean(no2_df$northing))/sd(no2_df$northing) covs_grid_sc$altitude &lt;- (covs_grid_sc$altitude - mean(no2_df$altitude))/sd(no2_df$altitude) # Extract samples from nimble model tidy_post_samples &lt;- mcmc.out$samples |&gt; tidy_draws() # Extract posterior samples for each of the parameters of interest post_beta0 &lt;- tidy_post_samples$beta0 |&gt; median() post_beta1 &lt;- tidy_post_samples$beta1 |&gt; median() post_beta2 &lt;- tidy_post_samples$beta2 |&gt; median() post_beta3 &lt;- tidy_post_samples$beta3 |&gt; median() post_sigmasq &lt;- tidy_post_samples$sigma_sq |&gt; median() post_phi &lt;- tidy_post_samples$phi |&gt; median() post_tausq &lt;- tidy_post_samples$tau_sq |&gt; median() n0 &lt;- nrow(covs_grid_sc) # L &lt;- length(post_tausq) L &lt;- 1 obsMu &lt;- cbind(1, easting_scaled, northing_scaled, altitude_scaled) %*% t(cbind(post_beta0, post_beta1, post_beta2, post_beta3)) predMu &lt;- cbind(1, as.matrix(covs_grid_sc)) %*% t(cbind(post_beta0, post_beta1, post_beta2, post_beta3)) pred2obsDist &lt;- fields::rdist(predCoords, obsCoords) Rcpp::sourceCpp(&quot;functions/prediction_marginal_gp12.cpp&quot;) args(prediction_marginal_gp12) ## function (y, obsMu, predMu, obsDistMat, pred2ObsDistMat, sigmasq, ## phi, tausq, iterprint) ## NULL system.time( pred_samples &lt;- prediction_marginal_gp12( y = no2_df$y, obsMu = obsMu, predMu = predMu, obsDistMat = obsDist, pred2ObsDistMat = pred2obsDist, sigmasq = post_sigmasq, phi = post_phi, tausq = post_tausq, iterprint = 100 ) ) ## Prediction upto the 0th MCMC sample is completed ## user system elapsed ## 4.36 0.16 29.20 covs_grid_utm &lt;- mutate(covs_grid_utm, pred = pred_samples) cut_grid &lt;- grid[(grid$id %in% covs_grid_utm$id),] cut_grid &lt;- cut_grid[order(cut_grid$id, decreasing = FALSE),] covs_grid_utm &lt;- covs_grid_utm[order(covs_grid_utm$id, decreasing = FALSE),] cut_grid$pred &lt;- covs_grid_utm$pred ggplot() + geom_sf(data = cut_grid, colour = NA, aes(fill = pred)) + ditch_the_axes + scale_fill_viridis_c(direction = -1, name = bquote(&quot;log&quot; ~NO[2]* &quot; concentration&quot;)) Example 10.10 Creating a mesh: black smoke monitoring locations in the UK mesh = inla.mesh.create( locations[, 1:2], extend = list(offset = -0.1), cutoff = 1, # Refined triangulation, minimal angles &gt;=26 degrees , # interior maximal edge lengths 0.08, # exterior maximal edge lengths 0.2: refine = (list( min.angle = 26, max.edge.data = 100, max.edge.extra = 200 )) ) ukmap &lt;- readShapeLines(&quot;uk_BNG.shp&quot;) plot(mesh , col = &quot;gray&quot;, main = &quot;&quot;) lines(ukmap) points(locations , col = &quot;red&quot;, pch = 20, bg = &quot;red&quot;) Example 10.12 Plotting directional variograms for temperatures in California library(geoR) library(sp) ### First we load the data CAmetadata&lt;- read.table(&quot;data/metadataCA.txt&quot;,header=TRUE) CATemp&lt;-read.csv(&quot;data/MaxCaliforniaTemp.csv&quot;,header=TRUE) CATemp_20120401&lt;-CATemp[CATemp$X==20120401, -c(1, 14)] CATemp_20120401&lt;-t(CATemp_20120401) ### Change names of lat, long names(CAmetadata)[names(CAmetadata)==&quot;Long&quot;]&lt;-&quot;x&quot; names(CAmetadata)[names(CAmetadata)==&quot;Lat&quot;]&lt;-&quot;y&quot; ### Augment data file with coordinates CATemp_20120401 &lt;- cbind(CAmetadata, CATemp_20120401/10) names(CATemp_20120401)[names(CATemp_20120401) == &quot;92&quot;] &lt;- &quot;Temp&quot; CATempOri.geo &lt;- as.geodata(CATemp_20120401, coords.col = 3:4, data.col = 7) ### Compute empirical variograms for the residual process with geoR. ### The power variogram seemed best ### although this points to nonstationarity in the process CA.vario4 &lt;- variog4(CATempOri.geo, uvec = seq(0, 8, l = 8)) ## variog: computing variogram for direction = 0 degrees (0 radians) ## tolerance angle = 22.5 degrees (0.393 radians) ## variog: computing variogram for direction = 45 degrees (0.785 radians) ## tolerance angle = 22.5 degrees (0.393 radians) ## variog: computing variogram for direction = 90 degrees (1.571 radians) ## tolerance angle = 22.5 degrees (0.393 radians) ## variog: computing variogram for direction = 135 degrees (2.356 radians) ## tolerance angle = 22.5 degrees (0.393 radians) ## variog: computing omnidirectional variogram plot(CA.vario4) Example 10.14: Spatial modeling of malaria in Gambia Note: There might be a warning when loading the \\(\\texttt{raster}\\) package, if necessary, uninstall and re-install the package. For this example we are using the same \\(\\texttt{gambia}\\) data set from the \\(\\texttt{geoR}\\) package but an \\(\\texttt{id_area}\\) column was added using QGIS to differentiate the different areas as in the original paper. Therefore, we need to load the gambia_area.csv file. library(dplyr) # to manipulate the data library(geoR) # to get the dataset library(leaflet) # to plot the map library(nimble) # for modeling library(sf) # manipulate spatial data library(sp) # for manipulating spatial data library(stringr) # to analyze posterior library(viridis) # for a more cheerful color palette # Note, we are using the same Gambia dataset as in the geoR package but an id_area # attribute has been added using QGIS to a gambia &lt;- read.csv(&quot;data/malaria_gambia/gambia_area.csv&quot;) # gambia dataset from geoR package Since the data is given at the individual level, we want to aggregate the malaria tests by village. If we explore the data frame we see that there are 2035 individuals at 65 villages. head(gambia) ## x y pos age netuse treated green phc id_area ## 1 349631.3 1458055 1 1783 0 0 40.85 1 1 ## 2 349631.3 1458055 0 404 1 0 40.85 1 1 ## 3 349631.3 1458055 0 452 1 0 40.85 1 1 ## 4 349631.3 1458055 1 566 1 0 40.85 1 1 ## 5 349631.3 1458055 0 598 1 0 40.85 1 1 ## 6 349631.3 1458055 1 590 1 0 40.85 1 1 dim(gambia) ## [1] 2035 9 dim(unique(gambia[, c(&quot;x&quot;, &quot;y&quot;)])) ## [1] 65 2 We create a new data frame aggregated by village containing the coordinates, the number of malaria tests, and the prevalence. malaria_village &lt;- group_by(gambia, x, y) |&gt; summarize(total = n(), positive = sum(pos), prev = positive / total) |&gt; as.data.frame() ## `summarise()` has grouped output by &#39;x&#39;. You can override using the `.groups` ## argument. head(malaria_village) ## x y total positive prev ## 1 349631.3 1458055 33 17 0.5151515 ## 2 358543.1 1460112 63 19 0.3015873 ## 3 360308.1 1460026 17 7 0.4117647 ## 4 363795.7 1496919 24 8 0.3333333 ## 5 366400.5 1460248 26 10 0.3846154 ## 6 366687.5 1463002 18 7 0.3888889 # create a new variable in &quot;sp&quot; format and define coordinates malaria_utm &lt;- malaria_village coordinates(malaria_utm) &lt;- ~ x + y proj4string(malaria_utm) &lt;- CRS(&quot;+proj=utm +zone=28&quot;) # convert to long lat malaria_geo &lt;- spTransform(malaria_utm, CRS(&quot;+proj=longlat +datum=WGS84&quot;)) # add long lat coordinates to malaria dataframe malaria_village[, c(&quot;long&quot;, &quot;lat&quot;)] &lt;- coordinates(malaria_geo) # create map with location dots marked on it in mypalette &lt;- colorFactor(palette=&quot;inferno&quot;, domain=malaria_village$prev) leaflet( data = malaria_village) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ long, ~ lat, fillOpacity = 0.8, color = ~ mypalette(prev), radius = 4, opacity = 5, stroke = FALSE) Nimble Example10_14_Nimble &lt;- nimbleCode({ # Define priors sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma ^ 2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv Sigma[1:N, 1:N] &lt;- sigma_sq*(1 + (sqrt(3)*obs_dist_mat[1:N, 1:N])/phi) * exp(-sqrt(3)*obs_dist_mat[1:N, 1:N] / phi) for(i in 1:N){ mean_S[i] &lt;- b0 } S[1:N] ~ dmnorm(mean_S[1:N], cov = Sigma[1:N,1:N]) for (j in 1:n) { # by child logit(p[j]) &lt;- inprod(b[1:k], X[j,1:k]) + S[index_village[j]] y[j] ~ dbern(p[j]) } for (l in 1:k){ b[l] ~ dnorm(0, sd = 5) } b0 ~ dnorm(0, sd = 5) }) # distance specification coords_sf &lt;- sf::st_as_sf(malaria_village[,c(&quot;long&quot;,&quot;lat&quot;)], coords = c(&quot;long&quot;,&quot;lat&quot;)) |&gt; sf::st_set_crs(4326) obs_dist_mat &lt;- sf::st_distance(coords_sf) obs_dist_mat &lt;- units::set_units(obs_dist_mat, km) obs_dist_mat &lt;- units::set_units(obs_dist_mat, NULL) # define indicator variables for each village gambia_df &lt;- mutate( gambia, id_child = 1:nrow(gambia), # add an id for each child value = 1, # this will be needed later for the villages id_village = as.numeric(interaction( # add an id for the villages x, y, drop = TRUE, lex.order = TRUE )) ) |&gt; tidyr::spread(id_area, value, fill = 0) |&gt; rename(area1 = &#39;1&#39;, area2 = &#39;2&#39;, area3 = &#39;3&#39;, area4 = &#39;4&#39;, area5 = &#39;5&#39;) ## Model specification # Variables matrix X &lt;- data.frame(age = scale(gambia_df[,&quot;age&quot;], center = TRUE, scale = FALSE), netuse = gambia_df[,&quot;netuse&quot;], treated = gambia_df[,&quot;treated&quot;], green = scale(gambia_df[,&quot;green&quot;], center = TRUE, scale = FALSE), phc = gambia_df[,&quot;phc&quot;], area2 = gambia_df[,&quot;area2&quot;], area3 = gambia_df[,&quot;area3&quot;], area4 = gambia_df[,&quot;area4&quot;], area5 = gambia_df[,&quot;area5&quot;] ) index_village &lt;- gambia_df[,&quot;id_village&quot;] n &lt;- nrow(X) # child number N &lt;- nrow(malaria_village) # number of villages zeroes &lt;- rep(0, N) # auxiliary vector of zeroes for model ones &lt;- rep(1, N) const_list &lt;- list(n = n, # number of childs N = N, # number of villages zeroes = zeroes, # vector of zeroes prior_max_dist = max(obs_dist_mat)/6, # max dist for phi prior k = ncol(X),# number of predictors index_village = index_village, Id10 = 10*diag(N)) dat_list &lt;- list(y = gambia_df$pos, # malaria positive test obs_dist_mat = obs_dist_mat, # distance matrix in km X = X # predictors matrix ) init_list &lt;- list(sigma = 0.5, p = rep(expit(rnorm(1, 0, 1)), n), phi_inv = 6/max(obs_dist_mat), b = rep(0, ncol(X)), b0 = rnorm(1, 0, 1), S = rnorm(N, 0, 1)) #init_list &lt;- list(p = runif(n, 0, 1), b = rnorm(ncol(X), 0, 1), b0 = rnorm(1, 0, 1)) Rmodel &lt;- nimbleModel( Example10_14_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c( &quot;b0&quot;, &quot;b&quot;, &quot;p&quot;, &quot;S&quot;, &quot;sigma&quot;, &quot;phi&quot;)) # conf$removeSamplers(c(&#39;S&#39;)) # conf$addSampler(target = c(&#39;S&#39;), type = &#39;AF_slice&#39;) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = TRUE ) niters &lt;- 80000 nburnins &lt;- 0.5 * niters nchains &lt;- 2 nthins &lt;- 14 post_samples &lt;- runMCMC( Cmcmc, niter = niters, nburnin = nburnins, thin = nthins, nchains = nchains, samplesAsCodaMCMC = TRUE, summary = TRUE ) plot(post_samples$samples[, c(&quot;b0&quot;)], bty = &quot;n&quot;, main = &quot;b0&quot;) plot(post_samples$samples[, c(&quot;b[1]&quot;)], bty = &quot;n&quot;, main = &quot;b[1]&quot;) plot(post_samples$samples[, c(&quot;b[2]&quot;)], bty = &quot;n&quot;, main = &quot;b[2]&quot;) plot(post_samples$samples[, c(&quot;b[3]&quot;)], bty = &quot;n&quot;, main = &quot;b[3]&quot;) plot(post_samples$samples[, c(&quot;b[4]&quot;)], bty = &quot;n&quot;, main = &quot;b[4]&quot;) plot(post_samples$samples[, c(&quot;b[5]&quot;)], bty = &quot;n&quot;, main = &quot;b[5]&quot;) plot(post_samples$samples[, c(&quot;b[6]&quot;)], bty = &quot;n&quot;, main = &quot;b[6]&quot;) plot(post_samples$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(post_samples$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) plot(post_samples$samples[, c(&quot;S[1]&quot;)], bty = &quot;n&quot;, main = &quot;S[1]&quot;) plot(post_samples$samples[, c(&quot;S[24]&quot;)], bty = &quot;n&quot;, main = &quot;S[24]&quot;) plot(post_samples$samples[, c(&quot;S[54]&quot;)], bty = &quot;n&quot;, main = &quot;S[54]&quot;) plot(post_samples$samples[, c(&quot;p[1]&quot;)], bty = &quot;n&quot;, main = &quot;p[1]&quot;) plot(post_samples$samples[, c(&quot;p[1805]&quot;)], bty = &quot;n&quot;, main = &quot;p[1805]&quot;) # Get minimum effective size (ESS) and which variable has the min ESS min(coda::effectiveSize(post_samples$samples)) ## [1] 121.2351 mcmc_variable_names &lt;- colnames(post_samples$samples$chain1) mcmc_variable_names[which(coda::effectiveSize(post_samples$samples) == min(coda::effectiveSize(post_samples$samples)))] ## [1] &quot;b[7]&quot; # Extract samples variables &lt;- c(&quot;b0&quot;, &quot;b[1]&quot;, &quot;b[2]&quot;, &quot;b[3]&quot;, &quot;b[4]&quot;, &quot;b[5]&quot;, &quot;b[6]&quot;, &quot;b[7]&quot;, &quot;b[8]&quot;, &quot;b[9]&quot; ,&quot;sigma&quot;, &quot;phi&quot;) summary_nimble &lt;- post_samples$summary$all.chains summary_nimble[variables,] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## b0 0.3315201138 0.3392450269 0.2505843962 -0.1729623331 0.8081442858 ## b[1] 0.0006860043 0.0006835488 0.0001234595 0.0004431429 0.0009273262 ## b[2] -0.3896279125 -0.3888672550 0.1560999545 -0.7048920797 -0.0819548994 ## b[3] -0.3757735359 -0.3769547462 0.2116118487 -0.7882866024 0.0385847122 ## b[4] 0.0442854311 0.0440496333 0.0178257390 0.0087224925 0.0792046195 ## b[5] -0.2100219775 -0.2056276065 0.2483450907 -0.7052031402 0.2462629975 ## b[6] -0.5000700991 -0.4943813970 0.3424221174 -1.2308098244 0.1291118929 ## b[7] -1.3233823781 -1.3263073092 0.2858536435 -1.8703937606 -0.7468568056 ## b[8] -0.7908411531 -0.7987714491 0.4042487749 -1.5835167007 0.0305095920 ## b[9] 0.0352274728 0.0345586394 0.4167655576 -0.7596146680 0.8762673597 ## sigma 0.7364758452 0.7317618930 0.1031994687 0.5487828612 0.9489075739 ## phi 1.2621830848 1.1370656814 0.5930172725 0.5070952884 2.7332364751 # Plot posterior summary for the spatial random effect by village post_summary &lt;- post_samples$summary$all.chains post_sum_S &lt;- as.data.frame(post_summary) |&gt; tibble::rownames_to_column() |&gt; filter(str_detect(rowname, &quot;S&quot;)) |&gt; dplyr::select(rowname, `95%CI_low`, Mean, `95%CI_upp`) |&gt; mutate(village = gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname)) post_sum_S$village &lt;- factor(post_sum_S$village , levels = 1:65) ggplot(data = post_sum_S, aes(x = village)) + geom_pointrange(aes(ymin = `95%CI_low`, ymax = `95%CI_upp`, y = Mean)) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + scale_x_discrete( breaks = post_sum_S$village[seq(1, length(post_sum_S$village), by = 5)]) + theme_classic() + ylab(&quot;&quot;) + xlab(&quot;village&quot;) + ggtitle(&quot;Posterior summary spatial random effect by village&quot;) Stan data { int&lt;lower=1&gt; n; // number of children int&lt;lower=1&gt; k; // number of covariates int N; // total number of villages int y[n]; // tests matrix[N,N] dist_matrix; // distance matrix matrix[n, k] X; // matrix with covariates int index_village[n]; } parameters { real&lt;lower=0&gt; phi_inv; real&lt;lower=0&gt; sigma; vector[N] S; // spatial random effect vector[k] betas; real beta0; } transformed parameters { // vector[n] p = inv_logit(logit_p); real sigma_sq = square(sigma); real&lt;lower=0&gt; phi = 1/phi_inv; } model { matrix[N, N] L; matrix[N, N] Sigma; vector[N] zeroes; // vector of zeroes vector[n] logit_p; for(i in 1:(N-1)){ for(j in (i+1):N){ //Sigma[i,j] = sigma_sq*exp(-dist_matrix[i,j]/phi); Sigma[i,j] = sigma_sq*(1 + (sqrt(3)*dist_matrix[i,j])/phi) * exp(-sqrt(3)*dist_matrix[i,j] / phi); Sigma[j,i] = Sigma[i,j]; } } // diagonal elements covariances for(i in 1:N){ Sigma[i,i] = sigma_sq; } // sample spatial random effect L = cholesky_decompose(Sigma); zeroes = rep_vector(0, N); S ~ multi_normal_cholesky(zeroes, L); for(i in 1:n) { logit_p[i] = beta0 + X[i,]*betas + S[index_village[i]]; y[i] ~ binomial_logit(1, logit_p[i]); } beta0 ~ normal(0,10); betas ~ normal(0,10); phi_inv ~ gamma(5, 5); sigma ~ cauchy(0,1); } ex.data &lt;- list( n = nrow(gambia), # number of children k = ncol(X), # number of covariates N = N, # number of villages y = gambia$pos, # positive tests dist_matrix = obs_dist_mat, # distance matrix in km X = X, # altitude per village index_village = index_village ) Example10_14Stan &lt;- stan( file = &quot;functions/Example10_14.stan&quot;, data = ex.data, warmup = 15000, iter = 30000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;betas&quot;,&quot;sigma&quot;, &quot;phi&quot;, &quot;S&quot;), include = TRUE ) #computing WAIC using the package loo rstan::traceplot(Example10_14Stan, pars = c(&quot;beta0&quot;,&quot;betas&quot;,&quot;sigma&quot;, &quot;phi&quot;)) # Extract samples summary_stan &lt;- summary( Example10_14Stan, pars = c(&quot;beta0&quot;,&quot;betas&quot;, &quot;sigma&quot;, &quot;phi&quot;), probs = c(0.025, 0.975) ) summary_stan$summary ## mean se_mean sd 2.5% 97.5% ## beta0 0.3458755538 4.778216e-03 0.2510986221 -0.1362639241 0.8442035273 ## betas[1] 0.0006871207 2.273556e-06 0.0001224843 0.0004482812 0.0009307211 ## betas[2] -0.3937790344 2.994197e-03 0.1630352170 -0.7217402843 -0.0793440251 ## betas[3] -0.3690088984 3.969896e-03 0.2141993706 -0.7771861338 0.0585086866 ## betas[4] 0.0441686717 3.655065e-04 0.0204927583 0.0038759447 0.0849912336 ## betas[5] -0.2188864063 4.524898e-03 0.2445430055 -0.6915407489 0.2553282584 ## betas[6] -0.5146223013 7.075673e-03 0.3790193760 -1.2623443596 0.2154029130 ## betas[7] -1.3569507901 5.737837e-03 0.3113742252 -1.9752438918 -0.7549112373 ## betas[8] -0.8086446199 7.945203e-03 0.4441463825 -1.6942523608 0.0389549590 ## betas[9] 0.0102733351 8.134941e-03 0.4361173159 -0.8699648369 0.8558910589 ## sigma 0.7474663299 2.086654e-03 0.1094484302 0.5576156752 0.9798656966 ## phi 1.2278687030 1.059571e-02 0.5680543393 0.5086054757 2.6412794837 ## n_eff Rhat ## beta0 2761.576 0.9994923 ## betas[1] 2902.347 0.9995322 ## betas[2] 2964.847 0.9994596 ## betas[3] 2911.241 0.9993715 ## betas[4] 3143.479 1.0007034 ## betas[5] 2920.740 0.9994372 ## betas[6] 2869.375 0.9999332 ## betas[7] 2944.889 0.9995635 ## betas[8] 3124.944 0.9998981 ## betas[9] 2874.074 0.9998750 ## sigma 2751.176 0.9995793 ## phi 2874.216 0.9995888 S_summary &lt;- summary(Example10_14Stan, pars = c(&quot;S&quot;))$summary S_summary_df &lt;- data.frame(S_summary) |&gt; tibble::rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;S[&quot;, 1:65, &quot;]&quot;)) |&gt; mutate(village = 1:65) |&gt; dplyr::select(mean, X2.5., X97.5., village) S_summary_df$village &lt;- factor(S_summary_df$village , levels = 1:65) ggplot(S_summary_df, aes(x = village, group = 1)) + geom_pointrange(aes(ymin = X2.5., ymax = X97.5., y = mean)) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + scale_x_discrete( breaks = S_summary_df$village[seq(1, length(S_summary_df$village), by = 5)]) + theme_classic() + ylab(&quot;&quot;) + xlab(&quot;village&quot;) + ggtitle(&quot;Posterior summary spatial random effect by village&quot;) Supplementary Material Example: Analysis of benzene concentrations across Montreal, QC, Canada Map the locations of the monitoring stations in Montreal. This example uses the montreal_benzene_apr.csv library(cowplot) library(geoR) library(leaflet) library(spdep) # Load data on benzene concentration in Montreal benzene &lt;- read.csv(&quot;data/voc/montreal_benzene_apr.csv&quot;) # TODO: Add description of the data, this is the April campaign # create a new variable in &quot;sp&quot; format and define coordinates benzene_geo &lt;- benzene coordinates(benzene_geo) &lt;- ~ lon + lat proj4string(benzene_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) # create map with location dots marked on it in range_values &lt;- range(benzene$Benzene) mybins &lt;- seq(range_values[1], range_values[2], length.out = 5) mypalette &lt;- colorBin(palette=&quot;inferno&quot;, domain=benzene$Benzene, na.color=&quot;transparent&quot;, bins=mybins) leaflet( data = benzene) |&gt; addTiles() |&gt; addProviderTiles(&quot;Esri.WorldImagery&quot;) |&gt; addCircleMarkers( ~ lon, ~ lat, fillOpacity = 0.8, color=&quot;orange&quot;, fillColor = ~ mypalette(Benzene), opacity = 5, radius = ~Benzene*4, stroke = FALSE) |&gt; addLegend( pal=mypalette, values=~Benzene, opacity=0.9, title = &quot;Benzene concentration&quot;, position = &quot;bottomright&quot; ) Using the geoR package we can also plot the following: the locations of the sampling sites the concentrations of ozone in relation to the x and y coordinates and a histogram of the concentrations indicating the distribution of concentrations together with an estimate of the density. # convert data to utm coordinates benzene_utm &lt;- spTransform(benzene_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_utm) colnames(benzene_utm_df) &lt;- c(&quot;Benzene&quot;, &quot;X&quot;, &quot;Y&quot;) # Save as geodata to generate the geodata plot benzene_geodata &lt;- as.geodata(benzene_utm_df, coords.col = 2:3, data.col = 1) plot(benzene_geodata) Extra 10.2: Examining the log concentrations of benzene in Montreal # Histogram for the log of the benzene concentration par(mfrow = c(1, 2)) log_histogram &lt;- hist(log(benzene$Benzene), main = &quot;&quot;, xlab = &quot;log(Benzene)&quot;) qqnorm(log(benzene$Benzene), bty = &quot;n&quot;) qqline(log(benzene$Benzene)) Extra 10.5: Variogram library(gstat) benzene_utm_geo &lt;- benzene_utm_df # get the coordinates in kms and turn into a Spatial object benzene_utm_geo[,c(&quot;X&quot;, &quot;Y&quot;)] &lt;- benzene_utm_geo[,c(&quot;X&quot;, &quot;Y&quot;)]/1000 coordinates(benzene_utm_geo) &lt;- ~ X + Y # Estimate variogram intercept only benzene_inter_vgm &lt;- variogram(log(Benzene) ~ 1, data = benzene_utm_geo, cutoff = 20,# cutoff distance width = 20 / 10) # bins width benzene_inter_vgm_fit &lt;- fit.variogram(benzene_inter_vgm, model = vgm(0.1, &quot;Exp&quot;, 15, 0.02)) benzene_inter_vgm_fit ## model psill range ## 1 Nug 0.01616521 0.00000 ## 2 Exp 0.17301314 23.97153 # Estimate variogram using coordinates benzene_vgm &lt;- variogram(log(Benzene) ~ X + Y, data = benzene_utm_geo, cutoff = 20, # cutoff distance width = 20 / 10) # bins width benzene_vgm_fit &lt;- fit.variogram(benzene_vgm, model = vgm(0.1, &quot;Exp&quot;, 3, 0.02)) benzene_vgm_fit ## model psill range ## 1 Nug 0.01638881 0.000000 ## 2 Exp 0.05598753 7.365469 plot_inter_variog &lt;- plot(benzene_inter_vgm, benzene_inter_vgm_fit, bty = &quot;n&quot;) plot_coord_variog &lt;- plot(benzene_vgm, benzene_vgm_fit, bty = &quot;n&quot;) plot_grid(plot_inter_variog, plot_coord_variog, labels = &quot;auto&quot;) Extra 10.6 Basic spatial modelling and prediction of benzene in Montreal # Generate grid MtlPred &lt;- expand.grid(seq (580, 615 , 0.5), seq (5020, 5060 , 0.5) ) # change names grid names(MtlPred)[ names(MtlPred)==&quot;Var1&quot;] &lt;- &quot;X&quot; names(MtlPred)[ names(MtlPred)==&quot;Var2&quot;] &lt;- &quot;Y&quot; # make the grid a Spatial object coordinates (MtlPred) = ~ X + Y gridded(MtlPred) = TRUE # define the model based on the variogram fit from the previous example mod &lt;- vgm (0.053 , &quot;Exp&quot;, 5.54, 0.0122) # use ordinary kriging to predict values in the grid x &lt;- krige(log(Benzene) ~ X + Y, benzene_utm_geo , MtlPred , model = mod ) ## [using universal kriging] # Plot the ordinary kriging predictions and their variance monitor_loc &lt;- list(&#39;sp.points&#39;, benzene_utm_geo, pch=19, cex=.8, col=&#39;cornsilk4&#39;) krig_pred &lt;- spplot ( x[&quot;var1.pred&quot;], main = &quot;Ordinary kriging predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) krig_var &lt;- spplot ( x[&quot;var1.var&quot;], main = &quot;Ordinary kriging variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(krig_pred, krig_var, labels = &quot;auto&quot;) Extra 10.8 Spatial modelling of Benzene in Montreal This example uses the montreal_benzene_apr.csv Nimble library(coda) library(geoR) library(magrittr) library(nimble) library(spdep) library(tidyverse) library(tidybayes) # Load data on benzene concentration in Montreal benzene &lt;- read.csv(&quot;data/voc/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_geo &lt;- benzene coordinates(benzene_geo) &lt;- ~ lon + lat proj4string(benzene_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_utm &lt;- spTransform(benzene_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_utm) # Change coordinates to kilometers and observations to the log scale Mtl_benzene_sample &lt;- data.frame( y = log(benzene_utm_df$Benzene), easting = benzene_utm_df$lon / 1000, northing = benzene_utm_df$lat / 1000 ) # Compute the distance matrix obsCoords &lt;- unname(as.matrix(Mtl_benzene_sample[,c(&quot;easting&quot;, &quot;northing&quot;)])) obsDist &lt;- fields::rdist(obsCoords) Extra10_9Code &lt;- nimbleCode ({ # Covariance matrix spatial effect Sigma[1:n, 1:n] &lt;- sigma_sq * exp(-distMatrix[1:n, 1:n] / phi) + tau_sq * identityMatrix(d = n) for (site in 1:n) { mean.site[site] &lt;- beta0 + beta1 * easting[site] + beta2 * northing[site] } y[1:n] ~ dmnorm(mean.site[1:n], cov = Sigma[1:n, 1:n]) # Set up the priors for the spatial model sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma^2 tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau_sq &lt;- tau^2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv # prior for the coefficients beta0 ~ dnorm (0, 10) beta1 ~ dnorm (0, 10) beta2 ~ dnorm (0, 10) }) # Define the constants, data, parameters and initial values set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) constants &lt;- list(n = nrow(Mtl_benzene_sample)) ex.data &lt;- list(y = Mtl_benzene_sample$y, easting = easting_scaled, northing = northing_scaled, distMatrix = obsDist) params &lt;- c( &quot;beta0&quot;, &quot;beta1&quot;,&quot;beta2&quot;, &quot;phi&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;tau_sq&quot;, &quot;sigma_sq&quot;) inits &lt;- list( sigma = 0.1, phi_inv = 6/max(obsDist), tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out &lt;- nimbleMCMC( code = Extra10_9Code, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] -32.37684 ## Field &quot;lppd&quot;: ## [1] 19.03084 ## Field &quot;pWAIC&quot;: ## [1] 2.842421 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 985.2176 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out$samples[, c(&quot;beta2&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) plot(mcmc.out$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) mcmc.out$summary$all.chains[c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), ] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 0.143589662 0.15187505 0.081381727 -4.284934e-02 0.27866526 ## beta1 0.051028164 0.04806334 0.060619017 -6.338295e-02 0.17328888 ## beta2 0.077775628 0.08250220 0.068519017 -7.013084e-02 0.20323633 ## sigma_sq 0.056087887 0.05202343 0.022624353 2.770244e-02 0.11140808 ## tau_sq 0.009128758 0.00841011 0.006709036 5.744089e-05 0.02380432 ## phi 3.697646345 3.09115863 2.228726265 1.463844e+00 9.40053808 # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) Following the posterior predictive distribution we have to define a model for the predictions. # Extract samples from nimble model tidy_post_samples &lt;- mcmc.out$samples |&gt; tidy_draws() # Extract posterior samples for each of the parameters of interest post_beta0 &lt;- tidy_post_samples$beta0 post_beta1 &lt;- tidy_post_samples$beta1 post_beta2 &lt;- tidy_post_samples$beta2 post_sigmasq &lt;- tidy_post_samples$sigma_sq post_phi &lt;- tidy_post_samples$phi post_tausq &lt;- tidy_post_samples$tau_sq # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) n0 &lt;- nrow(predCoords_sc) L &lt;- length(post_tausq) obsMu &lt;- cbind(1, easting_scaled, northing_scaled) %*% t(cbind(post_beta0, post_beta1, post_beta2)) predMu &lt;- cbind(1, as.matrix(predCoords_sc)) %*% t(cbind(post_beta0, post_beta1, post_beta2)) pred2obsDist &lt;- fields::rdist(predCoords, obsCoords) Rcpp::sourceCpp(&quot;functions/prediction_marginal_gp12.cpp&quot;) args(prediction_marginal_gp12) ## function (y, obsMu, predMu, obsDistMat, pred2ObsDistMat, sigmasq, ## phi, tausq, iterprint) ## NULL system.time( pred_samples &lt;- prediction_marginal_gp12( y = Mtl_benzene_sample$y, obsMu = obsMu, predMu = predMu, obsDistMat = obsDist, pred2ObsDistMat = pred2obsDist, sigmasq = post_sigmasq, phi = post_phi, tausq = post_tausq, iterprint = 1000 ) ) ## Prediction upto the 0th MCMC sample is completed ## Prediction upto the 1000th MCMC sample is completed ## Prediction upto the 2000th MCMC sample is completed ## user system elapsed ## 149.52 0.09 178.07 predict_res_dt &lt;- data.frame( xcoord = predCoords[, 1], ycoord = predCoords[, 2], post.mean = apply(pred_samples, 1, mean), post.var = apply(pred_samples, 1, var), q2.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.025)), q50 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.5)), q97.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.975)) ) x$nimble.pred &lt;- predict_res_dt$post.mean x$nimble.var &lt;- predict_res_dt$post.var nimble_pred &lt;- spplot ( x[&quot;nimble.pred&quot;], main = &quot;Nimble spatial predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) nimble_var &lt;- spplot ( x[&quot;nimble.var&quot;], main = &quot;Nimble predictions variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(nimble_pred, nimble_var, labels = &quot;auto&quot;) predict_res_dt$post.mean[1:5] ## [1] -0.2748897 -0.2610339 -0.2629892 -0.2617884 -0.2446157 predict_res_dt$post.var[1:5] ## [1] 0.08898525 0.09035693 0.08581058 0.08874131 0.08585555 Stan benzene_data &lt;- read.csv(&quot;data/voc/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_data_geo &lt;- benzene_data coordinates(benzene_data_geo) &lt;- ~ lon + lat proj4string(benzene_data_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_data_utm &lt;- spTransform(benzene_data_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_data_utm) # change the observed values to the log scale and the coordinates to km&#39;s Mtl_benzene_sample &lt;- data.frame( y = log(benzene_utm_df$Benzene), easting = benzene_utm_df$lon / 1000, northing = benzene_utm_df$lat / 1000 ) # Compute the distance matrix obsCoords &lt;- unname(as.matrix(Mtl_benzene_sample[,c(&quot;easting&quot;, &quot;northing&quot;)])) obsDist &lt;- fields::rdist(obsCoords) data { int&lt;lower=1&gt; N; int&lt;lower=0&gt; p; vector[N] y; matrix[N,N] dist_matrix; matrix[N,p] X; } parameters { real&lt;lower=0&gt; phi; real&lt;lower=0&gt; tau; real&lt;lower=0&gt; sigma; vector[p] beta; real beta0; } transformed parameters{ real&lt;lower=0&gt; sigma_sq = square(sigma); real&lt;lower=0&gt; tau_sq = square(tau); } model { vector[N] mu; matrix[N, N] L; matrix[N, N] Sigma; Sigma = sigma_sq * exp(-dist_matrix/ phi) + tau_sq *diag_matrix(rep_vector(1, N)); for(i in 1:N) { mu[i] = beta0 + X[i,]*beta; } L = cholesky_decompose(Sigma); beta0 ~ normal(0,10); beta ~ normal(0,10); phi ~ inv_gamma(5, 5); tau ~ cauchy(0,1); sigma ~ cauchy(0,1); y ~ multi_normal_cholesky(mu, L); } set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) N &lt;- nrow(benzene_utm_df) # Change coordinates to kilometers X &lt;- data.frame(easting = easting_scaled, northing = northing_scaled) ex.data &lt;- list( N = N, p = 2, y = log(benzene_utm_df$Benzene), dist_matrix = obsDist, X = as.matrix(X) ) Example10_9Stan &lt;- stan( file = &quot;functions/Example10_9.stan&quot;, data = ex.data, warmup = 10000, iter = 20000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), include = TRUE ) rstan::traceplot(Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;)) summary_exp_stan &lt;- summary( Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), probs = c(0.025, 0.975) ) summary_exp_stan$summary ## mean se_mean sd 2.5% 97.5% n_eff ## beta0 0.146457291 0.0018562983 0.07997902 -3.023587e-02 0.28132110 1856.336 ## beta[1] 0.047496231 0.0013261678 0.05957233 -6.496270e-02 0.17626742 2017.866 ## beta[2] 0.080290249 0.0014607308 0.06744570 -5.992676e-02 0.19665262 2131.906 ## sigma_sq 0.055772229 0.0004686651 0.02109726 2.831741e-02 0.10389363 2026.408 ## tau_sq 0.009142795 0.0001523047 0.00674511 6.533636e-05 0.02383244 1961.335 ## phi 3.666760755 0.0513903964 2.24757184 1.501948e+00 9.25712051 1912.772 ## Rhat ## beta0 0.9994055 ## beta[1] 0.9991897 ## beta[2] 0.9993034 ## sigma_sq 1.0000657 ## tau_sq 1.0016241 ## phi 0.9994019 # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) # Extract samples from nimble model stan_post_samples &lt;- rstan::extract(Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;, &quot;phi&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;)) # Extract posterior samples for each of the parameters of interest post_beta0 &lt;- stan_post_samples$beta0 post_beta &lt;- stan_post_samples$beta # post_beta2 &lt;- stan_post_samples$beta2 post_sigmasq &lt;- stan_post_samples$sigma_sq post_phi &lt;- stan_post_samples$phi post_tausq &lt;- stan_post_samples$tau_sq # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) n0 &lt;- nrow(predCoords_sc) L &lt;- length(post_tausq) obsMu &lt;- cbind(1, easting_scaled, northing_scaled) %*% t(cbind(post_beta0, post_beta)) predMu &lt;- cbind(1, as.matrix(predCoords_sc)) %*% t(cbind(post_beta0, post_beta)) pred2obsDist &lt;- fields::rdist(predCoords, obsCoords) Rcpp::sourceCpp(&quot;functions/prediction_marginal_gp12.cpp&quot;) args(prediction_marginal_gp12) ## function (y, obsMu, predMu, obsDistMat, pred2ObsDistMat, sigmasq, ## phi, tausq, iterprint) ## NULL system.time( pred_samples &lt;- prediction_marginal_gp12( y = Mtl_benzene_sample$y, obsMu = obsMu, predMu = predMu, obsDistMat = obsDist, pred2ObsDistMat = pred2obsDist, sigmasq = post_sigmasq, phi = post_phi, tausq = post_tausq, iterprint = 1000 ) ) ## Prediction upto the 0th MCMC sample is completed ## Prediction upto the 1000th MCMC sample is completed ## user system elapsed ## 107.44 0.09 126.70 predict_res_dt &lt;- data.frame( xcoord = predCoords[, 1], ycoord = predCoords[, 2], post.mean = apply(pred_samples, 1, mean), post.var = apply(pred_samples, 1, var), q2.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.025)), q50 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.5)), q97.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.975)) ) x$stan.pred &lt;- predict_res_dt$post.mean x$stan.var &lt;- predict_res_dt$post.var stan_pred &lt;- spplot ( x[&quot;stan.pred&quot;], main = &quot;Stan spatial predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) stan_var &lt;- spplot ( x[&quot;stan.var&quot;], main = &quot;Stan predictions variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(stan_pred, stan_var, labels = &quot;auto&quot;) Extra 10.9: Spatial predictions Nimble # NOTE: I am just doing for 5 locations (please double-check my equations) and it instead of running in 2 minutes it runs in 6 (in case you want to highlight that). But we should discuss the order of this cause I am doing the predictions with Paritosh&#39;s code in the previous section. # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) # As an example we are only going to predict 5 locations nu &lt;- 5 predCoords_five &lt;- predCoords_sc[1:nu,] obs2predDist &lt;- fields::rdist(obsCoords, predCoords[1:nu,]) pred2predDist &lt;- fields::rdist(predCoords[1:nu,]) Example10_10Code &lt;- nimbleCode ({ # Covariance matrix spatial effect Sigma_obs[1:n, 1:n] &lt;- sigma_sq * exp(-distMatrix[1:n, 1:n] / phi) + tau_sq * identityMatrix(d = n) Sigma_pred[1:nu, 1:nu] &lt;- sigma_sq * exp(-distMatrixUnobs[1:nu, 1:nu] / phi) + tau_sq * identityMatrix(d = nu) Sigma_obs_pred[1:n, 1:nu] &lt;- sigma_sq * exp(-distMatrixObsUnobs[1:n, 1:nu] / phi) for (site in 1:n) { mean.site[site] &lt;- beta0 + beta1 * easting[site] + beta2 * northing[site] } y[1:n] ~ dmnorm(mean.site[1:n], cov = Sigma_obs[1:n, 1:n]) # Spatial predictions for (usite in 1:nu) { mean.pred.site[usite] &lt;- beta0 + beta1 * easting_pred[usite] + beta2 * northing_pred[usite] } mean_sigma[1:nu] &lt;- t(Sigma_obs_pred[1:n, 1:nu])%*% inverse(Sigma_obs[1:n, 1:n])%*%(y[1:n] - mean.site[1:n]) mu_pred[1:nu] &lt;- mean.pred.site[1:nu] + mean_sigma[1:nu] cov_pred[1:nu, 1:nu] &lt;- Sigma_pred[1:nu, 1:nu] - t(Sigma_obs_pred[1:n, 1:nu]) %*% inverse(Sigma_obs[1:n, 1:n]) %*% Sigma_obs_pred[1:n, 1:nu] y_pred[1:nu] ~ dmnorm(mu_pred[1:nu], cov = cov_pred[1:nu, 1:nu]) # Set up the priors for the spatial model sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma ^ 2 tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau_sq &lt;- tau ^ 2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv # prior for the coefficients beta0 ~ dnorm (0, 10) beta1 ~ dnorm (0, 10) beta2 ~ dnorm (0, 10) }) # Define the constants, data, parameters and initial values set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) constants &lt;- list(n = nrow(Mtl_benzene_sample), nu = nu) ex.data &lt;- list( y = Mtl_benzene_sample$y, easting = easting_scaled, northing = northing_scaled, easting_pred = predCoords_five[,1], northing_pred = predCoords_five[,2], distMatrix = obsDist, distMatrixUnobs = pred2predDist, distMatrixObsUnobs = obs2predDist ) params &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;phi&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;tau_sq&quot;, &quot;y_pred&quot;, &quot;sigma_sq&quot;) inits &lt;- list(sigma = 0.1, phi_inv = 6 / max(obsDist), tau = 0.1) # Run model in nimble mcmc.out &lt;- nimbleMCMC( code = Example10_10Code, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] -32.28445 ## Field &quot;lppd&quot;: ## [1] 19.03747 ## Field &quot;pWAIC&quot;: ## [1] 2.895244 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 849.3096 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out$samples[, c(&quot;beta2&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) plot(mcmc.out$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) mcmc.out$summary$all.chains ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 0.143282027 0.151098526 0.080210901 -4.320255e-02 0.28188251 ## beta1 0.047132468 0.045714893 0.059074146 -7.036740e-02 0.16808769 ## beta2 0.078236153 0.083129502 0.067977770 -6.927811e-02 0.19791770 ## phi 3.672082520 3.089047660 2.158752561 1.484596e+00 9.20444422 ## sigma 0.233174608 0.227937419 0.040783337 1.687427e-01 0.33073631 ## sigma_sq 0.056033096 0.051955467 0.021212336 2.847411e-02 0.10938656 ## tau 0.086925465 0.091948800 0.039612987 5.328545e-03 0.15319110 ## tau_sq 0.009124676 0.008454582 0.006622246 2.839674e-05 0.02346751 ## y_pred[1] -0.255307410 -0.262034854 0.307921711 -8.442072e-01 0.36678929 ## y_pred[2] -0.255612451 -0.263067874 0.306518102 -8.718905e-01 0.36502437 ## y_pred[3] -0.247526801 -0.251652153 0.299628925 -8.238475e-01 0.37022723 ## y_pred[4] -0.248606948 -0.257696742 0.303087157 -8.451291e-01 0.37558929 ## y_pred[5] -0.246847841 -0.246510211 0.301927586 -8.437388e-01 0.35702082 Stan # Load data predictions # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) # As an example we are only going to predict 5 locations nu &lt;- 5 predCoords_five &lt;- predCoords_sc[1:nu,] obs2predDist &lt;- fields::rdist(obsCoords, predCoords[1:nu,]) pred2predDist &lt;- fields::rdist(predCoords[1:nu,]) data { int&lt;lower=1&gt; N; int&lt;lower = 0&gt; N_pred; int&lt;lower=0&gt; p; vector[N] y; matrix[N,N] obs_dist_matrix; matrix[N,N_pred] obs_pred_dist_matrix; matrix[N_pred, N_pred] pred_dist_matrix; matrix[N,p] X; matrix[N_pred,p] X_pred; } parameters { real&lt;lower=0&gt; phi; real&lt;lower=0&gt; tau; real&lt;lower=0&gt; sigma; vector[p] beta; real beta0; } transformed parameters{ real&lt;lower=0&gt; sigma_sq = square(sigma); real&lt;lower=0&gt; tau_sq = square(tau); } model { matrix[N, N] L; matrix[N, N] Sigma; vector[N] mu; Sigma = sigma_sq * exp(-obs_dist_matrix/ phi) + tau_sq *diag_matrix(rep_vector(1, N)); // diagonal elements for(i in 1:N) { mu[i] = beta0 + X[i,]*beta; } L = cholesky_decompose(Sigma); beta0 ~ normal(0,10); beta ~ normal(0,10); phi ~ inv_gamma(5, 5); tau ~ cauchy(0,1); sigma ~ cauchy(0,1); y ~ multi_normal_cholesky(mu, L); } generated quantities { matrix[N_pred, N_pred] L; matrix[N, N] Sigma_obs; matrix[N_pred, N_pred] Sigma_pred; matrix[N_pred, N_pred] Cov_pred; matrix[N, N_pred] Sigma_obs_pred; vector[N] mu; vector[N_pred] mu_unobs; vector[N_pred] mu_pred; vector[N_pred] y_pred; Sigma_obs = sigma_sq * exp(-obs_dist_matrix/ phi) + tau_sq *diag_matrix(rep_vector(1, N)); Sigma_pred = sigma_sq * exp(-pred_dist_matrix/ phi) + tau_sq *diag_matrix(rep_vector(1, N_pred)); Sigma_obs_pred = sigma_sq * exp(-obs_pred_dist_matrix/ phi); for (j in 1:N_pred) { mu_unobs[j] = beta0 + X_pred[j,]*beta; } for(i in 1:N) { mu[i] = beta0 + X[i,]*beta; } mu_pred = mu_unobs + Sigma_obs_pred&#39;*inverse(Sigma_obs)*(y - mu); Cov_pred = Sigma_pred - Sigma_obs_pred&#39;*inverse(Sigma_obs)*Sigma_obs_pred; L = cholesky_decompose(Cov_pred); y_pred = multi_normal_cholesky_rng(mu_pred, L); } set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) N &lt;- nrow(benzene_utm_df) # Change coordinates to kilometers X &lt;- data.frame(easting = easting_scaled, northing = northing_scaled) ex.data &lt;- list( N = N, N_pred = nrow(predCoords_five), p = 2, y = log(benzene_utm_df$Benzene), obs_dist_matrix = obsDist, obs_pred_dist_matrix = obs2predDist, pred_dist_matrix = pred2predDist, X = as.matrix(X), X_pred = predCoords_five ) Example10_10Stan &lt;- stan( file = &quot;functions/Example10_10.stan&quot;, data = ex.data, warmup = 10000, iter = 20000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;, &quot;y_pred&quot;), include = TRUE ) rstan::traceplot(Example10_10Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;, &quot;y_pred[1]&quot;, &quot;y_pred[5]&quot;)) summary_exp_stan &lt;- summary( Example10_10Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;, &quot;y_pred&quot;), probs = c(0.025, 0.975) ) summary_exp_stan$summary ## mean se_mean sd 2.5% 97.5% ## beta0 0.15258643 0.0018234279 0.079308240 -2.001743e-02 0.28617872 ## beta[1] 0.05045002 0.0014848444 0.060158681 -5.797474e-02 0.17469632 ## beta[2] 0.08097296 0.0015480754 0.068716147 -6.234031e-02 0.19892439 ## sigma_sq 0.05490898 0.0004512223 0.019765362 2.856358e-02 0.09963885 ## tau_sq 0.00908246 0.0001454411 0.006536201 7.628813e-05 0.02330783 ## phi 3.53683130 0.0439541748 1.944669225 1.459591e+00 8.51153142 ## y_pred[1] -0.27171824 0.0064573238 0.301755582 -8.662022e-01 0.32660398 ## y_pred[2] -0.27014241 0.0062501053 0.298996379 -8.465919e-01 0.34943459 ## y_pred[3] -0.27137893 0.0065656694 0.304186203 -8.428175e-01 0.36845440 ## y_pred[4] -0.26958521 0.0066000702 0.301509144 -8.596208e-01 0.33281974 ## y_pred[5] -0.26371260 0.0065379338 0.304303176 -8.687426e-01 0.37264053 ## n_eff Rhat ## beta0 1891.731 1.0012130 ## beta[1] 1641.477 1.0027991 ## beta[2] 1970.305 1.0025511 ## sigma_sq 1918.794 0.9998376 ## tau_sq 2019.653 1.0003639 ## phi 1957.452 1.0003897 ## y_pred[1] 2183.763 1.0000693 ## y_pred[2] 2288.533 0.9998625 ## y_pred[3] 2146.451 0.9993365 ## y_pred[4] 2086.911 0.9999384 ## y_pred[5] 2166.366 1.0000249 Extra 10.10: INLA, creating a mesh This example uses the montreal_benzene_apr.csv library(geoR) library(INLA) library(spdep) benzene_data &lt;- read.csv(&quot;data/voc/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_data_loc &lt;- benzene_data coordinates(benzene_data_loc) &lt;- ~ lon + lat proj4string(benzene_data_loc) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_data_utm &lt;- spTransform(benzene_data_loc, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame locations_df &lt;- as.data.frame(benzene_data_utm@coords) benzene_utm_df &lt;- data.frame( ID = 1:nrow(benzene_data), X = benzene_data_utm@coords[,1]/1000, Y = benzene_data_utm@coords[,2]/1000, logbenzene = log(benzene_data_utm$Benzene)) # change the observed values to the log scale and the coordinates to km&#39;s mesh = inla.mesh.create(locations_df, cutoff = 0.01, refine =(list(min.angle =20))) plot(mesh , col=&quot;gray&quot;, main=&quot;&quot;) Extra 10.12: Fitting an SPDE model using R–INLA: benzene concentration in Montreal # Field std . dev . for theta =0 sigma0 = 1 # find the range of the location data size = min(c(diff(range(mesh$loc[, 1])), diff (range(mesh$loc[, 2])))) # A fifth of the approximate domain width . range0 = size/5 kappa0 = sqrt(8)/range0 tau0 = 1/(sqrt (4*pi)*kappa0*sigma0) spde = inla.spde2.matern ( mesh, B.tau = cbind(log (tau0), -1, +1), B.kappa = cbind(log (kappa0), 0, -1), theta.prior.mean = c(0 , 0), constr = TRUE ) formula = logbenzene ~ 1 + X + Y + f(ID , model = spde) model = inla( formula, family = &quot;gaussian&quot;, data = benzene_utm_df , control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE , config = TRUE) ) model$summary.fixed ## mean sd 0.025quant 0.5quant 0.975quant ## (Intercept) 8.263245418 118.01725386 -189.26676108 -5.472126510 298.73628481 ## X 0.014532837 0.02130264 -0.02563712 0.013291090 0.06340346 ## Y -0.003363593 0.02371938 -0.06108402 -0.000648258 0.03642512 ## mode kld ## (Intercept) -20.057233541 5.350248e-05 ## X 0.011915234 7.754876e-05 ## Y 0.002407345 3.418529e-05 Extra 10.13: Directional variograms ### Compute and plot the directional variogram CA.geo &lt;- as.geodata( benzene_utm_df , coords.col = 2:3 , data.col=1) CA.vario4 &lt;- variog4(CA.geo ) plot(CA.vario4) "],["Time.html", "Chapter 11 Modelling temporal data: time series analysis and forecasting Example 11.1 Ground level ozone concentrations Example 11.2 Low-pass filtering of ozone levels using the moving average Example 11.12 Forecasting ozone levels Example 11.13 Forecasting volcanic ash Example 11.17 Implementation of a dynamic linear model: UK ozone data 11.1 Example 11.18 Dynamic GLMs for daily counts of carbon monoxide on counts of infant deaths in São Paulo, Brazil Solutions to Selected Exercises Exercise 11.11 Exercise 11.13", " Chapter 11 Modelling temporal data: time series analysis and forecasting The chapter contains the theory required for handling time series data. The reader will have gained an understanding of the following topics: that a temporal process consists of both low and high frequency components, the former playing a key role in determining long-term trends while the latter may be associated with shorter-term changes; techniques for the exploratory analysis of the data generated by the temporal process, including the ACF (correlogram) and PACF (periodogram); models for irregular (high frequency) components after the regular components (trend) have been removed; methods for forecasting, including exponential smoothing and ARIMA modelling; state space modelling approach, which sits naturally within a Bayesian setting and provides a general framework within which a wide class of models, including many classical time series models, can be expressed. Example 11.1 Ground level ozone concentrations Ozone is a colorless gas produced through a combination of photochemistry, sunlight, high temperatures, oxides of nitrogen, NO\\(_x\\), emitted by motor vehicles. Levels are especially high during morning and evening commute hours in urban areas. It is one of the criteria pollutants regulated by the US Clean Air Act (1970) because of a substantial body of literature that shows a strong association with human morbidity, notably respiratory diseases such as asthma, emphysema, and chronic obstructive pulmonary disorder (COPD) (EPA, 2005). Periods of high ozone concentrations can lead to acute asthma attacks leading to increased numbers of deaths, hospital admissions, and school absences. High levels of ozone concentrations can also lead to reduced lung capacity. Let’s start by loading the daily and hourly data for one site. library(ggplot2) # Load data for one site site_daily &lt;- read.csv(&quot;data/ozone/LA_ozone_daily.csv&quot;, header = TRUE) # change the format of the date column site_daily$date &lt;- as.Date(site_daily$date, &quot;%m/%d/%Y&quot;) # load hourly data from that same site site_hourly &lt;- read.csv(&quot;data/ozone/LA_ozone_hourly.csv&quot;, header = TRUE) The following plot shows daily concentrations of ozone measured at sites located in the geographical region of Los Angeles, California. Clear seasonal patterns can be observed due to the higher temperatures in summer. # Plot daily data ggplot(data = site_daily) + geom_line(aes(x = date, y = max.ozone, group = 1)) + # draw a line at the first data geom_vline(xintercept = as.Date(&quot;2013-01-01&quot;), color = &quot;grey&quot;) + # 8 hour regulatory standard of 0.075 (ppm) geom_hline(yintercept = 0.075, color = &quot;grey&quot;) + xlab(&quot;Day in 2013 at Los Angeles Site 060379033&quot;) + ylab(&quot;Max Daily 8hr Ozone Level (ppm)&quot;) + theme_classic() And the next plot depicts the same series at the hourly level, but restricted to the so-called `ozone season’, which is taken to be May 1 – Sep 30. Here a clear 24-hour daily cycle can be seen along with a period of missing data. The latter is likely due to the monitoring systems being checked each night by the injection of a calibrated sample of air. If the instrument reports the ozone incorrectly, an alert is sounded and that instrument is taken off-line until it is repaired. This can lead to gaps in the series, as seen around hour 1100. # Plot hourly data ggplot(data = site_hourly,) + geom_line(aes(x = time, y = ozone, group = 1)) + geom_vline(xintercept = 1, color = &quot;grey&quot;) + # 8 hour regulatory standard of 0.075 (ppm). geom_hline(yintercept = 0.075, color = &quot;grey&quot;) + xlab(&quot;Hour in 2013&#39;s ozone season&quot;) + ylab(&quot;Hourly Ozone Concentration (ppm)&quot;) + theme_classic() Example 11.2 Low-pass filtering of ozone levels using the moving average We use the Ozone data of example 11.1 to look at moving averages of daily as done for carbon monoxide in example 11.3 discussed in the book. For this example we will use the TTR package (Technical Trading Rules). This allow us to manipulate objects like time series for forecasting. library(TTR) # Load daily data site_daily &lt;- read.csv(&quot;data/ozone/LA_ozone_daily.csv&quot;, header = TRUE) # add an identifier column for each day in the sample site_daily$day &lt;- 1:nrow(site_daily) # Add loess smooth # Single day moving average ozone.loess &lt;- loess(max.ozone ~ day, span = 0.75, data = site_daily[,c(&quot;max.ozone&quot;, &quot;day&quot;)]) ozone.predict &lt;- predict(ozone.loess, data.frame(day = 1:nrow(site_daily))) ozone.predict_df &lt;- data.frame(day = 1:nrow(site_daily), ozone.prediction = ozone.predict) plot_SMA1 &lt;- ggplot(data = site_daily) + geom_line(aes(x = day, y = max.ozone, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Single day&quot;) + theme_classic() # Three day moving average ozone_SMA3 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 3), day = 1:nrow(site_daily)) plot_SMA3 &lt;- ggplot(data = ozone_SMA3) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Three days&quot;) + theme_classic() # Six day moving average ozone_SMA6 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 6), day = 1:nrow(site_daily)) plot_SMA6 &lt;- ggplot(data = ozone_SMA6) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Six days&quot;) + theme_classic() # Twelve day moving average ozone_SMA12 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 12), day = 1:nrow(site_daily)) plot_SMA12 &lt;- ggplot(data = ozone_SMA12) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Twelve days&quot;) + theme_classic() cowplot::plot_grid(plot_SMA1, plot_SMA3, plot_SMA6, plot_SMA12, labels = &quot;auto&quot;) Example 11.12 Forecasting ozone levels We return to Example 11.1 with the objective of forecasting ozone concentrations for the next twenty-four hours. These forecasts are based on measured concentrations in Los Angeles from the first week of July 2013. We first fit the Holt – Winters model and see the results in the first panel of the next Figure. The plot that follows shows the twenty-four-hour ahead forecast on day eight.\\ library(TTR) library(forecast) # Load hourly data for site 060379033 site_hourly &lt;- read.csv(&quot;data/ozone/LA_ozone_hourly.csv&quot;, header = TRUE) # one night hour per day missing for instrument calibration - imputed for simplicity imputeNA &lt;- mean(site_hourly$ozone, na.rm = TRUE) site_hourly$ozone[is.na(site_hourly$ozone)] &lt;- imputeNA # Select the first seven days in july # days_pattern contains the dates for the first seven days days_pattern &lt;- paste0(&quot;^2013070&quot;, 1:7, collapse=&quot;|&quot;) # select all the rows that follow that pattern using grepl july_seven_days &lt;- site_hourly[grepl(days_pattern, site_hourly$datetime),] july_seven_days$hours &lt;- 1:nrow(july_seven_days) # Holt-Winters model fitting # Turn this into a time series object to use Holt-Winters forecast level_ts &lt;- ts(july_seven_days$ozone, frequency = 24, start = c(1)) ozone_forecast &lt;- HoltWinters(level_ts) # Plot using the default function plot(ozone_forecast, xlab = &quot;Hours - First Week -July 2013&quot;, ylab = &quot;O3 (ppm)&quot;, col.predicted = 1, col = &quot;black&quot;, bty = &quot;n&quot;, lty = 2 ) # Holt- Winters 24 ahead forast on Day 8 # no need to specify Holt-Winters forecast as the object is already HoltWinters class ozoneforecast_day8 &lt;- forecast(ozone_forecast, h=24) # Plot using default function plot(ozoneforecast_day8, bty = &quot;n&quot;) Example 11.13 Forecasting volcanic ash Volcanic ash is not a substance that is commonly encountered in environmental epidemiology, however it can be widely distributed by winds and it can be a significant health hazard. A study of the Mount St. Helens volcanic eruption in May and June 1980 reports thirty-five deaths due to the initial blast and landslide. Others are reported to have died from asphyxiation from ash inhalation (Baxter et al., 1981). The respirable portion of the ash was found to contain a small percentage of crystalline free silica, a potential pneumoconiosis hazard, and a number of acute health effects were found in those visiting emergency rooms including asthma, bronchitis and ash-related eye problems. The data in this example consists of atmospheric levels of volcanic ash from 1500AD to 2000AD. library(ggplot2) library(forecast) library(TTR) # Load volcano dust data ## REVIEW: Data source: Hyndman, R.J. Time Series Data Library, http://data.is/TSDLdemo # Data cover the period from 1500AD to 2000AD volcano_dust &lt;- scan(&quot;https://robjhyndman.com/tsdldata/annual/dvi.dat&quot;, skip = 1) The following figure shows the plot of the original time series. # Turn data into data frame to plot it in ggplot volcano_dust_df &lt;- data.frame(year = 1500:(1500+length(volcano_dust)-1), dust = volcano_dust) ggplot(data = volcano_dust_df) + geom_line(aes(x = year, y = dust, group = 1)) + xlab(&quot;Year&quot;) + ylab(&quot;Atmospheric levels of volcanic ash&quot;) + theme_classic() From the plot of the time series a complex pattern can be seen which is the result of eruptions occurring at random times. The large spikes suggest the possibility of moving average components, this is a way that the ARMA process has of incorporating shocks that abate over the period following their occurrence. There are no obvious regular components in the series that would induce autocorrelation, so we turn to analysis of the ACF and PACF. # convert data into a time series object volcano_dust_series &lt;- ts(volcano_dust, start = c(1500)) # Compute autocorrelogram with max lag 20 acf( volcano_dust_series, lag.max = 20, bty = &quot;n&quot;, main = &quot;Autocorrelogram volcano dust&quot; ) # Compute the partial autocorrelogram with max lag 20 pacf( volcano_dust_series, lag.max = 20, bty = &quot;n&quot;, main = &quot;Partial autocorrelogram volcano dust&quot; ) The correlogram is the first panel and it points to a significant autocorrelation at lag three and the possibility of an ARMA(0,3) model to capture the persistence in the ash level following a shock, and hence an MA(3) component or a mixture of an AR and MA model, might be considered. The PACF plot suggests a significant lag 2 effect, suggesting an ARMA(2,0) model. The lack of obvious regular components, together with the correlogram plots, suggests the ARIMA approach differences may be used to achieve a stationary process. The auto.arima function in the R forecast library gives the following results (using the BIC criterion). # Finding an ARIMA model auto.arima(volcano_dust_series, ic = &quot;bic&quot;) ## Series: volcano_dust_series ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## 0.7533 -0.1268 57.5274 ## s.e. 0.0457 0.0458 8.5958 ## ## sigma^2 = 4901: log likelihood = -2662.54 ## AIC=5333.09 AICc=5333.17 BIC=5349.7 # fitting the ARIMA(2,0,0) model volcano_dust_arima &lt;- arima(volcano_dust_series, order = c(2, 0, 0)) # forecast 31 years with the ARIMA(2,0,0) model volcano_dust_forecast &lt;- forecast(volcano_dust_arima, h = 31) plot(volcano_dust_forecast, bty = &quot;n&quot;) NOTE The code for the implementation of DLMs in Stan and Nimble was developed by Paritosh Kumar Roy. This code and other more complex DLM structures are available through his github. Example 11.17 Implementation of a dynamic linear model: UK ozone data In this section we analyze the ozone concentration at one monitoring site in Aberdeen, UK from March to November 2017. Nimble library(coda) library(dplyr) #library(sf) library(ggplot2) library(nimble, warn.conflicts = FALSE) library(nleqslv) library(tidybayes) library(tidyverse) source(&quot;functions/FFBS_functions_nimble.R&quot;) UK_ozone &lt;- read.csv(&quot;data/ozone/uk_ozone_one_site.csv&quot;) head(UK_ozone) ## ozone temp wind date ## 1 81.86627 2.433333 5.116667 2017-03-01 ## 2 80.81055 2.754167 6.716667 2017-03-02 ## 3 90.12082 2.716667 3.495833 2017-03-03 ## 4 90.53892 5.320833 6.537500 2017-03-04 ## 5 70.61635 4.920833 3.825000 2017-03-05 ## 6 76.80751 2.479167 1.612500 2017-03-06 Example11_16_O3_Nimble &lt;- nimbleCode({ tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) for (t in 1:Tt) { Vt[t] &lt;- tau ^ 2 } for (j in 1:p) { sqrt_Wt_diag[j] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) } Wt[1:p, 1:p] &lt;- nim_diag(x = sqrt_Wt_diag[1:p] ^ 2) mt[1:p, 1] &lt;- m0[1:p] Ct[1:p, 1:p, 1] &lt;- C0[1:p, 1:p] for (t in 1:Tt) { at[1:p, t] &lt;- (Gt[1:p, 1:p] %*% mt[1:p, t])[1:p, 1] Rt[1:p, 1:p, t] &lt;- Gt[1:p, 1:p] %*% Ct[1:p, 1:p, t] %*% t(Gt[1:p, 1:p]) + Wt[1:p, 1:p] ft[t] &lt;- (t(Ft[1:p, t]) %*% at[1:p, t])[1, 1] Qt[t] &lt;- (t(Ft[1:p, t]) %*% Rt[1:p, 1:p, t] %*% Ft[1:p, t] + Vt[t])[1, 1] yt[t] ~ dnorm(mean = ft[t], var = Qt[t]) At[1:p, t] &lt;- (Rt[1:p, 1:p, t] %*% Ft[1:p, t])[1:p, 1] / Qt[t] mt[1:p, t + 1] &lt;- at[1:p, t] + (At[1:p, t] * (yt[t] - ft[t])) Ct[1:p, 1:p, t + 1] &lt;- Rt[1:p, 1:p, t] - (At[1:p, t] %*% t(At[1:p, t])) * Qt[t] } theta[1:p, 1:(Tt + 1)] &lt;- nim_bsample( mt = mt[1:p, 1:(Tt + 1)], Ct = Ct[1:p, 1:p, 1:(Tt + 1)], at = at[1:p, 1:Tt], Gt = Gt[1:p, 1:p], Rt = Rt[1:p, 1:p, 1:Tt] ) }) Time-varying level model We first fit a time-varying level model such that, \\[\\begin{aligned} Y_t &amp;= F_t\\theta_t + v_t, \\qquad &amp;v_t \\sim \\mathcal{N}(0, \\tau^2) \\\\ \\theta_t &amp;= G_t\\theta_{t-1} + \\omega_t \\qquad &amp;\\omega_t \\sim \\mathcal{N}(0, W^2) \\end{aligned}\\] For the time-varying mean model \\(F_t = (1, \\ wind_t, \\ temp_t)\\), and \\(G_t = I_3\\). # Model specification yt &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(yt) p &lt;- 3 # (intercept, wind, temp) Ft &lt;- array(0, dim = c( p, Tt)) Ft[ 1,] &lt;- 1 Ft[ 2,] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[ 3,] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) m0 &lt;- c(mean(yt), 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) const_list &lt;- list(Tt = Tt, p = p) dat_list &lt;- list( yt = yt, Ft = Ft, Gt = Gt, m0 = m0, C0 = C0 ) init_list &lt;- list(tau = 0.01, sqrt_Wt_diag = sqrt(rep(0.1, p))) Rmodel &lt;- nimbleModel( Example11_16_O3_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Rmodel$calculate() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;, &quot;ft&quot;)) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = FALSE ) niter &lt;- 10000 nburnin &lt;- 0.5 * niter nthin &lt;- 1 nchains &lt;- 2 start_time &lt;- Sys.time() post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = nchains, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time post_summary &lt;- nimSummary(post_samples) tidy_post_samples &lt;- post_samples |&gt; tidy_draws() tidy_post_samples |&gt; dplyr::select( .chain, .iteration, .draw, &#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39; ) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(panel.grid = element_blank(), strip.background = element_blank()) post_summary[c(&#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;), ] ## post.mean post.sd q2.5 q50 q97.5 f0 n.eff Rhat ## tau 0.550 0.033 0.487 0.551 0.614 1 1288.971 1.001 ## sqrt_Wt_diag[1] 0.117 0.040 0.051 0.113 0.202 1 1161.622 1.001 ## sqrt_Wt_diag[2] 0.017 0.009 0.004 0.016 0.039 1 1666.705 1.007 ## sqrt_Wt_diag[3] 0.034 0.031 0.001 0.025 0.118 1 724.404 1.004 post_sum_theta &lt;- as.data.frame(post_summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; dplyr::select(rowname, q2.5, q50, q97.5) |&gt; separate(rowname, into = c(&quot;x1&quot;, &quot;x2&quot;), sep = &quot;,&quot;) |&gt; mutate(component = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x1))) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x2))) |&gt; dplyr::select(component, time, q2.5, q50, q97.5) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7) + geom_path(aes(y = q50), col = &quot;blue&quot;, linewidth = 0.4) + facet_wrap( ~ component, nrow = 2, scales = &quot;free&quot;, labeller = label_bquote(theta[.(component)]) ) + ylab(&quot;&quot;) + xlab(&quot;Time&quot;) + theme_classic() npsample &lt;- floor((niter - nburnin)/nthin) # str(tidy_post_samples) Cnim_postfit_uoms &lt;- compileNimble(nim_postfit_uoms, showCompilerOutput = FALSE) ## Compiling ## [Note] This may take a minute. ## [Note] Use &#39;showCompilerOutput = TRUE&#39; to see C++ compilation details. post_tau &lt;- tidy_post_samples$tau post_sqrt_Wt_diag &lt;- tidy_post_samples |&gt; dplyr::select(starts_with(&quot;sqrt_Wt_diag&quot;)) |&gt; as.matrix()|&gt; unname() post_ft_list &lt;- lapply(1:(nchains*npsample), function(i) { post_Vt &lt;- post_tau[i]^2 post_Wt &lt;- nim_diag(post_sqrt_Wt_diag[i,]^2) post_ft &lt;- Cnim_postfit_uoms(yt = dat_list$yt, Ft = dat_list$Ft, Vt = post_Vt, Gt = dat_list$Gt, Wt = post_Wt, m0 = dat_list$m0, C0 = dat_list$C0) post_ft &lt;- data.frame(ft = post_ft) |&gt; mutate(time = row_number()) return(post_ft) }) tidy_post_ft &lt;- do.call(&quot;rbind&quot;, post_ft_list) str(tidy_post_ft) ## &#39;data.frame&#39;: 2750000 obs. of 2 variables: ## $ ft : num 10.67 10.06 9.77 8.48 9.02 ... ## $ time: int 1 2 3 4 5 6 7 8 9 10 ... head(tidy_post_ft) ## ft time ## 1 10.673309 1 ## 2 10.055283 2 ## 3 9.772315 3 ## 4 8.479460 4 ## 5 9.019850 5 ## 6 8.055427 6 ## posterior summaries of ft head(tidy_post_ft) ## ft time ## 1 10.673309 1 ## 2 10.055283 2 ## 3 9.772315 3 ## 4 8.479460 4 ## 5 9.019850 5 ## 6 8.055427 6 post_sum_ft &lt;- tidy_post_ft |&gt; group_by(time) |&gt; summarise(post.mean = mean(ft), post.sd = sd(ft), q2.5 = quantile(ft, prob = 0.025), q50 = quantile(ft, prob = 0.50), q97.5 = quantile(ft, prob = 0.975)) |&gt; ungroup() str(post_sum_ft) ## tibble [275 × 6] (S3: tbl_df/tbl/data.frame) ## $ time : int [1:275] 1 2 3 4 5 6 7 8 9 10 ... ## $ post.mean: num [1:275] 8.14 8.96 8.95 8.66 9.2 ... ## $ post.sd : num [1:275] 2.296 1.277 1.139 0.789 0.761 ... ## $ q2.5 : Named num [1:275] 3.67 6.48 6.73 7.1 7.68 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;2.5%&quot; &quot;2.5%&quot; &quot;2.5%&quot; &quot;2.5%&quot; ... ## $ q50 : Named num [1:275] 8.13 8.96 8.95 8.65 9.2 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;50%&quot; &quot;50%&quot; &quot;50%&quot; &quot;50%&quot; ... ## $ q97.5 : Named num [1:275] 12.6 11.5 11.2 10.2 10.7 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;97.5%&quot; &quot;97.5%&quot; &quot;97.5%&quot; &quot;97.5%&quot; ... post_sum_ft |&gt; head() ## # A tibble: 6 × 6 ## time post.mean post.sd q2.5 q50 q97.5 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8.14 2.30 3.67 8.13 12.6 ## 2 2 8.96 1.28 6.48 8.96 11.5 ## 3 3 8.95 1.14 6.73 8.95 11.2 ## 4 4 8.66 0.789 7.10 8.65 10.2 ## 5 5 9.20 0.761 7.68 9.20 10.7 ## 6 6 8.90 0.909 7.08 8.89 10.7 ggplot(data = post_sum_ft, aes(x = time)) + geom_ribbon(aes(ymin = q2.5,ymax = q97.5), fill = &quot;lightgray&quot;) + geom_path(aes(y = post.mean), linewidth = 0.25) + geom_point(aes(y=yt), size = 0.25) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() # Residuals plot res &lt;- post_sum_ft$post.mean - dat_list$yt acf(res, lag.max = 35,bty = &quot;n&quot;) Trend model In this case we include a trend then, \\(F_t = (1, 0, \\ wind_t, \\ temp_t)\\) and \\(G_t = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1\\end{pmatrix}\\). # Model specification yt &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(yt) p &lt;- 4 # (intercept, trend, wind, temp) Ft &lt;- array(0, dim = c( p, Tt)) Ft[1,] &lt;- 1 Ft[2,] &lt;- 0 Ft[3,] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[4,] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) Gt[1, 2] &lt;- 1 m0 &lt;- c(mean(yt), 0, 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) const_list &lt;- list(Tt = Tt, p = p) dat_list &lt;- list( yt = yt, Ft = Ft, Gt = Gt, m0 = m0, C0 = C0 ) init_list &lt;- list(tau = 0.01, sqrt_Wt_diag = sqrt(rep(0.1, p))) Rmodel &lt;- nimbleModel( Example11_16_O3_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Rmodel$calculate() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;, &quot;ft&quot;)) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = FALSE ) niter &lt;- 10000 nburnin &lt;- 0.5 * niter nthin &lt;- 1 nchains &lt;- 2 start_time &lt;- Sys.time() post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = nchains, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time post_summary &lt;- nimSummary(post_samples) tidy_post_samples &lt;- post_samples |&gt; tidy_draws() tidy_post_samples |&gt; dplyr::select( .chain, .iteration, .draw, &#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;, &#39;sqrt_Wt_diag[4]&#39; ) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(panel.grid = element_blank(), strip.background = element_blank()) post_summary[c(&#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;, &#39;sqrt_Wt_diag[4]&#39;), ] ## post.mean post.sd q2.5 q50 q97.5 f0 n.eff Rhat ## tau 0.544 0.037 0.471 0.543 0.615 1 875.124 1.004 ## sqrt_Wt_diag[1] 0.131 0.055 0.030 0.128 0.244 1 850.247 1.002 ## sqrt_Wt_diag[2] 0.002 0.002 0.000 0.002 0.007 1 930.546 1.000 ## sqrt_Wt_diag[3] 0.017 0.009 0.004 0.016 0.040 1 1694.931 1.009 ## sqrt_Wt_diag[4] 0.036 0.032 0.001 0.027 0.120 1 1002.860 1.023 post_sum_theta &lt;- as.data.frame(post_summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; dplyr::select(rowname, q2.5, q50, q97.5) |&gt; separate(rowname, into = c(&quot;x1&quot;, &quot;x2&quot;), sep = &quot;,&quot;) |&gt; mutate(component = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x1))) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x2))) |&gt; dplyr::select(component, time, q2.5, q50, q97.5) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7) + geom_path(aes(y = q50), col = &quot;blue&quot;, linewidth = 0.4) + facet_wrap( ~ component, nrow = 2, scales = &quot;free&quot;, labeller = label_bquote(theta[.(component)]) ) + ylab(&quot;&quot;) + xlab(&quot;Time&quot;) + theme_classic() npsample &lt;- floor((niter - nburnin)/nthin) Cnim_postfit_uoms &lt;- compileNimble(nim_postfit_uoms, showCompilerOutput = FALSE) ## Compiling ## [Note] This may take a minute. ## [Note] Use &#39;showCompilerOutput = TRUE&#39; to see C++ compilation details. post_tau &lt;- tidy_post_samples$tau post_sqrt_Wt_diag &lt;- tidy_post_samples |&gt; dplyr::select(starts_with(&quot;sqrt_Wt_diag&quot;)) |&gt; as.matrix()|&gt; unname() post_ft_list &lt;- lapply(1:(nchains*npsample), function(i) { post_Vt &lt;- post_tau[i]^2 post_Wt &lt;- nim_diag(post_sqrt_Wt_diag[i,]^2) post_ft &lt;- Cnim_postfit_uoms(yt = dat_list$yt, Ft = dat_list$Ft, Vt = post_Vt, Gt = dat_list$Gt, Wt = post_Wt, m0 = dat_list$m0, C0 = dat_list$C0) post_ft &lt;- data.frame(ft = post_ft) |&gt; mutate(time = row_number()) return(post_ft) }) tidy_post_ft &lt;- do.call(&quot;rbind&quot;, post_ft_list) ## posterior summaries of ft post_sum_ft &lt;- tidy_post_ft |&gt; group_by(time) |&gt; summarise(post.mean = mean(ft), post.sd = sd(ft), q2.5 = quantile(ft, prob = 0.025), q50 = quantile(ft, prob = 0.50), q97.5 = quantile(ft, prob = 0.975)) |&gt; ungroup() ggplot(data = post_sum_ft, aes(x = time)) + geom_ribbon(aes(ymin = q2.5,ymax = q97.5), fill = &quot;lightgray&quot;) + geom_path(aes(y = post.mean), size = 0.25) + geom_point(aes(y=yt), size = 0.25) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() ## Warning: Using `size` aesthetic for lines was deprecated ## in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. # Residuals plot res &lt;- post_sum_ft$post.mean - dat_list$yt acf(res, lag.max = 35,bty = &quot;n&quot;) Stan library(coda) library(ggplot2) library(gridExtra) library(rstan) library(tidyverse) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Load data for RW case UK_ozone &lt;- read.csv(&quot;data/ozone/uk_ozone_one_site.csv&quot;) Time-varying level model y &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(y) p &lt;- 3 # (intercept, wind, temp) Ft &lt;- array(0, dim = c(Tt, p)) Ft[, 1] &lt;- 1 Ft[, 2] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[, 3] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) m0 &lt;- c(mean(y), 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) stan_data &lt;- list(T = Tt, p = p, y = y, F = Ft, G = Gt, m0= m0, C0 = C0) functions { // FFBS for DLM with univariate observation equation and univariate system equation array[] vector uoms_ffbs_rng(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T] vector[p] theta; array[T] vector[p] a; array[T] matrix[p,p] R; array[T] vector[p] m; array[T] matrix[p,p] C; // Kalman filtering vector[p] mt = m0; matrix[p,p] Ct = C0; for(i in 1:T){ real ft; real Qt; vector[p] at; matrix[p,p] Rt; vector[p] At; at = G * mt; Rt = G * Ct * G&#39; + W; ft = F[i]&#39; * at; Qt = quad_form(Rt, F[i]) + V; //F[i]&#39; * Rt * F[i] + V; At = Rt * F[i] * inv(Qt); mt = at + At * (y[i] - ft); Ct = Rt - At * Qt * At&#39;; //store for backward sampling a[i] = at; R[i] = Rt; m[i] = mt; C[i] = Ct; } // backward sampling array[T-1] int ind = sort_indices_desc(linspaced_int_array(T-1,1,T-1)); theta[T] = multi_normal_rng(m[T], C[T]); for(i in ind) { matrix[p,p] Bt; vector[p] ht; matrix[p,p] Ht; Bt = C[i] * G&#39; * inverse(R[i+1]); ht = m[i] + Bt * (theta[i+1] - a[i+1]); Ht = C[i] - Bt * R[i+1] * Bt&#39;; theta[i] = multi_normal_rng(ht, Ht); } return theta; } real uoms_dlm_ldensity(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T+1] vector[p] a; array[T+1] matrix[p, p] R; array[T] real lldata; a[1] = m0; R[1] = C0; for (i in 1:T) { real u; real Q; real Qinv; vector[p] A; matrix[p, p] L; u = y[i] - F[i]&#39; * a[i]; Q = quad_form(R[i],F[i]) + V; //F[i]&#39; * R[i] * F[i] + V; Qinv = inv(Q); // A = G * R[i] * F[i] * Qinv; L = G - A * F[i]&#39;; //lldata[i] = -0.5 * (log(2 * pi()) + log(Q) + Qinv*square(u)); lldata[i] = normal_lpdf(u | 0, sqrt(Q)); // univariate a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L&#39; + W; } return sum(lldata); } array[] real uoms_dlm_one_step_ahead_rng(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T] real yfit; array[T+1] vector[p] a; array[T+1] matrix[p, p] R; array[T] real lldata; a[1] = m0; R[1] = C0; for (i in 1:T) { real u; real Q; real Qinv; vector[p] A; matrix[p, p] L; u = y[i] - F[i]&#39; * a[i]; Q = quad_form(R[i],F[i]) + V; //F[i]&#39; * R[i] * F[i] + V; Qinv = inv(Q); // A = G * R[i] * F[i] * Qinv; L = G - A * F[i]&#39;; yfit[i] = normal_rng(F[i]&#39; * a[i], sqrt(Q)); // univariate a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L&#39; + W; } return yfit; } } data{ int T; int p; array[T] real y; array[T] vector[p] F; matrix[p, p] G; vector[p] m0; cov_matrix[p] C0; } parameters{ real&lt;lower=0&gt; tau; vector&lt;lower=0&gt;[p] sqrt_W_diag; } model { real V = square(tau); matrix[p, p] W = diag_matrix(square(sqrt_W_diag)); tau ~ std_normal(); sqrt_W_diag ~ std_normal(); target += uoms_dlm_ldensity(y, F, G, V, W, m0, C0, T, p); } generated quantities{ array[T] vector[p] theta; array[T] real yfit; real V = square(tau); matrix[p, p] W = diag_matrix(square(sqrt_W_diag)); theta = uoms_ffbs_rng(y, F, G, V, W, m0, C0, T, p); yfit = uoms_dlm_one_step_ahead_rng(y, F, G, V, W, m0, C0, T, p); } Example11_16_UK_Stan &lt;- stan( file = &quot;functions/Example11_16_UK.stan&quot;, data = stan_data, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) rstan::traceplot(Example11_16_UK_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;, &quot;theta[1,1]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,1]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,2]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,3]&quot;)) fixedpars_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;))$summary fixedpars_summary ## mean se_mean sd 2.5% 25% ## tau 0.54952783 3.456478e-04 0.033200616 0.483950759 0.52724045 ## sqrt_W_diag[1] 0.11879062 4.682828e-04 0.042765301 0.051417741 0.08668241 ## sqrt_W_diag[2] 0.01702899 8.180045e-05 0.008928214 0.003498400 0.01079707 ## sqrt_W_diag[3] 0.03420662 3.088344e-04 0.030751202 0.001145905 0.01123936 ## 50% 75% 97.5% n_eff Rhat ## tau 0.54963949 0.57140419 0.61408371 9226.237 1.0003090 ## sqrt_W_diag[1] 0.11341398 0.14535262 0.21426583 8340.013 1.0000506 ## sqrt_W_diag[2] 0.01557511 0.02165377 0.03841738 11912.906 0.9999899 ## sqrt_W_diag[3] 0.02543997 0.04824451 0.11442124 9914.543 1.0001423 theta_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;theta&quot;))$summary yfit_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;yfit&quot;))$summary p1 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,1]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date, group = 1)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Intercept&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,2]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Wind&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p3 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,3]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Temperature&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1,p2,p3, ncol = 2) # Fitted values yfit_summary_dt &lt;- data.frame(yfit_summary) |&gt; rownames_to_column() |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(yfit_summary_dt, aes(x = time)) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + geom_point(aes(y = y)) + geom_path(aes(y = mean), linewidth = 0.1, size = 1) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. Trend model y &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(y) p &lt;- 4 # (intercept, trend, wind, temp) Ft &lt;- array(0, dim = c(Tt, p)) Ft[, 1] &lt;- 1 Ft[, 2] &lt;- 0 Ft[, 3] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[, 4] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) Gt[1, 2] &lt;- 1 m0 &lt;- c(mean(y), 0, 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) stan_data_trend &lt;- list( T = Tt, p = p, y = y, F = Ft, G = Gt, m0 = m0, C0 = C0 ) Example11_16_UK_trend_Stan &lt;- stan( file = &quot;functions/Example11_16_UK.stan&quot;, data = stan_data_trend, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) rstan::traceplot(Example11_16_UK_trend_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;, &quot;theta[1,1]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,1]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,2]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,3]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,4]&quot;)) fixedpars_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;))$summary fixedpars_summary ## mean se_mean sd 2.5% 25% ## tau 0.543609759 4.244185e-04 0.034916177 4.759466e-01 0.5198417513 ## sqrt_W_diag[1] 0.131495609 6.716602e-04 0.052039444 3.354044e-02 0.0950301137 ## sqrt_W_diag[2] 0.002097407 1.780222e-05 0.001951722 7.149022e-05 0.0007203768 ## sqrt_W_diag[3] 0.016756008 8.775711e-05 0.008939004 3.193146e-03 0.0104914524 ## sqrt_W_diag[4] 0.035244960 3.018273e-04 0.031556739 1.061907e-03 0.0115224492 ## 50% 75% 97.5% n_eff Rhat ## tau 0.543855509 0.566994280 0.612394177 6768.068 1.0005912 ## sqrt_W_diag[1] 0.129907861 0.165821794 0.237108079 6002.968 1.0005880 ## sqrt_W_diag[2] 0.001565133 0.002890185 0.007144111 12019.539 0.9998917 ## sqrt_W_diag[3] 0.015321489 0.021433185 0.037871038 10375.611 1.0001893 ## sqrt_W_diag[4] 0.026463865 0.049436417 0.118025666 10931.186 1.0000489 theta_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;theta&quot;))$summary yfit_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;yfit&quot;))$summary p1 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,1]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date, group = 1)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Intercept&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,4]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Trend&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p3 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,2]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Wind&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p4 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,3]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Temperature&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1,p2,p3, p4, ncol = 2) # Fitted values yfit_summary_dt &lt;- data.frame(yfit_summary) |&gt; rownames_to_column() |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(yfit_summary_dt, aes(x = time)) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + geom_point(aes(y = y)) + geom_path(aes(y = mean), linewidth = 0.1, size = 1) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() 11.1 Example 11.18 Dynamic GLMs for daily counts of carbon monoxide on counts of infant deaths in São Paulo, Brazil This dataset was analyzed by Alves et al. (2010) and is related to daily counts of deaths of infants under 5 years old due to respiratory causes in São Paulo, Brazil. The sampling period is from January 1, 1994, until 31 December 1997. We fit a simplified version of the model fitted in Alves et al. (2010), Solutions to Selected Exercises Exercise 11.11 The joint distribution of \\(\\mathbf{\\gamma} = \\{\\gamma_1, \\gamma_2, \\dots, \\gamma_{N_T}\\}\\) can be written as: \\[ p(\\mathbf{\\gamma}) = p(\\gamma_1) \\prod_{t=1}^{N_T-1}p(\\gamma_{t+1}\\mid\\gamma)\\\\ = p(\\gamma_1)(2\\pi\\sigma_w^2)^{-(N_T-1)/2}\\exp\\{- \\frac{1}{2\\sigma_w^2} \\sum_{t=1}^{N_T-1} (\\gamma_{t+1} - \\gamma)^2) \\}\\\\ \\propto p(\\gamma_1) \\exp\\{-\\frac{1}{2\\sigma_w^2}\\mathbf{\\gamma}^\\top M \\mathbf{\\gamma}\\}, \\] where the entries of \\(M\\) are: \\[ M_{ij} = \\begin{cases} 1, &amp; i = j = 1, N_T\\\\ 2, &amp; i = j = 2, 3, \\dots, N_T - 1 \\\\ -1 &amp; i = (j-1), \\text{where } j = 2, 3, \\dots, N_T\\\\ -1 &amp; i = (j+1), \\text{where } j = 1, 2, \\dots, N_T - 1 \\\\ 0 &amp; \\text{else} \\end{cases}. \\] Now, if we assume that \\(p(\\gamma_1) \\propto 1\\), i.e., an uninformative, improper prior, we have that \\[ p(\\mathbf{\\gamma}) \\propto \\exp\\{-\\frac{1}{2\\sigma_w^2}\\mathbf{\\gamma}^\\top M \\mathbf{\\gamma}\\}, \\] which implies that, for any \\(t\\), by Bayes rule, \\[ p(\\gamma_t|\\mathbf{\\gamma}_{-t}) = \\frac{p(\\mathbf{\\gamma})}{p(\\mathbf{\\gamma}_{-t})} \\propto p(\\mathbf{\\gamma}). \\] Eliminating terms in the joint distribution \\(P(\\gamma)\\) not related to \\(\\gamma_t\\), we obtain \\[ p(\\gamma_t|\\mathbf{\\gamma}_{-t}) \\propto \\begin{cases} \\exp\\{-\\frac{1}{2\\sigma_w^2}(\\gamma_t^2 - 2\\gamma_t\\gamma_{t+1})\\}, &amp; t = 1\\\\ \\exp\\{-\\frac{1}{2\\sigma_w^2}(2\\gamma_t^2 - 2\\gamma_t\\gamma_{t+1} - 2\\gamma_{t-1}\\gamma_t)\\}, &amp; t = 2, 3, \\dots, N_T-1 \\\\ \\exp\\{-\\frac{1}{2\\sigma_w^2}(\\gamma_t^2 - 2\\gamma_{t-1}\\gamma_{t})\\}, &amp; t = N_T \\end{cases}. \\] The proportional form implies that these are Normal densities. By completing the square, we can obtain \\[ p(\\gamma|\\gamma_{-t}) \\propto \\begin{cases} N(\\gamma_{t+1}, \\sigma_w^2), &amp; t = 1 \\\\ N(\\frac{\\gamma_{t-1}+\\gamma_{t+1}}{2}, \\sigma^2_w/2), &amp; t = 2, 3 \\dots, N_T-1 \\\\ N(\\gamma_{t-1}, \\sigma_w^2), &amp; t = N_T \\end{cases}, \\] as required. A necessary assumption for this conclusion was that \\(p(\\gamma_1) \\propto 1\\). ## Exercise 11.12 {-} The joint density of \\(\\gamma_t\\) and \\(\\tau_w\\), given \\(\\gamma_{t-1}\\) is given by \\[ p(\\gamma_t, \\tau_w \\mid \\gamma_{t-1}) = \\frac{\\tau_w^{1/2}}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{\\tau(\\gamma_t - \\gamma_{t-1})^2}{2} \\right) \\frac{b^a}{\\Gamma(a)}\\tau_w^{a-1}\\exp(-b\\tau_w). \\] Marginalizing \\(\\tau_w\\) gives \\[ p(\\gamma_t \\mid \\gamma_{t-1}) = \\int_{0}^{\\infty} \\frac{\\tau_2^{1/2}}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{\\tau(\\gamma_t - \\gamma_{t-1})^2}{2} \\right) \\frac{b^a}{\\Gamma(a)}\\tau_w^{a-1}\\exp(-b\\tau_w) d\\tau_w \\\\ = \\frac{b^2}{\\Gamma(a)\\sqrt{2\\pi}} \\int_{0}^\\infty \\tau_w^{a-0.5} \\exp \\left(-\\tau_w \\left[ \\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b\\right] \\right) d\\tau_w \\\\ = \\frac{b^a \\Gamma(a+0.5)}{\\gamma(a)\\sqrt{2\\pi} ((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}} \\int_{0}^\\infty \\frac{(\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}{\\Gamma(a+0.5)} \\tau_w^{(a +0.5) - 1} \\exp\\left( -\\tau_w \\left[ \\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b\\right]\\right)\\\\ = \\frac{b^a \\Gamma(a+0.5)}{\\gamma(a)\\sqrt{2\\pi} ((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}, \\] where the last equality follows because the integrand was the density of a Gamma distribution. Now, by Bayes rule, we have the following expression for the posterior: \\[ p(\\tau_{w}|\\gamma_t, \\gamma_{t-1}) = \\frac{p(\\gamma_t, \\tau_w \\mid \\gamma_{t-1})}{p(\\gamma_t \\mid \\gamma_{t-1}) } = \\frac{((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}{\\Gamma(a + 0.5)}\\tau^{(a+0.5)-1} \\exp \\left(-\\tau_w \\left[\\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b \\right] \\right), \\] which is the density function of the \\(Ga(a + 0.5, (\\gamma_t - \\gamma_{t-1})^2/2 + b)\\) distribution. The shape parameter of the posterior is the prior shape parameter shifted by \\(0.5\\), and the rate parameter is the prior rate parameter shifted by the term \\((\\gamma_t - \\gamma_{t-1})^2/2\\). Exercise 11.13 We use the rjags package here, as it most closely resembles WinBUGS, but is compatible with all operating systems. library(rjags) ## Warning: package &#39;rjags&#39; was built under R version 4.2.3 library(tidyverse) data &lt;- source(&quot;data/pm_london/pm10_London.txt&quot;) inits1 &lt;- source(&quot;data/pm_london/11_13_model1_inits1.txt&quot;) model_ar &lt;- jags.model(file = &quot;data/pm_london/11_13_model1.txt&quot;, data = data$value, inits = inits1$value, n.chains = 3) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1410 ## Unobserved stochastic nodes: 1514 ## Total graph size: 2934 ## ## Initializing model update(model_ar, 5000) ## burn in samples_sigmas &lt;- coda.samples(model_ar, variable.names = c( &quot;sigma.v&quot;, &quot;sigma.w&quot; ), n.iter = 2000, thin = 2) plot(samples_sigmas) apply(samples_sigmas[[1]], 2, quantile) ## sigma.v sigma.w ## 0% 0.009440187 0.3604371 ## 25% 0.010027101 0.4239586 ## 50% 0.010254047 0.4472214 ## 75% 0.010486342 0.4706450 ## 100% 0.011485331 0.6060564 samples_gamma &lt;- coda.samples(model_ar, variable.names = c( &quot;gamma&quot;, &quot;y&quot; ), n.iter = 2000, thin = 1) missings &lt;- which(is.na(data$value$y)) nonmissing &lt;- tibble(var = &quot;y&quot;, time = as.double(seq(1, length(data$value$y))), value = data$value$y) |&gt; filter(!(time %in% missings)) plot_df &lt;- data.frame(samples_gamma[[1]]) plot_df &lt;- plot_df |&gt; pivot_longer(cols = everything(), names_to = &quot;variable&quot;) |&gt; separate(variable, c(&quot;var&quot;, &quot;time&quot;)) |&gt; mutate(time = as.numeric(time)) |&gt; filter(!(var == &quot;y&quot; &amp; !(time %in% missings))) |&gt; rbind(nonmissing) |&gt; group_by(var, time) |&gt; summarise(q025 = quantile(value, prob = 0.025), q975 = quantile(value, prob = 0.975), median = median(value))|&gt; mutate(range = q975-q025) ## Warning: Expected 2 pieces. Additional pieces discarded in 5844000 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## `summarise()` has grouped output by &#39;var&#39;. You ## can override using the `.groups` argument. plot_scatter_df &lt;- plot_df |&gt; dplyr::select(var, time, median) |&gt; pivot_wider(names_from = var, values_from = median) plot_scatter &lt;- ggplot(plot_scatter_df, aes(x = y, y = gamma)) + geom_point(alpha = 0.5) plot_scatter plot &lt;- ggplot(filter(plot_df, var == &quot;gamma&quot;), aes(x = time)) + geom_line(aes(y = median), alpha = 0.5) + geom_line(aes(y = q025), alpha = 0.5, linetype = &quot;dashed&quot;)+ geom_line(aes(y = q975), alpha = 0.5, linetype = &quot;dashed&quot;)+ geom_point(data = filter(plot_df, (var == &quot;y&quot; &amp; time %in% missings)), aes(x = time, y = 0), color = &quot;blue&quot;) plot "],["exposure.html", "Chapter 12 Exposure assessment-over space and time Example IDK: Spatio-temporal model for the UK ozone data", " Chapter 12 Exposure assessment-over space and time In this chapter we have seen the many ways in which the time can be added to space in order to characterize random exposure fields. From this chapter, the reader will have gained an understanding of the following topics: Additional power that can be gained in an epidemiological study by combining the contrasts in the process over both time and space while characterizing the stochastic dependencies across both space and time for inferential analysis. Criteria that good approaches to spatio–temporal modelling should satisfy. General strategies for developing such approaches. Separability and non-separability in spatio–temporal models, and how these could be characterize using the Kronecker product of correlation matrices. Examples of the use of spatio–temporal models in modelling environmental exposures. Example IDK: Spatio-temporal model for the UK ozone data NOTE The code for the implementation of DLMs in Stan and Nimble was developed by Paritosh Kumar Roy. This code and other more complex DLM structures are available through his github. Nimble library(dplyr) library(sf) library(ggplot2) library(nimble, warn.conflicts = FALSE) library(nleqslv) source(&quot;functions/FFBS_functions_nimble.R&quot;) load(&quot;data/ozone/uk_o3.rda&quot;) load(&quot;data/ozone/uk_temp.rda&quot;) load(&quot;data/ozone/uk_wind.rda&quot;) load(&quot;data/ozone/uk_pollutant_coords.rda&quot;) load(&quot;data/ozone/uk_pollutant_date.rda&quot;) ls() ## [1] &quot;coords&quot; &quot;date&quot; ## [3] &quot;ddlm_moms_invariant&quot; &quot;ddlm_multi_obs&quot; ## [5] &quot;ddlm_uoms&quot; &quot;ddlm_uous&quot; ## [7] &quot;hyperParameterInvGamma&quot; &quot;nim_bsample&quot; ## [9] &quot;nim_chol2inv&quot; &quot;nim_diag&quot; ## [11] &quot;nim_ffbs_moms&quot; &quot;nim_ffbs_moms_invariant&quot; ## [13] &quot;nim_ffbs_uoms&quot; &quot;nim_ffbs_uous&quot; ## [15] &quot;nim_ffbs_woodbury&quot; &quot;nim_kf_moms&quot; ## [17] &quot;nim_postfit_uoms&quot; &quot;nim_postfit_uous&quot; ## [19] &quot;nim_woodbury&quot; &quot;nimSummary&quot; ## [21] &quot;o3&quot; &quot;rdlm_moms_invariant&quot; ## [23] &quot;rdlm_multi_obs&quot; &quot;rdlm_uoms&quot; ## [25] &quot;rdlm_uous&quot; &quot;sampler_ffbs_moms_invariant&quot; ## [27] &quot;sampler_ffbs_uoms&quot; &quot;sampler_ffbs_uous&quot; ## [29] &quot;sampler_ffbseta&quot; &quot;temp&quot; ## [31] &quot;wind&quot; Example12_O3_Nimble &lt;- nimbleCode({ sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) phi_inv ~ dgamma(shape = aPhi, rate = bPhi) phi &lt;- 1 / phi_inv Vt[1:n, 1:n] &lt;- (sigma ^ 2) * exp(-obs_dist_mat[1:n, 1:n] / phi) + (tau ^ 2) * identityMatrix(d = n) sqrt_Wt_diag[1] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sqrt_Wt_diag[2] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sqrt_Wt_diag[3] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) Wt[1:3, 1:3] &lt;- nim_diag(x = sqrt_Wt_diag[1:3] ^ 2) yt[1:n, 1:Tt] ~ ddlm_multi_obs( Ft = Ft[1:p, 1:n, 1:Tt], Vt = Vt[1:n, 1:n], Gt = Gt[1:p, 1:p], Wt = Wt[1:p, 1:p], m0 = m0[1:p], C0 = C0[1:p, 1:p] ) # one can sample theta after fitting the model, using the posterior samples # otherwise, sampling here is okay. theta[1:p, 1:(Tt + 1)] &lt;- nim_ffbs_woodbury( yt = yt[1:n, 1:Tt], Ft = Ft[1:p, 1:n, 1:Tt], Vt = Vt[1:n, 1:n], Gt = Gt[1:p, 1:p], Wt = Wt[1:p, 1:p], m0 = m0[1:p], C0 = C0[1:p, 1:p] ) }) # NOTE: I need to ask Paritosh about the ddlm_multi_obs function, it is missing # in the functions file # Model specification n &lt;- dim(o3)[1] Tt &lt;- dim(o3)[2] p &lt;- 3 Ft &lt;- array(0, dim = c(p,n,Tt)) str(Ft) Ft[1,1:n,1:Tt] &lt;- 1 Ft[2,1:n,1:Tt] &lt;- temp Ft[3,1:n,1:Tt] &lt;- wind Gt &lt;- diag(p) Gt # initials m0 &lt;- c(mean(o3),0,0) C0 &lt;- diag(x=c(1e2,1,1)) sqrt_Wt_diag &lt;- c(0.1,0.06,0.06) Wt &lt;- diag(x = sqrt_Wt_diag^2) tau &lt;- 1 sigma &lt;- 2 coords_sf &lt;- st_as_sf(coords, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) |&gt; st_set_crs(4326) obs_dist_mat &lt;- st_distance(coords_sf) obs_dist_mat &lt;- units::set_units(obs_dist_mat, km) obs_dist_mat &lt;- units::set_units(obs_dist_mat, NULL) obs_max_dist &lt;- max(obs_dist_mat) obs_max_dist obs_med_dist &lt;- median(obs_dist_mat) obs_med_dist phi &lt;- obs_med_dist / 6 phi Vt &lt;- (sigma ^ 2) * exp(-obs_dist_mat / phi) + diag(x = tau ^ 2, nrow = n, ncol = n) hyperPars &lt;- nleqslv( c(2, 1), hyperParameterInvGamma, lower = 10, upper = obs_med_dist, prob = 0.98 ) prior_phi &lt;- 1 / rgamma(n = 1000, shape = hyperPars$x[1], rate = hyperPars$x[2]) const_list &lt;- list( n = n, Tt = Tt, p = p, m0 = m0, C0 = C0, aPhi = hyperPars$x[1], bPhi = hyperPars$x[2] ) dat_list &lt;- list( yt = o3, Ft = Ft, Gt = Gt, obs_dist_mat = obs_dist_mat ) init_list &lt;- list( tau = 0.1, sigma = 1, sqrt_Wt_diag = rep(0.01, p), phi_inv = 6 / obs_max_dist ) Rmodel &lt;- nimbleModel( Example12_O3_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$calculate() # -4862795 Rmodel$initializeInfo() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sigma&quot;, &quot;phi&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;)) conf$removeSampler(target = &quot;sqrt_Wt_diag[1]&quot;) conf$addSampler( target = &quot;sqrt_Wt_diag[1]&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$removeSampler(target = &quot;sqrt_Wt_diag[2]&quot;) conf$addSampler( target = &quot;sqrt_Wt_diag[2]&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$removeSampler(target = &quot;sqrt_Wt_diag[3]&quot;) conf$addSampler( target = &quot;sqrt_Wt_diag[3]&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$removeSampler(target = &quot;phi_inv&quot;) conf$addSampler( target = &quot;phi_inv&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$removeSampler(target = &quot;sigma&quot;) conf$addSampler( target = &quot;sigma&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$removeSampler(target = &quot;tau&quot;) conf$addSampler( target = &quot;tau&quot;, type = &quot;RW&quot;, control = list( log = TRUE, adaptive = TRUE, adaptInterval = 100, adaptFactorExponent = 0.8, scale = 0.5 ) , silent = TRUE ) conf$printSamplers(byType = TRUE) conf$printSamplers(executionOrder = TRUE) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = FALSE ) niter &lt;- 10000 nburnin &lt;- 0.5 * niter nthin &lt;- 1 start_time &lt;- Sys.time() post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = 2, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time library(coda) post_summary &lt;- nimSummary(post_samples) post_summary[c(&#39;tau&#39;,&#39;sigma&#39;,&#39;phi&#39;,&#39;sqrt_Wt_diag[1]&#39;,&#39;sqrt_Wt_diag[2]&#39;,&#39;sqrt_Wt_diag[3]&#39;),] library(tidyverse) library(magrittr) library(tidybayes) tidy_post_samples &lt;- post_samples %&gt;% tidy_draws() tidy_post_samples tidy_post_samples %&gt;% select(.chain,.iteration,.draw,tau,sigma,phi,starts_with(&quot;sqrt_Wt_diag&quot;)) %&gt;% gather(vars,value,-.chain,-.iteration,-.draw) %&gt;% ggplot(aes(x=.iteration,y=value)) + geom_path(aes(color = factor(.chain)), size = 0.25, show.legend = FALSE) + facet_wrap(~vars, nrow = 2, scales = &quot;free_y&quot;) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) ggsave(filename = &quot;./figures/traceplot_integrated_model_ukO3Data55Sites.png&quot;, height = 6, width = 12) tidy_post_samples %&gt;% select(.chain, .iteration, .draw, starts_with(&quot;theta[1, 1]&quot;), starts_with(&quot;theta[1, 2]&quot;), starts_with(&quot;theta[1, 3]&quot;), starts_with(&quot;theta[1, 4]&quot;)) %&gt;% gather(vars,value,-.chain,-.iteration,-.draw) %&gt;% ggplot(aes(x=.iteration,y=value)) + geom_path(aes(color = factor(.chain)), size = 0.25, show.legend = FALSE) + facet_wrap(~vars, scales = &quot;free_y&quot;) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) tidy_post_samples %&gt;% select(.chain, .iteration, .draw, starts_with(&quot;theta[2, 1]&quot;), starts_with(&quot;theta[2, 2]&quot;), starts_with(&quot;theta[2, 3]&quot;), starts_with(&quot;theta[2, 4]&quot;)) %&gt;% gather(vars,value,-.chain,-.iteration,-.draw) %&gt;% ggplot(aes(x=.iteration,y=value)) + geom_path(aes(color = factor(.chain)), size = 0.25, show.legend = FALSE) + facet_wrap(~vars, scales = &quot;free_y&quot;) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) tidy_post_samples %&gt;% select(.chain, .iteration, .draw, starts_with(&quot;theta[3, 1]&quot;), starts_with(&quot;theta[3, 2]&quot;), starts_with(&quot;theta[3, 3]&quot;), starts_with(&quot;theta[3, 4]&quot;)) %&gt;% gather(vars,value,-.chain,-.iteration,-.draw) %&gt;% ggplot(aes(x=.iteration,y=value)) + geom_path(aes(color = factor(.chain)), size = 0.25, show.legend = FALSE) + facet_wrap(~vars, scales = &quot;free_y&quot;) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) post_sum_theta &lt;- as.data.frame(post_summary) %&gt;% rownames_to_column() %&gt;% filter(str_detect(rowname, &quot;theta&quot;)) %&gt;% separate(rowname, into=c(&quot;x1&quot;,&quot;x2&quot;), sep = &quot;,&quot;) %&gt;% mutate(component = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x1))) %&gt;% mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x2))) %&gt;% select(component,time,q2.5,q50,q97.5) str(post_sum_theta) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5,ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7)+ geom_path(aes(y = q50), col = &quot;blue&quot;, size = 0.4) + facet_wrap(~component, nrow = 1, scales = &quot;free&quot;, labeller = label_bquote(theta[.(component)])) + ylab(&quot;&quot;) + xlab(&quot;Time&quot;) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) ggsave(filename = &quot;./figures/states_integrated_model_ukO3Data55Sites.png&quot;, height = 3, width = 12) "],["causality.html", "Chapter 13 Causality-roadblocks on the way to it Solutions to Selected Exercises", " Chapter 13 Causality-roadblocks on the way to it This chapter contains a discussion of the differences between causality and association. It also covers specific issues that may be encountered in this area when investigating the effects of environmental hazards on health. From this chapter, the reader will have gained an understanding of the following topics: Issues with causality in observational studies. The Bradford–Hill criteria which are a group of minimal conditions necessary to provide adequate evidence of a causal relationship. Ecological bias which may occur when inferences about the nature of individuals are made using aggregated data. The role of exposure variability in determining the extent of ecological bias. Approaches to acknowledging ecological bias in ecological studies. Concentration and exposure response functions. Models for estimating personal exposures including micro-environments. Solutions to Selected Exercises Question 13.16: Load the Italy and China data, described in Example 13.3, and included in the supplementary Bookdown material into R. The data for this exercise follows that of Exercise 13.3, we can load it as follows. library(tidyverse) library(janitor) vonK_Data &lt;- read_csv(&quot;data/vonK_China_Italy.csv&quot;) # Aggregate these datasets to have just two age ranges---$&lt;70$ and $\\geq 70$. vonK_Data_70 &lt;- mutate( vonK_Data, .keep = &quot;unused&quot;, Age_Range = fct_collapse( `Age group`, Over_70 = c(&quot;70-79&quot;, &quot;80+&quot;), other_level = &quot;Under_70&quot; ) ) # Sum up all death data grouped by age range and country vonK_Data_70 &lt;- vonK_Data_70 |&gt; group_by(Age_Range, Country) |&gt; summarise( Fatalities = sum(`Confirmed fatalities`), Total_Cases = sum(`Confirmed cases`), .groups = &quot;drop_last&quot; ) |&gt; mutate(Survived = Total_Cases - Fatalities, .before = Total_Cases) |&gt; ## Convert from fatalities to survivals arrange(Country) |&gt; ungroup() i): Display a 2 by 2 contingency table of deaths/survivors vs. China/Italy aggregated over all age groups. ## Sum up all deaths by country vonK_Aggregated_Table &lt;- vonK_Data_70 |&gt; dplyr::select(-Age_Range) |&gt; group_by(Country) |&gt; summarise_all(.funs = c(sum)) |&gt; adorn_totals(&quot;row&quot;) Table 13.1: Aggregated Contingency Table Country Fatalities Survived Total_Cases China 1023 43649 44672 Italy 357 7669 8026 Total 1380 51318 52698 ii): Display 2 by 2 contingency tables stratified by the age groups \\(&lt;70\\) and \\(\\geq 70\\). ## Filter by age range and then sum up deaths vonK_Under70_Table &lt;- vonK_Data_70 |&gt; filter(Age_Range == &quot;Under_70&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) vonK_Over70_Table &lt;- vonK_Data_70 |&gt; filter(Age_Range == &quot;Over_70&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) Table 13.2: Under 70 Contingency Table Country Fatalities Survived Total_Cases China 503 38843 39346 Italy 41 4668 4709 Total 544 43511 44055 Table 13.3: Over 70 Contingency Table Country Fatalities Survived Total_Cases China 520 4806 5326 Italy 316 3001 3317 Total 836 7807 8643 iii): Does a Simpson’s disaggregation (SDis) empirically occur in these two tables? Following the notation of the textbook (Example 13.3), let \\(S\\) denote alive and \\(\\bar{S}\\) denote death. Let \\(T\\) and \\(\\bar{T}\\) denote China and Italy. Finally, let \\(C\\) and \\(\\bar{C}\\) denote the binarized age groups under 70 and over 70. Recall that \\[ P_1 = Pr(S|T), \\; P_2 = Pr(S|\\tilde{T}), \\; R_1 = Pr(S|\\bar{C}T), \\\\ R_2 = Pr(S|\\bar{C}\\bar{T}), \\; Q_1 = Pr(S|CT), \\; Q_2 = Pr(S|C\\bar{T}). \\] A Simpson’s disaggregation occurs when \\(P_1 &gt; P_2\\), while \\(R_1 &lt; R_2\\) and \\(Q_1 &lt; Q_2\\). We compute the empirical values denoted \\(\\hat{P}_1, \\hat{P}_2, \\hat{Q}_1, \\hat{Q}_2, \\hat{R}_1, \\hat{R}_2\\): P_hats = head(vonK_Aggregated_Table$Survived / vonK_Aggregated_Table$Total_Cases, -1) Q_hats = head(vonK_Under70_Table$Survived / vonK_Under70_Table$Total_Cases, -1) R_hats = head(vonK_Over70_Table$Survived / vonK_Over70_Table$Total_Cases, -1) Solution &lt;- tibble( index = c(1, 2), P_hat = P_hats, Q_hat = Q_hats, R_hat = R_hats ) knitr::kable(Solution) index P_hat Q_hat R_hat 1 0.9770997 0.9872160 0.9023658 2 0.9555196 0.9912933 0.9047332 Indeed, we see that \\(\\hat{P}_1 = 0.977 &gt; \\hat{P}_2 = 0.956\\), while \\(\\hat{Q}_1 = 0.987 &lt; \\hat{Q}_2 = 0.991\\) and \\(\\hat{R}_1 = 0.902 &lt; \\hat{R}_2 = 0.905\\), hence a Simpson’s disaggregation has occured. Question 13.17: Using the Italy and China data stratified by the age groups \\(&lt;70\\) and \\(\\geq 70\\), obtain empirical estimates of \\(P_1\\) and \\(P_2\\), as defined in an SDis. In the following questions, assume that \\(P_1\\) and \\(P_2\\) are the population values. i): What are the possible ranges of \\(\\gamma_1\\) and \\(\\gamma_2\\) for a controlled SDis to exist (can you visualize it in the rectangle \\([0,1]^2\\))? Keep all intermediate computations to three decimal precision. We compute \\(R\\) as defined in Theorem 2, with \\(P_i = \\hat{P}_i\\). \\[ R = \\frac{P_2/(1-P_2)}{P_1/(1-P_1)} = \\frac{0.956/0.044}{0.977/0.023} = 0.511. \\] The allowable region defined by Theorem 2 is hence: \\[ \\Gamma &lt; 0.511 \\implies \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] One can plug the above inequality into freeware such as (Desmos)[https://www.desmos.com/] or (Wolfram-Alpha)[https://www.wolframalpha.com/] to visualize the region. ii): Suppose a centered interval for the \\(\\gamma\\)’s are desired, i.e., \\(\\gamma_1 = 1 - \\gamma_2\\). What are the allowable values of \\(\\gamma_1\\) and \\(\\gamma_2\\)? Furthermore, what choices (considering three decimal points) yield the tightest interval for \\(\\beta_i\\)? We can re-write the above by substituting \\(\\gamma_1 = 1 - \\gamma_2\\) as \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{(1-\\gamma_2)(1-(1-\\gamma_2))} = \\left(\\frac{\\gamma_2}{1-\\gamma_2}\\right)^2 &lt; 0.511, \\] which, taking only the positive solution of the square root, yields the allowable interval for \\(\\gamma_2\\) \\[ \\gamma_2 &lt; 0.417, \\] implying that \\(\\gamma_1 &gt; 0.583\\). Being conservative up to three decimal points, the smallest interval is hence \\([0.416, 0.584]\\). iii): Compute empirical estimates of \\(\\beta_1\\) and \\(\\beta_2\\), and interpret these values. Recall that \\(\\beta_1 = Pr(T|\\bar{C}), \\beta_2 = Pr(T|C)\\). In our context, an empirical estimate of \\(\\beta_1\\) is hence the proportion of Chinese patients over 70, and \\(\\beta_2\\) the proportion of Chinese patients under 70. beta_hat_1 &lt;- vonK_Over70_Table$Total_Cases[1] / vonK_Over70_Table$Total_Cases[3] beta_hat_2 &lt;- vonK_Under70_Table$Total_Cases[2] / vonK_Over70_Table$Total_Cases[3] beta_hats &lt;- c(beta_hat_1, beta_hat_2) Solution &lt;- Solution |&gt; add_column(beta_hat = beta_hats) knitr::kable(Solution) index P_hat Q_hat R_hat beta_hat 1 0.9770997 0.9872160 0.9023658 0.6162212 2 0.9555196 0.9912933 0.9047332 0.5448340 We obtain \\(\\hat{\\beta}_1 = 0.616\\) and \\(\\hat{\\beta}_2 = 0.545\\). iv) (Bonus): What is the tightest centered interval at \\(\\frac{1}{2}(\\hat{\\beta}_1 - \\hat{\\beta}_2)\\) for \\(\\gamma_1\\) and \\(\\gamma_2\\) such that a controlled SDis can theoretically exist (assuming population parameters)? How well does it cover the estimated \\(\\hat{\\beta}_i\\) (if at all)? Was the empirically observed SDis surprising given these results? We compute the average: mean(c(beta_hat_1, beta_hat_2)) ## [1] 0.5805276 We wish to have the tightest interval centered at \\(0.581\\) such that \\(\\gamma_1, \\gamma_2\\) satisfy \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] We can re-write this as follows. The desired interval is \\([0.581-\\gamma, 0.581+\\gamma]\\), setting \\(\\gamma_2 = 0.581-\\gamma, \\gamma_1 = 0.581+\\gamma\\). Using numerical methods accessed via (Wolfram-Alpha)[https://www.wolframalpha.com/], we obtain the inequality \\[ \\gamma &gt; 0.081 \\] This indicates that \\(\\gamma = 0.080\\) is a boundary value (at 3 decimal points), and hence the tightest interval is \\([0.501, 0.661]\\), which well-covers the estimated \\(\\hat{\\beta}\\). This indicates that perhaps the empirical existence of an SDis is not so surprising. Question 13.18: Stratify the Italy and China data by the age groups \\(&lt;50\\) and \\(\\geq 50\\). Display the corresponding contingency tables. Re-loading the data and re-stratifying: vonK_Data_50 &lt;- mutate( vonK_Data, .keep = &quot;unused&quot;, Age_Range = fct_collapse( `Age group`, Over_50 = c(&quot;50-59&quot;, &quot;60-69&quot;, &quot;70-79&quot;, &quot;80+&quot;), other_level = &quot;Under_50&quot; ) ) i): Does an SDis still occur? Why or why not? We reproduce the contingency tables of Exercise 13.16. vonK_Data_50 &lt;- vonK_Data_50 |&gt; group_by(Age_Range, Country) |&gt; summarise( Fatalities = sum(`Confirmed fatalities`), Total_Cases = sum(`Confirmed cases`), .groups = &quot;drop_last&quot; ) |&gt; mutate(Survived = Total_Cases - Fatalities, .before = Total_Cases) |&gt; arrange(Country) |&gt; ungroup() vonK_Aggregated_Table &lt;- vonK_Data_50 |&gt; dplyr::select(-Age_Range) |&gt; group_by(Country) |&gt; summarise_all(.funs = c(sum)) |&gt; adorn_totals(&quot;row&quot;) vonK_Under50_Table &lt;- vonK_Data_50 |&gt; filter(Age_Range == &quot;Under_50&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) vonK_Over50_Table &lt;- vonK_Data_50 |&gt; filter(Age_Range == &quot;Over_50&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) Displaying the tables: Table 13.4: Aggregated Contingency Table Country Fatalities Survived Total_Cases China 1023 43649 44672 Italy 357 7669 8026 Total 1380 51318 52698 Table 13.5: Under 50 Contingency Table Country Fatalities Survived Total_Cases China 64 20691 20755 Italy 1 1784 1785 Total 65 22475 22540 Table 13.6: Over 50 Contingency Table Country Fatalities Survived Total_Cases China 959 22958 23917 Italy 356 5885 6241 Total 1315 28843 30158 Now, checking if an SDis still occurs. P_hats &lt;- head(vonK_Aggregated_Table$Survived / vonK_Aggregated_Table$Total_Cases, -1) Q_hats &lt;- head(vonK_Under50_Table$Survived / vonK_Under50_Table$Total_Cases, -1) R_hats &lt;- head(vonK_Over50_Table$Survived / vonK_Over50_Table$Total_Cases, -1) Solution &lt;- tibble( index = c(1, 2), P_hat = P_hats, Q_hat = Q_hats, R_hat = R_hats ) knitr::kable(Solution) index P_hat Q_hat R_hat 1 0.9770997 0.9969164 0.9599030 2 0.9555196 0.9994398 0.9429579 An SDis no longer occurs. \\(\\hat{P}_1 = 0.977 &gt; \\hat{P}_2 = 0.956\\), but \\(\\hat{R}_1 = 0.960 &gt; \\hat{R}_2 = 0.943\\) also. Probabilistically: different definitions of the random variables involved change the probabilities (empirical or population), and an SDis is not at all guaranteed to appear, even under the same dataset. Conceptually: see Figure 1 in Von Kügelen et al. Although each 10-year range results in a higher CFR for China, the aggregated data shows a higher CFR for Italy. This is exactly Simpson’s paradox. The same aggregation phenomenon happens in our binarization, resulting in a smaller version of Simpson’s paradox. By aggregating the bins 50-80+, we have produced an estimate where the Italian CFR is higher than the Chinese, resulting in a similar bin to the “Total” one given in Figure 1. ii): Compute \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\), and repeat the bonus question of Exercise 13.17. What has changed? Do the changes match your observations? Note that because the aggregated data does not change from Exercise 13.17, our estimates for \\(P_1\\) and \\(P_2\\) also remain the same. Therefore, the allowable region for \\(\\gamma_i\\) does not change: \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] We first compute \\(\\hat{\\beta_i}\\) again. beta_hat_1 &lt;- vonK_Over50_Table$Total_Cases[1] / vonK_Over50_Table$Total_Cases[3] beta_hat_2 &lt;- vonK_Under50_Table$Total_Cases[1] / vonK_Over50_Table$Total_Cases[3] beta_hats &lt;- c(beta_hat_1, beta_hat_2) print(beta_hats) ## [1] 0.7930566 0.6882088 We obtain \\(\\hat{\\beta}_1 = 0.793\\), while \\(\\hat{\\beta}_2 = 0.688\\). We compute their mid-point to be \\(0.741\\). Hence, we look for an interval of the form \\([0.741 - \\gamma, 0.741 + \\gamma]\\). Repeating the exercise, we obtain \\[ \\gamma &gt; 0.063. \\] Taking \\(\\gamma = 0.064\\), this results in the interval \\([0.677, 0.805]\\). our estimated \\(\\hat{\\beta}_i\\) does still fall in this interval, but it is near the boundary values. This perhaps suggests that an SDis was less likely to occur in this stratification (and indeed, it does not occur empirically). "],["monitoring.html", "Chapter 14 Better monitoring network design-getting better measurements", " Chapter 14 Better monitoring network design-getting better measurements This chapter looks at the emergence of a central purpose; to explore or reduce uncertainty about aspects of the environmental processes of interest. One form of uncertainty, aleatory, cannot be reduced by definition whereas with the other, epistemic, where uncertainty can be reduced (see Chapter 3). However that reduction does not stop the original network from becoming sub-optimal over time, pointing to the need to regularly reassess its performance. From that perspective we see that the design criteria must allow for the possibility of ‘gauging’ (adding monitors to) sites that Maximally reduce uncertainty at their space–time points (measuring their responses eliminates their uncertainty); Best minimise uncertainty at other locations; Best inform about process parameters; Best detect non-compliers. From this chapter, the reader will have gained an understanding of many of the challenges that the network designer may face. These involve the following topics: A multiplicity of valid design objectives. Unforeseen and changing objectives. A multiplicity of responses at each site, i.e. which should be monitored. A need to use prior knowledge and to characterise prior uncertainty. A need to formulate realistic process models. A requirement to take advantage of, and integrate with, existing networks. The need to be realistic, meaning to contend with economic as well as administrative demands and constraints. "],["frontiers.html", "Chapter 15 Current frontiers", " Chapter 15 Current frontiers In this chapter the reader will have encountered a selection of new frontiers in spatio– temporal epidemiology including the following: A number of areas that are currently under active development. Two modern approaches to addressing the problem of non-stationarity in random spatio– temporal fields; warping and dimension expansion. How dimension expansion can be used to dramatically reduce non-stationarity and suggest its possible causes. A powerful approach combining both physical and statistical modelling within a single framework. "],["course.html", "Course Course description: Spatio-temporal methods in environmental epidemiology Course lectures", " Course Course description: Spatio-temporal methods in environmental epidemiology Description: This course will emphasize the spatio-temporal methods used in environmental epidemiology to characterize the distribution of morbidity and mortality over space and time. The course relies mainly on the first edition that was published after the course was given, as it lays the groundwork for the second edition. Some of the methods that are described in the course, involve a single area and look at variations in the health responses over time in relation to those of predictors, including environmental hazards. Some involve a single time period in which case, it’s the variations over space that can help identify relationships between these responses and possible explanatory variables. However the scale of modern investigation involves means those relationships over time and space must be both be handled leading to the need for sophisticated methods whose development is an active area of risk in statistical science. The course does assume a solid background in statistics although the required material will be briefly reviewed. These include but are not limited to: likelihood methods; linear and logistic regression; Bayesian methods; the generalized linear model. Other topics will be developed from first principles including: Poisson regression; quasi likelihood. The course will not assume a background in epidemiology however. Thus the relevant building blocks from that subject will be described including all necessary definitions and terminology. In particular, we will give an overview of: types of study; rates and standardization; measures of risk; the role of confounders and the problem of causation. Statistics and epidemiology come together when we address important special topics including, disease mapping, ecological analysis, disease clustering, infectious disease modelling, depending on the interests of those enrolled in the course and time available. Labs and projects for the course are being included in the GitHub site for Reference 2 https://spacetime-environ.github.io/stepi2 References: Le, N.D. and Zidek, J.V. Statistics Analysis of Environmental Space-Time Processes. New York: Springer. (2006) Shaddick, G. and Zidek, J.V. Spatio-temporal methods for environmental epidemiology. London: Chapman and Hall/CRC (2015) Shaddick, G. and Zidek, J.V and Schmidt, A., Spatio–Temporal Methods in Environmental Epidemiology with R: Second Edition (2023) Sudipto Banerjee, Alan E. Gelfand and Bradley P. Carlin (2005). Hierarchical Modeling and Analysis for Spatial Data. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Noel Cressie and Christopher K Wikle (2011). Statistics for spatio-temporal data. Wiley Series in Probability and Statistics. Peter J. Diggle (2002). Statistical analysis of spatial point patterns, Second Edition. Hodder Education Peter J. Diggle and Paulo J. Ribeiro Jr. (2007). Model-based geostatistics. Springer series in Statistics. Andrew B. Lawson (2006). Statistical Methods in Spatial Epidemiology, Second Edition. Wiley Series in Probability and Statistics. Nhu D. Le and James V. Zidek (2006). Statistical Analysis of Environmental Space-Time Processes. Springer Series in Statistics Duncan Thomas (2009). Statistical Methods in Environmental Epidemiology. Oxford University Press Course lectures The following shows some suggested course material for instructors. These are based on the first edition of the book but might be still suitable for anyone teaching a course on Environmental epidemiology. Lecture 1: Introduction Lecture 2: Spatial epidemiology Lecture 3 and 4: Disease Mapping Lecture 5 and 6: Diseases spatial-patterns Lecture 7 and 8: Spatial models Lecture 9 and 11: Spatio and temporal processes Lecture 12: Spatio-temporal processes Lecture 13 and 14: Spatio-temporal processes cont. Lecture 15: Introduction to Bayesian Analysis and WinBUGS Lecture 16: Bayesian Methods Lecture 17: Why is computation important? Lecture 18: Generalised Linear regression Models Lecture 19: Time series epidemiology Lecture 20: Spatio-temporal modelling of exposures Lecture 21: Environmental health risk assessment Lecture 22: Measurement error Lecture 23: Designing good monitoring networks Lecture 24: Dealing with high dimensional data "],["suggested-projects.html", "Suggested projects Project 1 Project 2", " Suggested projects Project 1 Introduction For this exercise you should perform an analysis of the spatial distribution of lung cancer in Ohio between 1968-1988. For each county in Ohio the number of lung cancer deaths and people at risk are given conditioned on county, year, sex, age category and race. You are to perform both non-spatial and spatial analyses of the data and to compare the results between the different approaches. You should consider the best of dealing with data over multiple years. You should pay special consideration to how the results are most meaningfully presented, both in terms of visual representation and in choosing appropriate methods of summarizing risk. The Problem The following is a list of pointers to consider: You should clearly state a non-spatial model that is suitable for this analysis and fit it in R. You should describe clearly the results and present them in an informative way. Explain any potential deficiencies in adopting this approach. Explain why a spatial smoothing approach might be more applicable in this case. Define clearly how an empirical Bayes approach which might be used in this case, explaining clearly what it adds to the analyses. Fit your spatial model using R and present the resulting smoothed relative risks in a meaningful way. Investigate the effects of the negative binomial dispersion parameter on the smoothed risks you obtain. Consider alternative outputs which might be useful, in addition to relative risks, and write a report of your findings collating the information gained from all of your analyses. The OhioMap function on the course webpage can be used as a basis for presenting the results of your analyses. Solution Project 1 Project 2 Introduction This exercise is about visualization, important for spatial statistics. R provides a variety of options for plotting data on maps. An individual has described a selection of these in the project folder called examples. A vignette can also be found for one of these at http://cran.r-project.org/web/packages/plotGoogleMaps/ vignettes/plotGoogleMaps-intro.pdf for one of these packages. The data for this part of the exercise were produced by the MURI group at the U of Washington (www.probcast.washington.edu) and provide amongst other things temperature (degrees Kelvin) for a large number of sites in the Pacific NW (with spatial coordinates). Consider the one labelled phase1.temp.txt, that gives (for GMT) temperatures (in the “obs” column) for the period January 12, 2000 to June 30, 2000. The data are incomplete since not all sites collect data on the same day. We will focus on Apr 1, 2000 and stations located in Oregon State. You can use Google Earth (setting spatial coordinates to be given in decimal form) to determine coordinates of sites that lie in Oregon. Those coordinates can also be used to construct a bounding box for Oregon. The Problem Using a package of your choice, plot the points for sites active on Apr 1 on a map of Oregon. Create a regular grid of spatial points that cover Oregon. Using a method of your choice, predict for Apr 1, values of temperature at the points of intersection in your lattice. Construct a contour plot of temperature in Oregon on Apr 1, 2000. Add any other informative features to your plot that you deem useful. Solution Solution Project 2 "],["references.html", "References", " References print(sessionInfo()) ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 22631) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.utf8 LC_CTYPE=English_Canada.utf8 ## [3] LC_MONETARY=English_Canada.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.utf8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] janitor_2.1.0 rjags_4-13 gridExtra_2.3 ## [4] nleqslv_3.3.3 forecast_8.19 TTR_0.24.3 ## [7] magrittr_2.0.3 coda_0.19-4 cowplot_1.1.1 ## [10] viridis_0.6.2 viridisLite_0.4.1 tidybayes_3.0.2 ## [13] gdata_2.18.0.1 gstat_2.1-0 loo_2.5.1 ## [16] SpatialEpi_1.2.8 reshape2_1.4.4 boot_1.3-28 ## [19] mvtnorm_1.1-3 INLA_22.12.16 foreach_1.5.2 ## [22] Matrix_1.5-1 leaflet_2.1.1 GGally_2.1.2 ## [25] geoR_1.9-2 rstan_2.26.13 StanHeaders_2.26.13 ## [28] nimble_0.12.2 rworldmap_1.3-6 raster_3.6-14 ## [31] RColorBrewer_1.1-3 rgdal_1.6-2 CARBayes_5.3 ## [34] Rcpp_1.0.9 MASS_7.3-58.1 shapefiles_0.7.2 ## [37] foreign_0.8-83 spdep_1.2-7 sf_1.0-9 ## [40] spData_2.2.0 sp_1.5-0 forcats_0.5.2 ## [43] stringr_1.5.0 dplyr_1.0.10 purrr_0.3.5 ## [46] readr_2.1.3 tidyr_1.2.1 tibble_3.1.8 ## [49] ggplot2_3.4.1 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] utf8_1.2.2 tidyselect_1.2.0 htmlwidgets_1.5.4 ## [4] grid_4.2.2 maptools_1.1-8 munsell_0.5.0 ## [7] codetools_0.2-18 units_0.8-0 withr_2.5.0 ## [10] colorspace_2.0-3 highr_0.9 knitr_1.40 ## [13] rstudioapi_0.14 stats4_4.2.2 wk_0.7.0 ## [16] labeling_0.4.2 splancs_2.01-43 mnormt_2.1.1 ## [19] MCMCpack_1.6-3 bit64_4.0.5 farver_2.1.1 ## [22] vctrs_0.5.0 generics_0.1.3 xfun_0.34 ## [25] R6_2.5.1 fields_14.1 cachem_1.0.6 ## [28] reshape_0.8.9 assertthat_0.2.1 vroom_1.6.0 ## [31] scales_1.2.1 nnet_7.3-18 googlesheets4_1.0.1 ## [34] gtable_0.3.1 processx_3.8.0 mcmc_0.9-7 ## [37] spam_2.9-1 timeDate_4021.107 rlang_1.0.6 ## [40] MatrixModels_0.5-1 splines_4.2.2 gargle_1.2.1 ## [43] broom_1.0.1 checkmate_2.1.0 inline_0.3.19 ## [46] abind_1.4-5 s2_1.1.0 yaml_2.3.6 ## [49] modelr_0.1.9 crosstalk_1.2.0 backports_1.4.1 ## [52] quantmod_0.4.20 CARBayesdata_3.0 tensorA_0.36.2 ## [55] tools_4.2.2 tcltk_4.2.2 bookdown_0.29 ## [58] ellipsis_0.3.2 posterior_1.3.1 jquerylib_0.1.4 ## [61] proxy_0.4-27 plyr_1.8.7 classInt_0.4-8 ## [64] ps_1.7.2 prettyunits_1.1.1 deldir_1.0-6 ## [67] fracdiff_1.5-2 zoo_1.8-11 haven_2.5.1 ## [70] fs_1.5.2 ggdist_3.2.0 spacetime_1.2-8 ## [73] SparseM_1.81 lmtest_0.9-40 reprex_2.0.2 ## [76] googledrive_2.0.0 truncnorm_1.0-8 matrixStats_0.62.0 ## [79] hms_1.1.2 evaluate_0.17 arrayhelpers_1.1-0 ## [82] readxl_1.4.1 compiler_4.2.2 maps_3.4.1 ## [85] KernSmooth_2.23-20 V8_4.2.1 crayon_1.5.2 ## [88] htmltools_0.5.3 tzdb_0.3.0 RcppParallel_5.1.5 ## [91] lubridate_1.8.0 DBI_1.1.3 dbplyr_2.2.1 ## [94] cli_3.4.1 quadprog_1.5-8 dotCall64_1.0-2 ## [97] igraph_1.3.5 pkgconfig_2.0.3 sn_2.1.0 ## [100] numDeriv_2016.8-1.1 terra_1.7-3 xml2_1.3.3 ## [103] svUnit_1.0.6 bslib_0.4.0 rvest_1.0.3 ## [106] snakecase_0.11.0 distributional_0.3.1 callr_3.7.3 ## [109] digest_0.6.30 rmarkdown_2.17 cellranger_1.1.0 ## [112] intervals_0.15.2 curl_4.3.3 urca_1.3-3 ## [115] gtools_3.9.3 quantreg_5.94 nlme_3.1-160 ## [118] tseries_0.10-52 lifecycle_1.0.3 jsonlite_1.8.3 ## [121] fansi_1.0.3 pillar_1.9.0 lattice_0.20-45 ## [124] fastmap_1.1.0 httr_1.4.4 pkgbuild_1.3.1 ## [127] survival_3.4-0 glue_1.6.2 xts_0.12.2 ## [130] FNN_1.1.3.1 iterators_1.0.14 leaflet.providers_1.9.0 ## [133] bit_4.0.4 class_7.3-20 stringi_1.7.8 ## [136] sass_0.4.2 e1071_1.7-12 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
