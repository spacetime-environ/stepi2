# Disease-spatial patterns {#Disease}

This chapter contains an introduction to different types of spatial data, the theory of spatial lattice processes and introduces disease mapping and models for performing smoothing of risks over space. The  reader will have gained an understanding of the following topics:

  disease mapping and how to improve estimates of risk by borrowing strength from adjacent regions which can reduce the instability inherent in risk estimates based on small (expected) numbers;

- how smoothing can be performed using either the empirical Bayes  or fully Bayesian approaches;
- computational methods for handling areal data;
- Besag's seminal contributions to the field of spatial statistics including the very important concept  of a Markov random field;
- approaches to modelling  areal data including conditional autoregressive models;
-  how  Bayesian spatial models for lattice data can be fit using NIMBLE, RStan and R--INLA.
    
    
## Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 {-}

Here we consider hospitalization for a respiratory condition, chronic obstructive pulmonary disease (COPD), in England in 2010. There are $N_l=324$ local authority administrative areas  each with an observed, $Y_l$ and expected, $E_l$,  number of cases, $l=1,...,324$. As described in 
Section 2.4 the expected numbers were calculated using indirect standardization by applying the 
age--sex specific rates for the whole of England to the age--sex population profile of each of the areas.  In order to perform empirical Bayes smoothing, we will use the `eBayes` function in the `SpatialEpi` package. 
This example uses the [englandlocalauthority.shp](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/englandlocalauthority.shp), [copdmortalityobserved.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityobserved.csv), and
[copdmortalityexpected.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityexpected.csv) datasets.


### Empirical Bayes estimate using `SpatialEpi`

```{r Ex 9.1 SpatialEpi, message=FALSE, echo=TRUE, warning=FALSE,message=FALSE}

# requires SpatialEpi package
library (SpatialEpi)
library(sf) # package used to read shapefiles
# Laoding the borders of England
england <- read_sf("data/copd/englandlocalauthority.shp")
# Loading the data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv", row.names = 1)
#observed data
Y <- observed$Y2010
# offset
E <- expected$E2010

RRs = eBayes (Y , E )
plot ( RRs $ SMR , RRs $ RR , xlim = c (0 ,2) , ylim = c (0 ,2) , xlab = "
SMRs " , ylab = " RRs " )
abline ( a =0 , b =1 , col = " red " , lwd =3)

```


The smoothing can be seen with lower and higher SMRs being brought close to the overall average, $\mu$ which in this case is $\exp(-0.0309) =  0.9696$. Note that in this example, the areas are relatively large and would be expected to have substantial populations, so the effect of the smoothing is  limited. In this example, $\alpha$ was estimated to be 14.6.   For a fully Bayesian, rather than empirical Bayes, analysis we can use NIMBLE; and this is what we present next.

### Nimble {-}

<!--- ```{r Ex 9.1 nimble load, echo=TRUE, message=FALSE, warning=FALSE}

# Load nimble
library(nimble)
library(sf) # package used to read shapefiles
```
--->


```{r Ex 9.1 nimble model, results='hide', warning=FALSE, message= FALSE, cache = TRUE}
library(nimble)


Example9_1Nimble <- nimbleCode({
  for (i in 1:N) {
    Y[i] ~ dpois(mu[i])
    # REVIEW: There is an intercept in the book, 
    # but then we wouldn't be able to compare it to example 5.2
    mu[i] <- E[i]*exp(beta0)* theta[i]
    #mu[i] <- E[i]* theta[i]
    # REVIEW: Same as before, the example in the book has theta[i] ~ Ga(a,a)
    theta[i] ~ dgamma(a, a)
    Y.fit[i] ~ dpois(mu[i])
  }
  # Priors
  a ~ dgamma(1,1)
  beta0 ~ dnorm(0, 10)

})

# Define the constants, data and initials lists for the `nimble` model.

# observations
y <- observed$Y2010
# offset
E <- expected$E2010
N <- length(y)
# constants list
constants <- list(
  N = N,
  E = E
)
# data list
ex.data <- list(Y = y)
# initial values list
inits <-
    list(
      theta = rgamma(N, 1, 1),
      a = rgamma(1, 1),
      beta0 = 0,
      Y.fit = rpois(N, E)
    )
# parameters to monitor
params <- c("theta", "a", "beta0", "Y.fit")

# Run model in nimble
mcmc.out <- nimbleMCMC(
  code = Example9_1Nimble,
  constants = constants,
  data = ex.data,
  inits = inits,
  monitors = params,
  niter = 50000,
  nburnin = 20000,
  thin = 14,
  WAIC = TRUE,
  nchains = 2,
  summary = TRUE,
  samplesAsCodaMCMC = TRUE
)

```



Effective sample size is checked through the minimum value among all the samples available. This is to check if this minimum is acceptable. We also provide trace plots of the chains for some of the parameters. We provide the command to obtain WAIC in case model comparison will be performed in a later stage. <!---Show the WAIC, effective sample size, and trace plots for some of the parameters.--->

```{r Ex 9.1 nimble WAIC, ESS and traceplots}


min(coda::effectiveSize(mcmc.out$samples))
mcmc.out$WAIC

#storing the samples from the posterior distribution in mvSamples
mvSamples <- mcmc.out$samples

# trace plot of a
plot(mvSamples[, c("a")], bty = "n")
# trace plot of beta0
plot(mvSamples[, c("beta0")], bty = "n")
# trace plots of theta
for (i in 1:3)
  plot(mvSamples[, c(paste("theta[", i, "]", sep = ""))], bty = "n")

```

Now that we have checked the convergence of the chains we can plot the posterior mean and 95% CIs for each of the parameters.


```{r Ex 9.1 nimble posterior summary}
# Print posterior summary for parameters a and b
summary(mvSamples[, c("a", "beta0")])
# posterior summaries of theta_i
post_summary <- mcmc.out$summary$all.chains |> as.data.frame() |>
  tibble::rownames_to_column("variable")
# plot the mean and 95% CIs for the thetas
post_theta <-
  post_summary[grepl("theta\\[", post_summary$variable),]

par(mfrow = c(1, 1))
plot(
  post_theta$Mean,
  pch = 19,
  cex = 0.8,
  bty = "n",
  xlab = "Borough",
  ylab = "Posterior Summary Rate",
  ylim = c(min(post_theta$`95%CI_low`), max(post_theta$`95%CI_upp`))
)
for (i in 1:N)
  segments(i, post_theta$`95%CI_low`[i], i, post_theta$`95%CI_upp`[i])
abline(h = 1, lwd = 2, lty = 2)

# posterior summary of fitted values
post_fitted <-
  post_summary[grepl("Y.fit\\[", post_summary$variable),]
# plot mean and 95% CIs for the fitted values
par(mfrow = c(1, 1))
plot(
  y,
  post_fitted$Mean,
  ylim = c(min(post_fitted$`95%CI_low`), max(post_fitted$`95%CI_upp`)),
  xlab = "Observed",
  ylab = "Fitted",
  pch = 19,
  cex = 0.7,
  bty = "n"
)
for (i in 1:N)
  segments(y[i], post_fitted$`95%CI_low`[i], y[i], post_fitted$`95%CI_upp`[i])
abline(a = 0, b = 1)

```

Now we present the code for the same model in {\tt Stan}. We start by cleaning the ´R environment`. 

### Stan {-}

```{r Ex 9.1 clean, include = FALSE}
rm(list=ls())
```

```{r Ex 9.1 stan load, message=FALSE, echo = TRUE, warning=FALSE, message= FALSE}

library(rstan)
library(sf) # to read shapefile
library(loo) # To calculate WAIC
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load data
# Reading in borders
england <- read_sf("data/copd/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv", row.names = 1)

```

The `Stan` model is in a separate file called `Example9_1.stan` that will be called later.

```{r engine='bash', comment='', echo = FALSE}
cat functions/Example9_1.stan
``` 


Similarly to `NIMBLE`, we now define the different objects that will contain information about the data. 

```{r Ex 9.1 stan define data, error = FALSE, message = FALSE, results='hide', cache=TRUE}
# observations
y <- observed$Y2010
# offset
E <- expected$E2010
N <- length(y)
# data list
ex.data <- list(
  N = length(y),
  Y = y,
  E = E
)
# Run the model in Stan
Ex9_1Stan <- stan(
  file = "functions/Example9_1.stan",
  data = ex.data,
  chains = 3,
  iter = 10000,
  warmup = 3000,
  thin = 14,
  # QUESTION: should we explain this?
  control = list(adapt_delta = 0.8, max_treedepth = 15),
  init = "random",
  pars = c("a","beta0","theta", "log_lik", "yfit"),
  include = TRUE
)

```

We now check the trace plots of some of the parameters, check the effective sample size. Once convergence has been checked we obtain the posterior summaries of some of the parameters. <!---Compute the WAIC, show the trace plots and posterior summaries of the parameters.--->

```{r Ex 9.1 stan waic and traceplots, message= FALSE, warning=FALSE}

# traceplots of parameters a and b
rstan::traceplot(Ex9_1Stan, pars = c("a","beta0"))
# traceplots of parameter theta
rstan::traceplot(Ex9_1Stan, pars = c("theta[1]", "theta[2]", "theta[3]"))
# posterior summaries together with Rhat
rstan::traceplot(Ex9_1Stan, pars = c("a","theta[1]", "theta[2]", "theta[3]"))
rstan::summary(Ex9_1Stan, pars = c("a","theta[1]", "theta[2]", "theta[3]"))

# To be able to compute the WAIC in Stan, we need to define the log_lik quantity in the generated quantities block
# Compute WAIC
loglik0 <- extract_log_lik(Ex9_1Stan)
waic0 <- waic(loglik0)
waic0


```

Note that the ´summary` function of a `RStan` object provides the effective sample size and Rhat. We now plot the posterior summaries of $\theta_i$ and each of the fitted values with their respective 95% posterior credible intervals.

```{r Ex 9.1 stan posterior summary}

summary_theta <-
  summary(Ex9_1Stan,
          pars = c("theta"),
          probs = c(0.05, 0.95))$summary |> as.data.frame()

par(mfrow = c(1, 1))
plot(
  summary_theta$mean,
  pch = 19,
  cex = 0.8,
  bty = "n",
  xlab = "Borough",
  ylab = "Posterior Summary Rate",
  ylim = c(min(summary_theta$`5%`), max(summary_theta$`95%`))
)
for (i in 1:N)
  segments(i, summary_theta$`5%`[i], i, summary_theta$`95%`[i])
abline(h = 1, lwd = 2, lty = 2)

# Posterior summary of fitted values
summary_fit <-
  summary(Ex9_1Stan,
          pars = c("yfit"),
          probs = c(0.05, 0.95))$summary |> as.data.frame()
# Plot mean and 95% CIs for the fitted values
par(mfrow = c(1, 1))
plot(
  y,
  summary_fit$mean,
  ylim = c(min(summary_fit$`5%`), max(summary_fit$`95%`)),
  xlab = "Observed",
  ylab = "Fitted",
  pch = 19,
  cex = 0.7,
  bty = "n"
)
for (i in 1:N)
  segments(y[i], summary_fit$`5%`[i], y[i], summary_fit$`95%`[i])
abline(a = 0, b = 1)
```

## Example 9.3: Fitting a conditional spatial model in `NIMBLE` and `RStan` {-}

In this example, we see how to fit the Poisson log--normal model seen in Section 9.4 with spatial random effects coming from  the ICAR model described in Section 9.3.1. 

This example uses the [englandlocalauthority.shp](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/englandlocalauthority.shp), [copdmortalityobserved.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityobserved.csv), and
[copdmortalityexpected.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityexpected.csv) datasets. 

### Nimble {-}

In this example, we see how to fit the Poisson log--normal model seen in Section 9.4 with spatial random effects coming from  the ICAR model described in Section 9.3.1.

```{r Ex 9.3 nimble clean, include = FALSE}
rm(list=ls())
```

We start by loading some packages and the data.

```{r Ex 9.3 nimble load, message=FALSE, warning=FALSE, message= FALSE}

library(ggplot2) # to plot map
library(spdep) # read the shapefile (read_sf) and build neighbors list (poly2nb)
library(nimble)

# Load data
# Reading in borders
england <- read_sf("data/copd/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copd/copdavgscore.csv")
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )

```

The following command provides a scatter plot of the average deprivation score and the estimated SMR for 2010. The plot suggests that as deprivation increases so does the SMR for 2010.

```{r Ex 9.3 plot copd and avg score}

copd_df$SMR2010 = copd_df$Y2010 / copd_df$E2010

ggplot(copd_df) + geom_point(aes(x = Average.Score, y = SMR2010)) + 
  theme_classic() + ylab("SMR 2010") + xlab("Average deprivation score")

```

To fit the ICAR model in `Nimble` we need to obtain the neighborhood matrix $W$. We start by defining a function that computes the number of neighbors for each 
area.

```{r Ex 9.3 nimble munge functions, echo = TRUE}

adjlist = function(W, N) {
  adj = 0
  for (i in 1:N) {
    for (j in 1:N) {
      if (W[i, j] == 1) {
        adj = append(adj, j)
      }
    }
  }
  adj = adj[-1]
  return(adj)
}

```

Next we obtain the adjacency matrix, W,Define the adjacency matrix and indexes for `stan` using the functions `nb2mat` and `adjlist` from package `spdep`.

```{r Ex 9.3 nimble neighborhood}

# Create the neighborhood
W.nb <-poly2nb(england, row.names = rownames(england))
# Creates a matrix for following function call
W.mat <- nb2mat(W.nb, style = "B")
# Define the spatial structure to use in stan
N <- length(unique(england$ID))
#creating a vector listing the ID numbers of the adjacent areas for each area
neigh  <- adjlist(W.mat, N)
# computing a vector of length N (the total number of areas) giving the number of neighbors for each area.
numneigh  <- apply(W.mat,2,sum)

```

```{r Ex 9.3 nimble car model, warning=FALSE, message = FALSE, results='hide'}

Example9_3Nimble <- nimbleCode({
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- log(E[i]) + b[i] + beta0 + beta1*x[i]
  }
  
  # Priors
  beta0 ~ dnorm(0, sd = 1)
  beta1 ~ dnorm(0, sd = 1)
  b[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1)
  tau <- 1 / (sigma_b ^ 2)
  #prior for sigma_b is a half-normal distribution
  sigma_b ~ T(dnorm(0, sd = 1), 0,)
  # Fitted values and likelihood for WAIC
  for (i in 1:N) {
    fitted[i] ~ dpois(lambda[i])
  }
})

#  Define the constants, data and initial values lists and run the model. 

# constants list
constants <- list(
  N = N,
  E = copd_df$E2010,
  L = length(neigh),
  adj = neigh,
  weights = rep(1, length(neigh)),
  num = as.vector(numneigh),
  p = 3
)
# data list
ex.data <-
  list(y = copd_df$Y2010, 
       x = as.vector(copd_df$Average.Score)) # vector of covariates
inits <-  list(
  beta0 = rnorm(1),
  beta1 = rnorm(1),
  fitted = rpois(N, 2),
  sigma_b = 1,
  b = rnorm(N)
)
params <- c("beta0","beta1", "fitted", "b", "tau")
# Run model in nimble
mcmc.out <- nimbleMCMC(
  code = Example9_3Nimble,
  constants = constants,
  data = ex.data,
  inits = inits,
  monitors = params,
  niter = 40000,
  nburnin = 20000,
  thin = 20,
  WAIC = TRUE,
  nchains = 2,
  summary = TRUE,
  samplesAsCodaMCMC = TRUE
)

```

Next we double-check convergence of the chains through trace plots and the effective sample sizes for some of the parameters.


```{r Ex 9.3 ESS and traceplots }

 min(coda::effectiveSize(mcmc.out$samples))
 plot(mcmc.out$samples[, c("beta0")], bty = "n", main = "beta0")
 plot(mcmc.out$samples[, c("beta1")], bty = "n")
 plot(mcmc.out$samples[, c("b[2]")], bty = "n")
 plot(mcmc.out$samples[, c("tau")], bty = "n")
```

Posterior summaries of the intercept, the fixed effect for average deprivation index and the precision of the ICAR prior distribution.

```{r Ex 9.3 nimble CAR summary, echo = TRUE}

# Extract samples
variables <- c("beta0", "beta1","tau")
summary_CAR_nimble <- mcmc.out$summary$all.chains
summary_CAR_nimble[variables,]

```

Map of the posterior mean of the latent effects.

```{r Ex 9.3 plot map nimble, eval=FALSE}

samples_CAR_b <-
  summary_CAR_nimble[grepl("b\\[", rownames(summary_CAR_nimble)), ] |> as.data.frame()
observed <- tibble::rownames_to_column(observed, "ID")
samples_CAR_b$ID <- observed$ID
CAR_nimble_merge <- merge(england, samples_CAR_b, by = "ID")
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = CAR_nimble_merge, aes(fill = Mean)) +
  # Change legend's label
  labs(fill = 'Latent effects under Nimble') +
  # Clear background and plot borders
 theme_void()
```

Now we perform the same analysis using `Stan`. Note that, different from `Nimble`, the sum-to-zero constraint in `Stan` is a soft one.

### Stan {-}


```{r Ex 9.3 stan clean, include = FALSE}
rm(list=ls())
```


```{r Ex 9.3 stan load, message=FALSE, echo = FALSE, warning=FALSE, message= FALSE}

library(ggplot2)
library(loo) # To compute WAIC and loo later
library(rstan) 
library(spdep) 

# Additional settings for Stan
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load data
# Reading in borders
england <- read_sf("data/copd/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copd/copdavgscore.csv")
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )

```

As `Stan` does not have a specific function for the ICAR prior we need to compute the neighborhood structure through the following functions which were obtained from [this site](https://mc-stan.org/users/documentation/case-studies/icar_stan.html):

```{r Ex 9.3 stan munge functions, echo = TRUE}

adjlist = function(W, N) {
  adj = 0
  for (i in 1:N) {
    for (j in 1:N) {
      if (W[i, j] == 1) {
        adj = append(adj, j)
      }
    }
  }
  adj = adj[-1]
  return(adj)
}

mungeCARdata4stan = function(adjBUGS, numBUGS) {
  N = length(numBUGS)
  nn = numBUGS
  N_edges = length(adjBUGS) / 2
  node1 = vector(mode = "numeric", length = N_edges)
  node2 = vector(mode = "numeric", length = N_edges)
  iAdj = 0
  iEdge = 0
  
  for (i in 1:N) {
    for (j in 1:nn[i]) {
      iAdj = iAdj + 1
      if (i < adjBUGS[iAdj]) {
        iEdge = iEdge + 1
        node1[iEdge] = i
        node2[iEdge] = adjBUGS[iAdj]
      }
    }
  }
  return (list(
    "N" = N,
    "N_edges" = N_edges,
    "node1" = node1,
    "node2" = node2
  ))
  
}
```



This model is in a separate file called `Example9_3.stan` that will be called later.

```{r engine='bash', comment='', echo = FALSE}
cat functions/Example9_3.stan
``` 
<!-- REVIEW: I am missing the adjacency matrix but I calculated like they did for the CARBayes example, is that right? -->

Define the adjacency matrix and the set of indices of the neighboring areas using functions `nb2mat` and `adjlist`:


```{r Ex 9.3 stan car effects, warning=FALSE, results='hide', message = FALSE, cache=TRUE}

# Create the neighborhood
W.nb <-
  poly2nb(england, row.names = rownames(england))
# Creates a matrix for following function call
W.mat <- nb2mat(W.nb, style = "B")

# Define the spatial structure to use in stan
N <- length(unique(england$ID))
neigh <- adjlist(W.mat, N)
numneigh  <-  apply(W.mat, 2, sum)
nbs <- mungeCARdata4stan(neigh, numneigh)


# Define data and variables for Stan model
y <- copd_df$Y2010
E <- copd_df$E2010
X <- as.numeric(copd_df$Average.Score)

ex.data <- list(
  N =  nbs$N,
  y = y,
  E = E,
  p = 1,
  X = as.matrix(X),
  N_edges = nbs$N_edges,
  node1 = nbs$node1,
  node2 = nbs$node2
)

Example9_3Stan  <- stan(
  file = "functions/Example9_3.stan",
  data = ex.data,
  warmup = 2000,
  iter = 10000,
  chains = 2,
  thin = 8,
  pars = c("beta0", "beta","sigma_s", "riskdep", "b", "log_lik"),
  include = TRUE
)

```


Trace plots of the posterior samples for some of the parameters.

```{r Ex 9.3 stan CAR traceplots}

#traceplots of some parameters
rstan::traceplot(Example9_3Stan, pars = c("beta0","beta","sigma_s"))

```

We now obtain the posterior summaries of the intercept, the fixed effect associated with the deprivation score and the standard deviation of the random effect. The quantity `riskdep` shows the relative risk associated with an one unit increase in deprivation.
<!---Show the posterior summary for the parameters if interest. 
Keep in mind that in the stan model the we are sampling the standard deviation of the random effect unlike `CARBayes` where the variance is obtained.--->

```{r Ex 9.3 stan CAR results, echo = TRUE}

# Extract samples
summary_CAR_stan <-
  summary(
    Example9_3Stan,
    pars = c("beta0", "beta","sigma_s","riskdep"),
    probs = c(0.025, 0.975)
  )

summary_CAR_stan$summary

```

 The result suggests that an one unit increase in deprivation, increases the relative risk by 2.2%. Recall that the standard deviation of the ICAR prior refers to the standard deviation of a conditional distribution.


We now plot the posterior means of the latent effects

```{r Ex 9.3 plot map stan}

summary_CAR_stan_b <-
  summary(
    Example9_3Stan,
    pars = c("b"),
    probs = c(0.025, 0.975)
  )

observed <- tibble::rownames_to_column(observed, "ID")

summary_CAR_stan_b$ID <- observed$ID

CAR_stan_merge <- merge(england, summary_CAR_stan_b, by = "ID") 

ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = CAR_stan_merge, aes(fill = summary.mean)) + 
  # Change legend's label
  labs(fill = 'Latent effects stan') +
  # Clear background and plot borders
  theme_void()


```

<!-- IDEA: Maybe we should agree on what output we will show for every method -->

## Example 9.4: Fitting a conditional spatial model using CARBayes {-}

Here we consider fitting the Poisson log--normal model with spatial effects to the data for respiratory admissions seen in Example 9.3 using the R package `CARBayes`. The adjacency matrix is obtained using functions of the package `spdep`.

`CARBayes` performs MCMC simulation similar to NIMBLE but has the distinct advantage that you just call specific functions to run the model.


```{r Ex 9.4 clean, include=FALSE}
rm(list=ls())
```



```{r Ex 9.4 carbayes load, message=FALSE, warning=FALSE, message= FALSE}

library(CARBayes)
library(ggplot2)
library(sf)
library(spdep)

# Reading in borders
england <- read_sf("data/copd/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copd/copdavgscore.csv") 
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )
copd_df$Average.Score <- as.numeric(scale(copd_df$Average.Score))

```


Next we compute the neighborhood matrix through `poly2nb()` and `nb2mat()` from the `spdep`.

```{r Ex 9.4 carbayes neighborhood}

# Create the neighborhood
W.nb <-poly2nb(england, row.names = rownames(england))

# Creates a matrix for following function call
W.mat <-  nb2mat(W.nb, style = "B")

```

The function `S.CARleroux()` allows us to use the neighborhood structure and performs a Bayesian analysis assuming a CAR prior distribution to the random effects. This is obtained by setting the parameter rho equals 1 in the call of `S.CARleroux()`.


```{r Ex 9.4 carbayes run, results='hide', message = FALSE, cache = TRUE}

# Running smoothing model
Ex9_4 <-
  S.CARleroux(
    # Model Formula
    formula = Y2010 ~ offset(log(E2010)) + Average.Score,
    # data frame with data
    data = copd_df,
    # Choosing Poisson Regression
    family = "poisson",
    # Neighborhood matrix
    W = W.mat,
    # Number of burn in samples
    burnin = 20000,
    # Number of MCMC samples
    n.sample = 100000,
    thin = 10,
    #ICAR model given a 0-1 neighborhood structure
    rho = 1
  )

```



Using `ggplot()` and `geom_sf()` to plot the map of the latent spatial effects.

```{r Ex 9.4 carbayes map smoothed SMRs, class.source = 'foldable'}

observed <- tibble::rownames_to_column(observed, "ID")
phi_car <-  Ex9_4$samples$phi

latent_car_df  <-  data.frame(
  phi_mean = apply(phi_car, 2, mean),
  phi_sd = apply(phi_car, 2, sd)
)
# Combine latent spatial effect with england dataset
latent_car_df$ID <- observed$ID
latent_car_england <-  merge(england, latent_car_df, by = "ID")
# Creating map of smoothed SMRs in England in 2010
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = latent_car_england, aes(fill = phi_mean)) +
  labs(fill = 'Latent effects CARbayes') +
  # Clear background and plot borders
  theme_void()
```

## Example 9.5: Fitting a conditional model using INLA {-}

Now we fit the Poisson log--normal model with spatial effects to the data for respiratory admissions seen in Example 9.3 using  `R-INLA`. Recall that `R-INLA` approximates the resultant posterior distribution using an integrated nested Laplace approximation. The call to fit the model using `R-INLA` is very similar to the one when fitting a `glm`. The latent effects are introduced through the function `f(.)`.


```{r Ex 9.5 clean, include = FALSE}
rm(list = ls())
```

```{r Ex 9.5 inla load, message=FALSE, echo = FALSE, warning=FALSE, message= FALSE}

library(INLA)
library(spdep) 
# Reading in borders
england <- read_sf("data/copd/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copd/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copd/copdavgscore.csv") 
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )
copd_df$Average.Score <- as.numeric(scale(copd_df$Average.Score))
# Create areas IDs to match the values in UK.adj
copd_df$ID <- 1:324

# Create the neighborhood matrix
W.nb <- poly2nb (england , row.names = england$ID)
W.list <- nb2listw (W.nb , style = "B")
W.mat <- nb2mat (W.nb , style = "B")

# Convert the adjacency matrix into a file in the INLA format
nb2INLA("UK.adj", W.nb)

```

<!-- TODO: Show output for this model  -->


```{r Ex 9.5 inla run}
# run the INLA model
Ex9_5 <- inla(
  Y2010 ~ Average.Score + f(ID  , model = "besag", graph = "UK.adj"),
  family = "poisson",
  E = E2010,
  data = copd_df,
  control.predictor = list(compute = TRUE)
)
```


Summarize the results

```{r Ex 9.5 inla summary}
# Summarizing smoothed SMRs
summary(Ex9_5)

```


```{r Ex 9.5 inla smoothed SMRs, class.source = 'foldable'}

observed <- tibble::rownames_to_column(observed, "ID")
phi_car <-  Ex9_5$summary.random$ID

latent_car_df  <-  data.frame(
  phi_mean = phi_car$mean,
  phi_sd = phi_car$sd
)
# Combine latent spatial effect with england dataset
latent_car_df$ID <- observed$ID
latent_car_england <-  merge(england, latent_car_df, by = "ID")
# Creating map of smoothed SMRs in England in 2010
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = latent_car_england, aes(fill = phi_mean)) +
  labs(fill = 'Latent effects INLA') +
  # Clear background and plot borders
  theme_void()

```

## Supplementary Example: Fitting a Leroux model to the COPD dataset using Nimble, Stan and CARBayes
<!-- ## Example 9.6: Fitting a Leroux Model using Nimble, Stan and CARBayes -->

<!-- This example uses the [englandlocalauthority.shp](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/englandlocalauthority.shp), [copdmortalityobserved.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityobserved.csv), and -->
<!-- [copdmortalityexpected.csv](https://github.com/spacetime-environ/stepi2/blob/main/data/copd/copdmortalityexpected.csv) datasets from Chapter 6. -->

<!-- ### Nimble -->

<!-- ```{r Ex 9.6 nimble clean, include = FALSE} -->
<!-- rm(list=ls()) -->
<!-- ``` -->


<!-- ```{r Ex 9.6 nimble load, message=FALSE, warning=FALSE, message= FALSE} -->

<!-- library(ggplot2) # to plot map -->
<!-- library(spdep) # read the shapefile (read_sf) and build neighbors list (poly2nb) -->
<!-- library(nimble) -->

<!-- # Load data -->
<!-- # Reading in borders -->
<!-- england <- read_sf("data/copd/englandlocalauthority.shp") -->
<!-- # Reading in data -->
<!-- observed <- -->
<!--   read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1) -->
<!-- expected <- -->
<!--   read.csv(file = "data/copd/copd   mortalityexpected.csv") -->
<!-- covariates <- read.csv(file = "data/copd/copdavgscore.csv") -->
<!-- # Merge everything into one data frame -->
<!-- copd_df <- -->
<!--   cbind(observed,  expected) |> merge( -->
<!--     covariates, -->
<!--     by.x = "code", -->
<!--     by.y = "LA.CODE", -->
<!--     all.x = TRUE, -->
<!--     all.y = FALSE -->
<!--   ) -->

<!-- ``` -->


<!-- Define a function to obtain the number of neighbors. -->

<!-- ```{r Ex 9.6 nimble munge functions, echo = TRUE} -->

<!-- adjlist = function(W, N) { -->
<!--   adj = 0 -->
<!--   for (i in 1:N) { -->
<!--     for (j in 1:N) { -->
<!--       if (W[i, j] == 1) { -->
<!--         adj = append(adj, j) -->
<!--       } -->
<!--     } -->
<!--   } -->
<!--   adj = adj[-1] -->
<!--   return(adj) -->
<!-- } -->

<!-- ``` -->

<!-- Define the adjacency matrix and indexes for `stan` using the `nb2mat` and `adjlist` -->

<!-- ```{r Ex 9.6 nimble neighborhood} -->

<!-- # Create the neighborhood -->
<!-- W.nb <- -->
<!--   poly2nb(england, row.names = rownames(england)) -->
<!-- # Creates a matrix for following function call -->
<!-- W.mat <- nb2mat(W.nb, zero.policy = TRUE, style = "B") -->
<!-- # Define the spatial structure to use in stan -->
<!-- N <- length(unique(england$ID)) -->
<!-- neigh  <- adjlist(W.mat, N) -->
<!-- numneigh  <- apply(W.mat,2,sum) -->

<!-- # Define the precision matrix of Leroux -->
<!-- Q <- matrix(0, nrow = nrow(W.mat), ncol = ncol(W.mat)) -->
<!-- diag(Q) <- apply(W.mat, 1, sum) -->
<!-- Q <- Q - W.mat -->

<!-- ``` -->

<!-- ```{r Ex 9.6 nimble leroux model, warning=FALSE, message = FALSE, results='hide'} -->

<!-- Example9_6Nimble <- nimbleCode({ -->
<!--   # Likelihood -->
<!--   for (i in 1:N) { -->
<!--     y[i] ~ dpois(lambda[i]) -->
<!--     log(lambda[i]) <- log(E[i]) + b[i]  -->
<!--     mean_b[i] <- beta0 + beta1*x[i] -->
<!--    # b_centered[i] <- b[i] - b_mean -->
<!--   } -->

<!--   PrecMat[1:N, 1:N] <- tau*(rho*Q[1:N, 1:N] + (1-rho)*diag(N)) -->
<!--   # mean_b[1:N] <- nimRep(beta0, N) -->

<!--  # b_mean <- sum(b[1:N])/N -->
<!--     # Priors -->
<!--   beta0 ~ dnorm(0, sd = 1) -->
<!--   beta1 ~ dnorm(0, sd = 1) -->
<!--   b[1:N] ~ dmnorm(mean = mean_b[1:N], prec = PrecMat[1:N, 1:N]) -->
<!--   tau <- 1 / (sigma_b ^ 2) -->
<!--   sigma_b ~ T(dt(0,1,1),0,) -->
<!--   rho ~ dunif(0, 1) -->
<!--   # Fitted values and likelihood for WAIC -->
<!--   # for (i in 1:N) { -->
<!--   #   fitted[i] ~ dpois(lambda[i]) -->
<!--   # } -->
<!-- }) -->

<!-- #  Define the constants, data and initial values lists and run the model. -->

<!-- # constants list -->
<!-- constants <- list( -->
<!--   N = N, -->
<!--   E = copd_df$E2010, -->
<!--   Q = Q -->
<!-- ) -->
<!-- # data list -->
<!-- ex.data <- -->
<!--   list(y = copd_df$Y2010, -->
<!--        x = as.vector(scale(copd_df$Average.Score))) # vector of covariates -->
<!-- inits <-  list( -->
<!--   beta0 = rnorm(1), -->
<!--   beta1 = rnorm(1), -->
<!-- #  fitted = rpois(N, 2), -->
<!--   sigma_b = 1, -->
<!--   b = rnorm(N), -->
<!--   rho = 0.5 -->
<!-- ) -->
<!-- params <- c("beta0", "beta1", "b", "sigma_b", "tau" ,"rho") -->
<!-- # Run model in nimble -->
<!-- mcmc.out <- nimbleMCMC( -->
<!--   code = Example9_6Nimble, -->
<!--   constants = constants, -->
<!--   data = ex.data, -->
<!--   inits = inits, -->
<!--   monitors = params, -->
<!--   niter = 80000, -->
<!--   nburnin = 40000, -->
<!--   thin = 15, -->
<!--   WAIC = TRUE, -->
<!--   nchains = 2, -->
<!--   summary = TRUE, -->
<!--   samplesAsCodaMCMC = TRUE -->
<!-- ) -->

<!-- ``` -->

<!-- Show the WAIC, effective sample size, and trace plots for some of the parameters. -->


<!-- ```{r Ex 9.6 WAIC, ESS and traceplots } -->

<!-- mcmc.out$WAIC -->
<!-- min(coda::effectiveSize(mcmc.out$samples)) -->
<!-- plot(mcmc.out$samples[, c("beta0")], bty = "n", main = "beta0") -->
<!-- plot(mcmc.out$samples[, c("beta1")], bty = "n",  main = "beta1") -->
<!-- plot(mcmc.out$samples[, c("b[2]")], bty = "n",  main = "b[2]") -->
<!-- plot(mcmc.out$samples[, c("sigma_b")], bty = "n", main = "sigma_b") -->
<!-- ``` -->


<!-- ```{r Ex 9.6 nimble leroux summary, echo = TRUE} -->

<!-- # Extract samples -->
<!-- variables <- c("beta0", "beta1","sigma_b", "rho") -->
<!-- summary_nimble <- mcmc.out$summary$all.chains -->
<!-- summary_nimble[variables,] -->

<!-- ``` -->

<!-- Map the mean of the posterior estimate for the latent effect. -->

<!-- ```{r Ex 9.6 plot map nimble} -->

<!-- samples_Leroux_b <- -->
<!--   summary_Leroux_nimble[grepl("b\\[", rownames(summary_CAR_nimble)), ] |> as.data.frame() -->
<!-- observed <- tibble::rownames_to_column(observed, "ID") -->
<!-- samples_Leroux_b$ID <- observed$ID -->
<!-- CAR_nimble_merge <- merge(england, samples_CAR_b, by = "ID") -->
<!-- ggplot() + -->
<!--   # Choose spatial object and column for plotting -->
<!--   geom_sf(data = Leroux_nimble_merge, aes(fill = Mean)) + -->
<!--   # Change legend's label -->
<!--   labs(fill = 'Latent effects nimble') + -->
<!--   # Clear background and plot borders -->
<!--  theme_void() -->
<!-- ``` -->

<!-- ### Stan -->

<!-- Here, we implement the CAR model using `stan`. -->

<!-- ```{r Ex 9.6 stan clean, include = FALSE} -->
<!-- rm(list=ls()) -->
<!-- ``` -->


<!-- ```{r Ex 9.6 stan load, message=FALSE, echo = FALSE, warning=FALSE, message= FALSE} -->

<!-- library(ggplot2) -->
<!-- library(loo) # To calculate WAIC and loo later -->
<!-- library(rstan) -->
<!-- library(spdep) -->

<!-- # Additional settings for stan -->
<!-- options(mc.cores = parallel::detectCores()) -->
<!-- rstan_options(auto_write = TRUE) -->

<!-- # Load data -->
<!-- # Reading in borders -->
<!-- england <- read_sf("data/copd/englandlocalauthority.shp") -->
<!-- # Reading in data -->
<!-- observed <- -->
<!--   read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1) -->
<!-- expected <- -->
<!--   read.csv(file = "data/copd/copdmortalityexpected.csv") -->
<!-- covariates <- read.csv(file = "data/copd/copdavgscore.csv") -->
<!-- # Merge everything into one data frame -->
<!-- copd_df <- -->
<!--   cbind(observed,  expected) |> merge( -->
<!--     covariates, -->
<!--     by.x = "code", -->
<!--     by.y = "LA.CODE", -->
<!--     all.x = TRUE, -->
<!--     all.y = FALSE -->
<!--   ) -->

<!-- ``` -->


<!-- We will need two functions to structure the matrix of neighbors that will be needed in `stan`. -->

<!-- ```{r Ex 9.6 stan munge functions, echo = TRUE} -->

<!-- adjlist = function(W, N) { -->
<!--   adj = 0 -->
<!--   for (i in 1:N) { -->
<!--     for (j in 1:N) { -->
<!--       if (W[i, j] == 1) { -->
<!--         adj = append(adj, j) -->
<!--       } -->
<!--     } -->
<!--   } -->
<!--   adj = adj[-1] -->
<!--   return(adj) -->
<!-- } -->

<!-- mungeCARdata4stan = function(adjBUGS, numBUGS) { -->
<!--   N = length(numBUGS) -->
<!--   nn = numBUGS -->
<!--   N_edges = length(adjBUGS) / 2 -->
<!--   node1 = vector(mode = "numeric", length = N_edges) -->
<!--   node2 = vector(mode = "numeric", length = N_edges) -->
<!--   iAdj = 0 -->
<!--   iEdge = 0 -->

<!--   for (i in 1:N) { -->
<!--     for (j in 1:nn[i]) { -->
<!--       iAdj = iAdj + 1 -->
<!--       if (i < adjBUGS[iAdj]) { -->
<!--         iEdge = iEdge + 1 -->
<!--         node1[iEdge] = i -->
<!--         node2[iEdge] = adjBUGS[iAdj] -->
<!--       } -->
<!--     } -->
<!--   } -->
<!--   return (list( -->
<!--     "N" = N, -->
<!--     "N_edges" = N_edges, -->
<!--     "node1" = node1, -->
<!--     "node2" = node2 -->
<!--   )) -->

<!-- } -->
<!-- ``` -->



<!-- This model is in a separate file called `Example8_3.stan` that will be called later. -->

<!-- ```{r engine='bash', comment='', echo = FALSE} -->
<!-- cat functions/Example9_6.stan -->
<!-- ``` -->
<!-- <!-- REVIEW: I am missing the adjacency matrix but I calculated like they did for the CARBayes example, is that right? --> -->

<!-- Define the adjacency matrix and indexes for `stan` using the `nb2mat` and `adjlist` -->


<!-- ```{r Ex 9.6 stan car effects, warning=FALSE, results='hide', message = FALSE, cache=TRUE} -->

<!-- # Create the neighborhood -->
<!-- W.nb <- -->
<!--   poly2nb(england, row.names = rownames(england)) -->
<!-- # Creates a matrix for following function call -->
<!-- W.mat <- nb2mat(W.nb, style = "B") -->

<!-- # Define the spatial structure to use in stan -->
<!-- N <- length(unique(england$ID)) -->
<!-- neigh <- adjlist(W.mat, N) -->
<!-- numneigh  <-  apply(W.mat, 2, sum) -->
<!-- nbs <- mungeCARdata4stan(neigh, numneigh) -->


<!-- # Define data and variables for Stan model -->
<!-- y <- copd_df$Y2010 -->
<!-- E <- copd_df$E2010 -->
<!-- X <- as.numeric(scale(copd_df$Average.Score)) -->


<!-- # Obtain cumulative number of neighbors -->
<!-- N1=N+1 -->
<!-- C=numeric()  -->
<!-- C[1]=0 -->
<!-- for (j in 2:N1)  -->
<!--   C[j]=C[j-1]+ numneigh[j-1] # cumulative number of neighbours -->



<!-- ex.data <- list( -->
<!--   N = nbs$N, -->
<!--   y = y, -->
<!--   E = E, -->
<!--   p = 1, -->
<!--   C = C, -->
<!--   d = numneigh, -->
<!--   X = as.matrix(X), -->
<!--   N_edges = nbs$N_edges, -->
<!--   node1 = nbs$node1, -->
<!--   node2 = nbs$node2 -->
<!-- ) -->

<!-- Example9_6Stan  <- stan( -->
<!--   file = "functions/Example9_6.stan", -->
<!--   data = ex.data, -->
<!--   warmup = 10000, -->
<!--   iter = 20000, -->
<!--   chains = 2, -->
<!--   thin = 10, -->
<!--   pars = c("beta0", "beta","sigma_s", "b", "log_lik", "rho"), -->
<!--   include = TRUE -->
<!-- ) -->

<!-- ``` -->


<!-- Show traceplots. -->

<!-- ```{r Ex 9.6 stan CAR traceplots} -->
<!-- #computing WAIC using the package loo -->

<!-- loglikcar <- extract_log_lik(Example9_6Stan) -->
<!-- waiccar <- waic(loglikcar) -->
<!-- waiccar -->

<!-- rstan::traceplot(Example9_6Stan, pars = c("beta0","beta","sigma_s")) -->

<!-- ``` -->

<!-- Show the posterior summary for the parameters if interest. -->
<!-- Keep in mind that in the stan model the we are sampling the standard deviation of the random effect unlike `CARBayes` where the variance is obtained. -->

<!-- ```{r Ex 9.6 stan CAR results, echo = TRUE} -->

<!-- # Extract samples -->
<!-- summary_CAR_stan <- -->
<!--   summary( -->
<!--     Example9_6Stan, -->
<!--     pars = c("beta0", "beta","sigma_s"), -->
<!--     probs = c(0.025, 0.975) -->
<!--   ) -->

<!-- summary_CAR_stan$summary -->

<!-- ``` -->



<!-- Map the mean of the posterior estimate for the latent effect. -->

<!-- ```{r Ex 9.6 plot map stan} -->

<!-- summary_CAR_stan_b <- -->
<!--   summary( -->
<!--     Example9_6Stan, -->
<!--     pars = c("b"), -->
<!--     probs = c(0.025, 0.975) -->
<!--   ) -->

<!-- observed <- tibble::rownames_to_column(observed, "ID") -->

<!-- summary_CAR_stan_b$ID <- observed$ID -->

<!-- CAR_stan_merge <- merge(england, summary_CAR_stan_b, by = "ID") -->

<!-- ggplot() + -->
<!--   # Choose spatial object and column for plotting -->
<!--   geom_sf(data = CAR_stan_merge, aes(fill = summary.mean)) + -->
<!--   # Change legend's label -->
<!--   labs(fill = 'Latent effects stan') + -->
<!--   # Clear background and plot borders -->
<!--   theme_void() -->


<!-- ``` -->

<!-- ### CARBayes -->


<!-- ```{r Ex 9.6 clean, include=FALSE} -->
<!-- rm(list=ls()) -->
<!-- ``` -->


<!-- ```{r Ex 9.6 carbayes load, message=FALSE, warning=FALSE, message= FALSE} -->

<!-- library(CARBayes) -->
<!-- library(ggplot2) -->
<!-- library(sf) -->
<!-- library(spdep) -->

<!-- # Reading in borders -->
<!-- england <- read_sf("data/copd/englandlocalauthority.shp") -->
<!-- # Reading in data -->
<!-- observed <- -->
<!--   read.csv(file = "data/copd/copdmortalityobserved.csv", row.names = 1) -->
<!-- expected <- -->
<!--   read.csv(file = "data/copd/copdmortalityexpected.csv") -->
<!-- covariates <- read.csv(file = "data/copd/copdavgscore.csv") -->
<!-- # Merge everything into one data frame -->
<!-- copd_df <- -->
<!--   cbind(observed,  expected) |> merge( -->
<!--     covariates, -->
<!--     by.x = "code", -->
<!--     by.y = "LA.CODE", -->
<!--     all.x = TRUE, -->
<!--     all.y = FALSE -->
<!--   ) -->
<!-- copd_df$Average.Score <- as.numeric(scale(copd_df$Average.Score)) -->

<!-- ``` -->


<!-- ```{r Ex 9.6 carbayes neighborhood} -->

<!-- # Create the neighborhood -->
<!-- W.nb <- -->
<!--   poly2nb(england, row.names = rownames(england)) -->

<!-- # Creates a matrix for following function call -->
<!-- W.mat <- nb2mat(W.nb, style = "B") -->

<!-- ``` -->

<!-- ```{r Ex 9.6 carbayes run, results='hide', message = FALSE, cache = TRUE} -->

<!-- # Running smoothing model -->
<!-- Ex9_6 <- -->
<!--   S.CARleroux( -->
<!--     # Model Formula -->
<!--     formula = Y2010 ~ offset(log(E2010)) + Average.Score, -->
<!--     # data frame with data -->
<!--     data = copd_df, -->
<!--     # Choosing Poisson Regression -->
<!--     family = "poisson", -->
<!--     # Neighborhood matrix -->
<!--     W = W.mat, -->
<!--     # Number of burn in samples -->
<!--     burnin = 20000, -->
<!--     # Number of MCMC samples -->
<!--     n.sample = 100000, -->
<!--     thin = 10 -->
<!--     # Note that here we don't set rho to any value as it need to be estimated -->
<!--   ) -->

<!-- ``` -->

<!-- We can extract the new smoother values from the model output and divide them by the expected values in order to compare both methods. -->


<!-- ```{r Ex 9.6 carbayes smoothed SMRs, class.source = 'foldable' } -->

<!-- # Creating a dataset with smoothed SMRs in 2010 -->
<!-- SMR2010 <- Ex9_6$fitted.values /  copd_df$E2010 -->
<!-- SMR_smooth <- as.data.frame(SMR2010, row.names = rownames(observed)) -->
<!-- # Printing first six rows of smoothed SMRs -->
<!-- head(SMR_smooth) -->
<!-- # Summarizing smoothed SMRs -->
<!-- summary(SMR_smooth) -->
<!-- # Summary of the parameters under the CAR model. -->
<!-- Ex9_6$summary.results[] -->
<!-- ``` -->

<!-- <!-- QUESTION: What do we want the latent effect or the fitted values? --> -->

<!-- Use `ggplot()` and `geom_sf()` to plot the map of the latent spatial effect. -->

<!-- ```{r Ex 9.6 carbayes map smoothed SMRs, class.source = 'foldable'} -->

<!-- observed <- tibble::rownames_to_column(observed, "ID") -->
<!-- phi_car <-  Ex9_6$samples$phi -->

<!-- latent_car_df  <-  data.frame( -->
<!--   phi_mean = apply(phi_car, 2, mean), -->
<!--   phi_sd = apply(phi_car, 2, sd) -->
<!-- ) -->
<!-- # Combine latent spatial effect with england dataset -->
<!-- latent_car_df$ID <- observed$ID -->
<!-- latent_car_england <-  merge(england, latent_car_df, by = "ID") -->
<!-- # Creating map of smoothed SMRs in England in 2010 -->
<!-- ggplot() + -->
<!--   # Choose spatial object and column for plotting -->
<!--   geom_sf(data = latent_car_england, aes(fill = phi_mean)) + -->
<!--   labs(fill = 'Latent effects CARbayes Leroux model') + -->
<!--   # Clear background and plot borders -->
<!--   theme_void() -->
<!-- ``` -->

